\documentclass[english]{reedthesis}

\usepackage[T1]{fontenc}
\usepackage{babel}

\usepackage{appendix}
\usepackage{amsfonts, amscd, amssymb, amsthm, amsmath}
\usepackage{mathtools} %xmapsto etc
\usepackage{pdfsync} %leaves makers for tex searching
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{complexity}
\usepackage{mleftright}

\usepackage[linesnumbered]{algorithm2e}
\usepackage{biblatex}
\usepackage{imakeidx}
\usepackage{microtype}
\usepackage{tikz}

% FIXME: Really bad hack to get cleveref working
% This is unnecessary on literally every other LaTeX setup except mine
% When working again, will need to reset theorem defs to point to the thm counter
\usepackage{aliascnt}

\usepackage[colorlinks]{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}

%%% Theorems %%%---------------------------------------------------------
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newaliascnt{lemma}{thm}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\newaliascnt{prop}{thm}
\newtheorem{prop}[prop]{Proposition}
\aliascntresetthe{prop}
\newaliascnt{cor}{thm}
\newtheorem{cor}[cor]{Corollary}
\aliascntresetthe{cor}
\theoremstyle{definition}
\newtheorem*{def*}{Definition}
\newaliascnt{defn}{thm}
\newtheorem{defn}[defn]{Definition}
\aliascntresetthe{defn}
\theoremstyle{remark}
\newtheorem{example}{Example}[thm]
\newtheorem{remark}[thm]{Remark}
\newtheorem{subrem}[example]{Remark}


\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dimn}{dim}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\ev}{ev}
\def\f{\varphi}
\def\half{\hbox{$\frac12$}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\img}{img}
\DeclareMathOperator{\Inn}{Inn}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\mdeg}{mdeg}
\DeclareMathOperator{\rad}{rad}
\DeclareMathOperator{\Rep}{Rep}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\rk}{rank}
\def\normeq{\trianglelefteq}
\DeclareMathOperator{\nul}{nullity}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Syl}{Syl}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\tr}{tr}
\def\vep{\varepsilon}
\DeclareMathOperator{\lcm}{lcm}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\ang}{\langle}{\rangle}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\middlemid}{%
  \ensuremath{\;\middle\vert\;}
}

\newcommand{\dblang}[1]{%
  \ensuremath{\left\langle\!\left\langle#1\right\rangle\!\right\rangle}
}

\newcommand{\comment}[1]{%
  \text{\phantom{(#1)}} \tag{#1}%
}

\newcommand{\commath}[1]{%
  \phantom{(#1)} \tag{#1}%
}


\addbibresource{bibliography.bib}

\makeindex

\title{Thesis Draft: Algebrization}
\author{Patrick Norton}

\approvedforthe{Committee}
\thedivisionof{The Established Interdisciplinary Committee for \\}
\division{Mathematics and Computer Science}
\department{Mathematics and Computer Science}
\advisor{Zajj Daugherty}
\altadvisor{Adam Groce}

\begin{document}

\maketitle

\tableofcontents

% TODO: Abstract

\chapter*{Introduction}

The $\P$ vs $\NP$ problem is perhaps the most important open problem in
complexity theory.

\chapter{Preliminaries}

% TODO: Do we need Hamming distance? I can't find them actually using it in CFGS22

\section{Turing machines}

% TODO: Do we even need to formally define a TM?

Central to our definitions of complexity is that of a Turing machine. This is
the most common mathematical model of a computer, and is the jumping-off point
for mant variants. There are many ways to think of a Turing machine, but the
most common is that of a small machine that can read and write to an
arbitrarily-long ``tape'' according to some finite set of rules. We give a more
formal definition below, and then we will attempt to take this definition into a
more manageable form.
\begin{defn}[{\cite[Def.\ 3.1]{Sip97}}]\label{def:TM}\index{Turing machine}
  A \emph{Turing machine} is a 7-tuple $(Q, \Sigma, \Gamma, \delta, q_{0}, q_{a}, q_{r})$ where
  $Q$, $\Sigma$, and $\Gamma$ are all finite sets and
  % TODO: Rephrase to separate out terminology and definitions?
  \begin{enumerate}
    \item $Q$ is the set of \emph{states},
    \item $\Sigma$ is the \emph{input alphabet},
    \item $\Gamma$ is the \emph{tape alphabet},
    \item $\delta: Q \times \Gamma \rightarrow Q \times \Gamma \times \{L, R\}$ is the \emph{transition function},
    \item $q_{0} \in Q$ is the \emph{start state},
    \item $q_{a} \in Q$ is the \emph{accept state},
    \item $q_{r} \in Q$ is the \emph{reject state}, with $q_{a} \ne q_{r}$.
  \end{enumerate}
\end{defn}

While we have this formalism here as a useful reference, even here we will most
frequently refer to Turing machines in a more intuitionisitc form. There are
several ways we will think about Turing machines.

The first way to think about a Turing machine is as a little computing box with
a tape. We let the box read and write to the tape, and each step it can move the
tape one space in either direction. At some point, the machine can decide it is
done, in which case we say it ``halts''; however it does not necessarily need to
halt. For this paper, we will only think about machines that \emph{do} halt, and
in particular we will care about how many it takes us to get there. Further, we
will use this informalism as a base from which we can define our Turing machine
variants intuitively, without needing to deal with the (potentially extremely
convoluted) formalism.

% TODO: Cite Church-Turing
Another way we think about a Turing machine is as an algorithm. Perhaps the
foundational paper of modern computer science theory, the \emph{Church-Turing
  thesis}~\cite{Tur36}, states that any actually-computable algorithm has an
equivalent Turing machine, and vice versa. We will use this fact liberally; in
many cases we will simply describe an algorithm and not deal with putting it
into the context of a Turing machine. If we have explained the algorithm well
enough that a reader can execute it (as we endeavor to do), then we know a
Turing machine must exist.

% TODO: Nondeterministic TM

\section{Complexity classes}

% TODO: Do I need to define big-O for this?

Complexity classes are the main way we think about the hardness of problems in
computer science. A complexity class\index{complexity class} is a collection of
languages that all share a common level of difficulty.

\subsection{Time complexity}

The most intuitive (and most important) notion of complexity is that of time
complexity. Time complexity is the answer of the question of how long it takes
to solve a problem. We begin with an abstract base for our time classes, and
will then introduce some specific ones that we care about.

\begin{defn}[{\cite[Def.\ 1.19]{AB09}}]\label{def:dtime}\index{DTIME@$\DTIME$}
  % FIXME: Reword to make more clear what $n$ is
  Let $f: \mathbb{N} \rightarrow \mathbb{N}$ be a function. The class $\DTIME(f(n))$ is the class of all
  problems computable by a deterministic Turing machine in $O(f(n))$ steps for
  some constant $c > 0$.
\end{defn}

While $\DTIME$ is a useful base to start from, it is rare that we deal with
$\DTIME$ classes directly. % TODO

\begin{defn}[{\cite[Def.\ 1.20]{AB09}}]\label{def:p}\index{P@$\P$}
  The complexity class $\P$ is the class
  \[
    \P = \bigcup_{c > 0}\DTIME(n^{c}).
  \]
\end{defn}

The class $\P$ is perhaps the most important complexity class. Mathematically,
we care about $\P$ because it is closed under composition: a polynomial-time
algorithm iterated a polynomial number of times is still in $\P$. Further, $\P$
turns out to generally be invariant under change of (deterministic) computation
model, which allows us to reason about $\P$ problems easily without needing to
resort to the formal definition of a Turing machine. More philosophically, $\P$
generally represents the set of ``efficient'' algorithms in the real world.

\begin{example}\label{ex:polynomial-is-p}
  The language
  \[
    \{(p, x, y) \mid p \text{ a polynomial and } p(x) = y\}
  \]
  is in $\P$. We can calculate whether a string is in this language by
  calculating $p(x)$ (which we can do efficiently), and then comparing it to
  $y$.
\end{example}

As we have defined $\P$ in terms of $\DTIME$, the question arises of whether
there is an equivalent in terms of $\NTIME$. Naturally, there is, and we call it
$\NP$.

\begin{defn}[{\cite[Cor.\ 7.22]{Sip97}}]\label{def:np}\index{NP@$\NP$}
  The complexity class $\NP$ is the class
  \[
    \NP = \bigcup_{c > 0}\NTIME(n^{c}).
  \]
\end{defn}

While this definition demonstrates how $\NP$ is similar to $\P$, there are other
equivalent ones that we can use. In particular, we very often like to think of
$\NP$ in terms of deterministic \emph{verifiers}. Since nondeterministic
machines do not exist in real life, this definition gives a practical meaning to
$\NP$.

\begin{example}\label{ex:sat-is-np}
  The language $\SAT$ is the language of Boolean formulas with at least one
  solution. $\SAT$ is in $\NP$: we can nondeterministically pick a potential
  solution and then evaluate our formula (which can be done efficiently); there
  will be an accepting path if and only if a solution to the formula exists.
\end{example}

\begin{thm}[{\cite[Def.\ 7.19]{Sip97}}]\label{thm:np-verifier}
  $\NP$ is exactly the class of all languages verifiable by a $\P$-time Turing
  machine.
\end{thm}

% TODO: Should I prove this?

\begin{example}\label{ex:sat-np-verifier}
  The language $\SAT$ we defined in \cref{ex:sat-is-np} can be verified
  efficiently, where the certificate is an accepting set of variables. Since we
  can evaluate a Boolean formula efficiently, if we already have an accepting
  set of variables we can therefore verify it in $\P$.
\end{example}

\subsection{Space complexity}

In addition to time complexity, the an additional notion of complexity is that
of space complexity. Space complexity is the question of how much space on its
memory tape a machine needs in order to compute a problem. In many ways, our
definitions of space complexity are analagous to those for time complexity that
we have already defined. In particular, $\DSPACE$ will correspond nicely to
$\DTIME$, and $\NSPACE$ to $\NTIME$.

\begin{defn}[{\cite[Def.\ 4.1]{AB09}}]\label{def:dspace}\index{DSPACE@$\DSPACE$}
  Let $f: \mathbb{N} \rightarrow \mathbb{N}$ be a function. A language $L$ is in $\DSPACE(f(n))$ if there
  exists a deterministic Turing machine $M$ such that the number of locations on
  the tape that are non-blank at some point during the execution of $M$ is
  in $O(f(n))$.
\end{defn}

In the same way as we have defined $\DSPACE$ for deterministic machines, we now
need to define $\NSPACE$ for nondeterministic machines.

\begin{defn}[{\cite[Def. 4.1]{AB09}}]\label{def:nspace}\index{NSPACE@$\NSPACE$}
  Let $f: \mathbb{N} \rightarrow \mathbb{N}$ be a function. A language $L$ is in $\NSPACE(f(n))$ if there
  exists a nondeterministic Turing machine $M$ such that the number of locations
  on the tape that are non-blank at some point during the execution of $M$ is in
  $O(f(n))$.
\end{defn}

Analagously to $\P$ and $\NP$, our two main classes of space complexity are
$\PSPACE$ and $\NPSPACE$.

\begin{defn}[{\cite[Def.\ 4.5]{AB09}}]\label{def:pspace}\index{PSPACE@$\PSPACE$}
  The complexity class $\PSPACE$ is the class
  \[
    \PSPACE = \bigcup_{c > 0}\DSPACE(n^{c}).
  \]
\end{defn}

\begin{defn}[{\cite[Def.\ 4.5]{AB09}}]\label{def:npspace}\index{NPSPACE@$\NPSPACE$}
  The complexity class $\NPSPACE$ is the class
  \[
    \NPSPACE = \bigcup_{c > 0}\NSPACE(n^{c}).
  \]
\end{defn}

Unlike with $\P$ and $\NP$, the relationship between $\PSPACE$ and $\NPSPACE$ is
well known. Due to the complexity of the proof of the theorem, we will not prove
it here, as it is mostly not relevant to what we will be doing.

\begin{thm}[{Savitch's theorem;~\cite{Sav70}}]\label{thm:savitch}\index{Savitch's theorem}
  $\PSPACE = \NPSPACE$.
\end{thm}

Upon seeing this, one might ask why it is that we believe $\P \ne \NP$ if we know
that $\PSPACE = \NPSPACE$, given they are defined analogously. The answer to
this question boils down to the fact that we are able to reuse space, while we
are not able to reuse time. Space on the tape that is no longer needed can be
overwritten, while time that is no longer needed is gone forever.

Since $\PSPACE$ and $\NPSPACE$ are equal classes, it is relatively rare to see
$\NPSPACE$ referred to. Here, we will only refer to it when it makes a class
relationship clearer; most frequently when comparing $\NPSPACE$ to some other
nondeterministic class.

\begin{example}\label{ex:regex-is-pspace}
  The language
  \[
    \{(x, y) \mid x, y \text{ regexes that accept the same set of strings}\}
  \]
  is in $\PSPACE$. % TODO: Explain why
\end{example}

% TODO: If/when we pull random/quantum stuff in here, probably worth defining
% BPP and BQP (QMA might need its own section?)

% TODO: Draw a graph of all these complexity inclusions

\subsection{Completeness}

% TODO: Not a huge fan of this paragraph
Even within a complexity class, not all problems are created equal. The notion
of \emph{completeness} gives us a mathematically-rigorous way to talk about
which problems in a class are the hardest. Since putting upper bounds on hard
problems naturally puts those same bounds on any easier problems, complete
problems can be useful in reasoning about the relationship between complexity
classes.

\begin{defn}[{\cite[Def.\ 7.29]{Sip97}}]\label{def:p-reduction}\index{Polynomial-time reduction}
  A language $A$ is \emph{polynomial-time reducible} to a language $B$ if a
  polynomial-time computable function $f: \Sigma^{*} \rightarrow \Sigma^{*}$ exists such that for
  all $w \in \Sigma^{*}$, $w \in A$ if and only if $f(w) \in B$.
\end{defn}

Polynomial-time reductions are important because they give us a way to say that
$A$ is \emph{no harder} than $B$. In particular, if we have an algorithm $M$
that determines $B$, we can construct the following algorithm that determines
$A$ with only a polynomial amount of additional work:

\begin{algorithm}[H]
  \KwIn{A string $w \in \Sigma^{*}$}
  \KwOut{Whether $w \in A$}
  Compute $f(w)$\;
  Use $M$ to check whether $f(w) \in B$\;
  \KwRet{the result of $M$}\;
  \caption{An algorithm to reduce $A$ to $B$}
\end{algorithm}

\begin{defn}[{\cite[Def.\ 7.34]{Sip97}}]\label{def:np-complete}\index{NP-complete@$\NP$-complete}
  A language $L$ is $\NP$-complete if $L \in \NP$ and every $A \in \NP$ is
  polynomial-time reducible to $L$.
\end{defn}

This is a practical use of our polynomial-time reductions: since an
$\NP$-complete language has a reduction from every other language in $\NP$, it
follows that it is \emph{at least as hard} as any other language in $\NP$. Of
particular interest to complexity theorists is the fact that $\P = \NP$ if and
only if \emph{any} $\NP$-complete language is in $\P$.

\begin{example}\label{ex:sat-is-complete}
  A famous result of Cook~\cite{Cook71}, also proved around the same time by
  Levin and thus called the \emph{Cook-Levin theorem}, is that the $\SAT$
  problem we defined earlier in \cref{ex:sat-is-np} is $\NP$-complete.
\end{example}

Just as we have $\NP$-completeness for time complexity, we also have notions of
completeness for space complexity. Since $\PSPACE = \NPSPACE$, instead of
calling the class $\NPSPACE$-complete, we call it $\PSPACE$-complete.

\begin{defn}[{\cite[Def.\ 8.8]{Sip97}}]\label{def:pspace-complete}\index{PSPACE-complete@$\PSPACE$-complete}
  A language $L$ is $\PSPACE$-complete if $L \in \PSPACE$ and every $A \in \PSPACE$
  is polynomial-time reducible to $\NP$.
\end{defn}

While this definition is mostly analagous to that of $\NP$-completeness, one
might wonder why we use a time complexity for our reduction when $\PSPACE$ is a
space-complexity class. This is because if we were to use space complexity, we
would want to use $\PSPACE$-reductions, but that would make every language in
$\PSPACE$ trivially $\PSPACE$-complete. Since that is not a useful definition,
we instead restrict ourselves to polynomial-time reductions.

\begin{example}
  A result of Stockmeyer and Meyer~\cite{SM73} is that the language we defined
  in \cref{ex:regex-is-pspace} is $\PSPACE$-complete.
\end{example}

\section{Polynomials}\label{sec:polynomial}

\begin{defn}[{\cite{Knu92}}]\label{def:iverson-bracket}\index{Iverson bracket}
  Let $P$ be a mathematical statement. The function $[P]$ is the the function
  \begin{equation}\label{eqn:iverson-bracket}
    [P] = \begin{cases}
      1 & P \text{ is true} \\
      0 & \text{otherwise.}
    \end{cases}
  \end{equation}
  This is called the \emph{Iverson bracket}, after its inventor Kenneth Iverson,
  who originally included it in the programming language APL (although
  originally using parentheses)~\cite[11]{APL}.
\end{defn}

\begin{example}
  Using the Iverson bracket, the Kronecker delta function can be defined as
  \[
    \delta_{ij} = [i = j].
  \]
\end{example}
% TODO: Transition sentence

Much of our work will deal with multivariate polynomials. For a given field
$\mathbb{F}$, we will denote the set of $m$-variable polynomials over
$\mathbb{F}$ with $\mathbb{F}[x_{1, \ldots, m}]$.

\begin{defn}[{\cite[8]{AW09}}]\label{def:mdeg}\index{multidegree}
  The \emph{multidegree} of a multivariate polynomial $p$, written $\mdeg(d)$,
  is the maximum degree of any variable $x_{i}$ of $p$.
\end{defn}

It is worth noting that for monovariate polynomials, multidegree and degree
coincide. The difference between multidegree and degree is subtle, but
important. We shall illustrate the difference with a simple example.

\begin{example}
  Consider the polynomial $x_{1}^{2}x_{2} + x_{2}^{2}$. The multidegree of this
  polynomial is 2, while its degree is 3.
\end{example}

We denote by $\mathbb{F}[x_{1, \ldots, m}^{\le d}]$ the subset of
$\mathbb{F}[x_{1, \ldots, m}]$ of polynomials with multidegree at most $d$. We also
need two special cases of these polynomials, which we will want to quickly be
able to reference throughout the paper.

\begin{defn}[{\cite[8]{AW09}}]\label{def:mlin}\index{multilinear}\index{multiquadratic}
  A polynomial is \emph{multilinear} if it has multidegree at most 1. Similarly,
  a polynomial is \emph{multiquadratic} if it has multidegree at most 2.
\end{defn}

From here, we need to define the notion of an \emph{extension polynomial}. This
gives the ability to take an arbitrary multivariate function defined on a subset
of a field and extend it to be a multivariate polynomial over the \emph{whole}
field.

\begin{defn}[{\cite[8]{AW09}}]\label{def:ext-poly}\index{extension polynomial}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f: H^{m} \rightarrow \mathbb{F}$ be a function. An \emph{extension polynomial} of $f$
  is any polynomial $f' \in \mathbb{F}[x_{1, \ldots, m}]$ such that $f(h) = f'(h)$ for
  all $h \in H$.
\end{defn}
% TODO: Examples

It turns out that this polynomial needs only to be of a surprisingly low
multidegree. Since polynomials of lower degree are generally easier to compute,
we would like to have some measure of what a ``small'' polynomial actually is in
this context.

% TODO: Rewrite \hat{f} as \tilde{f} to agree with AW09
\begin{defn}[{\cite[\defaultS 5.1]{CFGS22}}]\label{def:low-deg-ext}\index{low-degree extension}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f: H^{m} \rightarrow \mathbb{F}$ be a function. A \emph{low-degree extension} $\hat{f}$
  of $f$ is an extension of $f$ with multidegree at most $\abs{H} - 1$.
\end{defn}

% TODO: Cite these statements
It turns out that this is the minimum possible degree of any extension
polynomial. Further, it turns out that for any $f$, there is a \emph{unique}
low-degree extension. Neither of these statements are particularly important for
our further work, so we will not endeavor to prove them here. Something of
practical use to us is an explicit formula for the low-degree extension, which
we shall now calculate.

\begin{thm}[{\cite[\defaultS 5.1]{CFGS22}}]\label{thm:low-deg-ext-exists}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f: H^{m} \rightarrow \mathbb{F}$. Then a low-degree extension $\hat{f}$ of $f$ is the
  function
  \begin{equation}
    \hat{f}(x) = \sum_{\beta \in H^{m}}\delta_{\beta}(x)f(\beta),
  \end{equation}
  where $\delta$ is the polynomial
  % TODO? Flip x and y here
  \begin{equation}\label{eqn:delta-poly}
    \delta_{x}(y) = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
        \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
      \mright)\mright).
  \end{equation}
\end{thm}

\begin{proof}
  First, we must show $\hat{f}$ has multidegree $\abs{H} - 1$. First, note that
  $\hat{f}$ is a linear combination of some $\delta_{x}$es; hence asking about the
  multidegree of $\hat{f}$ is really just asking about the multidegree of
  $\delta_{x}$. Looking at $\delta_{x}$, the innermost product has $\abs{H} - 1$ terms,
  each with the same $y_{i}$; thus those terms have multidegree $\abs{H} - 1$.
  Summing terms preserves their multidegree, and the outer product iterates over
  the variables, thus it preserves multidegree as well. Thus, $\delta_{x}$ has
  multidegree $\abs{H} - 1$.

  % TODO: Turn this into a lemma?
  To understand why $\hat{f}(x)$ agrees with $f(x)$ on $H$, we first should look
  at $\delta_{\beta}(x)$. In particular, for all $x, y \in H^{m}$,
  \begin{equation}\label{eqn:delta-is-delta}
    \delta_{y}(x) = [x = y] = \delta_{xy}.
  \end{equation}
  This can be shown through some algebra which we have worked through in full
  detail in \cref{app:ext-poly}. This is the reason why we have named the
  polynomial in \cref{eqn:delta-poly} as we have; it functions as the Kronecker
  delta function over the set $H^{m}$.

  Taking the above statement, we get that for all $x \in H^{m}$, the only nonzero
  term of $\hat{f}(x)$ is the term where $\beta = x$; thus $\hat{f}(x) = f(x)$.
  Hence, $\hat{f}$ is a low-degree extension of $f$.
  % TODO: Does this even need a proof or can we leave it as a claim?
\end{proof}

Of particular interest to us will be the case of low-degree extensions where
$H = \{0, 1\}$. Since every field contains both $0$ and $1$, this will allow us
to construct a set consisting of an extension for \emph{every} field. Further,
since $\abs{H} = 2$ here, it means our low-degree extensions will be
multilinear. Not only do we thus constrain our polynomial to have a very low
multidegree, the $\delta$ function also dramatically simplifies in this case, which
makes it much easier to reason about.

\begin{cor}[{\cite[\defaultS 4.1]{AW09}}]\label{cor:low-degree-boolean}
  Let $\mathbb{F}$ be a finite field, $m \in \mathbb{N}$ a number, and
  $f: \{0, 1\}^{m} \rightarrow \mathbb{F}$. Then
  \begin{equation}\label{eqn:low-deg-ext-small}
    \hat{f}(x) = \sum_{\beta \in \{0, 1\}^{m}}\delta_{\beta}(x)f(\beta)
  \end{equation}
  is a low-degree extension of $f$, where $\delta$ is the polynomial
  \begin{equation}\label{eqn:delta-poly-small}
    \delta_{y}(x) = \mleft(\prod_{i:y_{i}=1}x_{i}\mright)\mleft(\prod_{i:y_{i}=0}(1 - x_{i})\mright).
  \end{equation}
\end{cor}
Note that in the product bound $i:y_{i} = 1$, we mean the product over all
numbers $i$ such that $y_{i} = 1$.
% TODO: Prove?

As we can see, the form of $\delta$ in \cref{eqn:delta-poly-small} is much more
manageable than the form in \cref{eqn:delta-poly}, and it is perhaps more
immediately apparent here why $\delta$ has the property it does. Further, since this
equation has no division, it turns out that it is valid for arbitrary
(non-trivial) rings, while the more complex equation is only valid for fields.
We show the algebra that brings us from the first to the second in
\cref{app:ext-poly}.

The form of $\delta_{y}$ defined in \cref{eqn:delta-poly-small} has further use to us
than just being simpler. In particular, these $\delta_{y}$ form a basis of
multilinear polynomials (and hence a generating set for the ring of all
polynomials). This is a particularly useful basis because it allows us to reason
about multilinear polynomials based solely on their outcomes on the Boolean
cube.\footnote{As an aside, this fact provides a relatively slick proof of the
  special case of our unproven statement earlier that low-degree extensions are
  both of minimal degree and unique.}

% TODO: Find better statement of this theorem
\begin{thm}[{\cite[\defaultS 4.1]{AW09}}]
  For any field $\mathbb{F}$, the set $\{\delta_{x} \mid x \in \{0, 1\}^{n}\}$ forms a
  basis for the vector space of multilinear polynomials
  $\mathbb{F}^{n} \rightarrow \mathbb{F}$.
\end{thm}

\begin{proof}
  Since $\delta_{y}(x) = 0$ for all $y \ne x \in \{0, 1\}^{n}$, it follows that the only
  way to get
  \begin{equation}
    \sum_{y \in \{0, 1\}^{n}}a_{y}\delta_{y} = 0
  \end{equation}
  is to have each $a_{y} = 0$. Hence the set of $\delta_{x}$ is linearly independent.
  Further, the vector space of multilinear polynomials has $2^{n}$ dimensions;
  since there are $2^{n}$ distinct $\delta_{x}$ polynomials, it follows that they
  form a basis.
\end{proof}

Now, we can use this fact to prove some cases where our low-degree extensions
turn out to have a particularly low degree. Unfortunately, these do have a lot
of qualifiers to them, but they will be useful in later theorems (in particular
\cref{lem:multiquad-adversary}).

\begin{thm}[{\cite[Theorem 4.3]{AW09}}]\label{thm:multiquad-extension}
  Let $\mathbb{F}$ be a field and $Y \subseteq \mathbb{F}^{n}$ be a set of $t$ points
  $y_{1}, \ldots, y_{t}$. Then for at least $2^{n} - t$ Boolean points
  $w \in \{0, 1\}^{n}$, there exists a multiquadratic extension polynomial
  $p: \mathbb{F}^{n} \rightarrow \mathbb{F}$ such that
  \begin{enumerate}
    \item $p(y_{i}) = 0$ for all $i \in [t]$,
    \item $p(w) = 1$,
    \item $p(z) = 0$ for all Boolean $z \ne w$.
  \end{enumerate}
\end{thm}

\begin{proof}
  % TODO
  % NOTE: AW09 Lemma 4.2 is *almost* a corollary of our Theorem 1.3.5, except
  % for the fact that not all our points are in \{0, 1\}^n
\end{proof}

% TODO: Do we want this here?
% TODO: Add preceding lemmas
% TODO: Come up with descriptive names for these things (adversary polynomials?)
\begin{lemma}[{\cite[Lemma 4.5]{AW09}}]\label{lem:multiquad-adversary}
  Let $\mathcal{F}$ be a collection of fields. Let $f: \{0, 1\}^{n} \rightarrow \{0, 1\}$ be a
  Boolean function, and for every $\mathbb{F} \in \mathcal{F}$, let
  $p_{\mathbb{F}}: \mathbb{F}^{n} \rightarrow \mathbb{F}$ be a multiquadratic polynomial
  over $\mathbb{F}$ extending $f$. Also let $\mathcal{Y}_{\mathbb{F}} \in \mathbb{F}^{n}$
  for each $\mathbb{F} \in \mathcal{F}$, and define
  $t = \sum_{\mathbb{F} \in \mathcal{F}}\abs{\mathcal{Y}_{\mathbb{F}}}$.

  Then, there exists a subset $B \subseteq \{0, 1\}^{n}$, with $\abs{B} \le t$, such that
  for all Boolean functions $f': \{0, 1\}^{n} \rightarrow \{0, 1\}$ that agree with $f$ on
  $B$, there exist multiquadratic polynomials
  $p_{\mathbb{F}}':\mathbb{F}_{n} \rightarrow \mathbb{F}$ (one for each $\mathbb{F} \in \mathcal{F}$)
  such that
  \begin{enumerate}
    \item $p_{\mathbb{F}}'$ extends $f'$, and
    \item $p_{\mathbb{F}}'(y) = p_{\mathbb{F}}(y)$ for all $y \in \mathcal{Y}_{\mathbb{F}}$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Call $z \in \{0, 1\}^{n}$ \emph{good} if for every $\mathbb{F} \in \mathcal{F}$ there exists
  a multiquadratic poylnomial $u_{\mathbb{F},z}: \mathbb{F}^{n} \rightarrow \mathbb{F}$
  such that
  \begin{enumerate}[label=\alph*.]
    \item\label{item:zero-in-y} $u_{\mathbb{F},z}(y) = 0$ for all
          $y \in \mathcal{Y}_{\mathbb{F}}$,
    \item\label{item:delta-one} $u_{\mathbb{F},z}(z) = 1$, and
    \item\label{item:delta-zero} $u_{\mathbb{F},z} = 0$ for all
          $w \in \{0, 1\}^{n} \setminus \{z\}$.
  \end{enumerate}
  % TODO: Rephrase w/more explanation
  From \cref{thm:multiquad-extension}, each $\mathbb{F} \in \mathcal{F}$ can prevent at most
  $\abs{\mathcal{Y}_{\mathbb{F}}}$ points from being good. Since
  $t = \abs{\mathcal{Y}_{\mathbb{F}}}$, there are at least $2^{n} - t$ good points.

  Let $G$ be the set of good points, and thus let $B = \{0, 1\}^{n} \setminus G$ be the
  set of not-good points. Define
  \begin{equation}\label{eqn:p-prime}
    p'_{\mathbb{F}}(x) = p_{\mathbb{F}}(x) + \sum_{z \in G}(f'(z) - f(z))u_{\mathbb{F},z}(x).
  \end{equation}
  Now, all we need is to show that $p'_{\mathbb{F}}(x)$ satisfies the two
  conditions from the theorem statement.

  First, we show that $p_{\mathbb{F}}'$ extends $f'$; that is,
  $p_{\mathbb{F}}'(x) = f'(x)$ for all $x \in \{0, 1\}^{n}$. There are two cases
  here: $x \in G$ and $x \in B$. If $x \in B$, then the sum term of \cref{eqn:p-prime}
  is $0$; hence $p'_{\mathbb{F}}(x) = p_{\mathbb{F}}(x)$. Since
  $p_{\mathbb{F}}(x)$ extends $f(x)$, and since $f(x) = f'(x)$ on $B$, this
  means $p'_{\mathbb{F}}(x) = f'(x)$. If $x \in G$, then the only term of the sum
  where $u_{\mathbb{F},z}(x)$ is nonzero is where $x = G$, as per
  \cref{item:delta-one,item:delta-zero} above. Hence, we have
  \[
    p'_{\mathbb{F}}(x) = p_{\mathbb{F}}(x) + f'(x) - f(x),
  \]
  and since $p_{\mathbb{F}}(x) = f(x)$, it follows that
  $p'_{\mathbb{F}}(x) = f'(x)$.

  Next, we show that $p_{\mathbb{F}}'(y)$ and $p_{\mathbb{F}}(y)$ agree for all
  $y \in \mathcal{Y}_{\mathbb{F}}$. Since by \cref{item:zero-in-y} above, we have that
  $u_{\mathbb{F},z}(y) = 0$, it follows that the entire sum term is zero.
  Therefore, $p'_{\mathbb{F}}(y) = p_{\mathbb{F}}(y)$ for all
  $y \in \mathcal{Y}_{\mathbb{F}}$.

  As such, we have constructed a polynomial $p'_{\mathbb{F}}$ and a set $B$ that
  satisfy our conditions of the theorem.
\end{proof}

% TODO: Unpack all that

% NOTE: JKRS09 is actually even stronger than this (we need it only to be linear
% in at least one variable)
\begin{lemma}[{\cite[Lemma 7]{JKRS09}}]\label{lem:monomial-sum}
  Let $m(x_{1}, \ldots, x_{n})$ be a multilinear monomial. Over a field of
  characteristic other than 2, we have
  \begin{equation}
    \sum_{b \in \{-1, 1\}}m(b) = 0.
  \end{equation}
\end{lemma}

\begin{proof}
  For some $x_{i}$, we can write $m = x_{i} \cdot m'$, where the degree of $x_{i}$
  in $m'$ is 0. Then
  \begin{align*}
    \sum_{b \in \{1, -1\}^{n}}m(b)
    &= \sum_{a \in \{-1, 1\}}\sum_{b' \in \{1, -1\}^{n-1}}a \cdot m'(b') \\
    &= \sum_{a \in \{-1, 1\}} a \cdot \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) \\
    &= \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) - \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) \\
    &= 0.
  \end{align*}
\end{proof}

\chapter{Relativization}

% TODO: Examples

An important prerequisite to understanding algebrization is the similar, but
simpler, concept of \emph{relativization}, also called \emph{oracle separation}.
To do this, we first must define an \emph{oracle}.
\begin{defn}[{\cite[Def.\ 2.1]{AW09}}]\label{def:oracle}\index{oracle}
  An \emph{oracle} $A$ is a collection of Boolean functions
  $A_{m}: \{0, 1\}^{m} \rightarrow \{0, 1\}$, one for each natural number $m$.
\end{defn}
There are several ways to think of an oracle; this will extend the most
naturally when it comes time to define an extension oracle in
\cref{def:ext-oracle}. Another way to think of an oracle is as a subset
$A \subseteq \{0, 1\}^{*}$. This allows us to think of $A$ as a language. Since we can
do this, it gives us the ability to think of the complexity of the oracle. If we
want to think about the subset in terms of our functions, we can write $A$ as
\begin{equation}
  A = \bigcup_{m \in \mathbb{N}}\mleft\{x \in \{0, 1\}^{m} \mid A_{m}(x) = 1\mright\}.
\end{equation}
% FIXME: Will we actually do this?
We will use the Iverson bracket defined in \cref{def:iverson-bracket} for this
purpose: allowing us to think of $A$ as the set and $[A]$ as the function.

\begin{example}\label{ex:oracle-function}
  Let $m = 3$. The function
  \begin{equation}
    \begin{aligned}
      f: \{0, 1\}^{3} &\rightarrow \{0, 1\} \\
      abc &\mapsto b
    \end{aligned}
  \end{equation}
  is an oracle function. We can think of $f$ as corresponding to the set
  $\{010, 011, 110, 111\}$.
\end{example}

\begin{example}\label{ex:oracle-full}
  For each $n \in \mathbb{N}$, define
  \begin{equation}
    \begin{aligned}
      f_{n}: \{0, 1\}^{n} &\rightarrow \{0, 1\} \\
      a_{1}a_{2} \cdots a_{n} &\mapsto a_{n}.
    \end{aligned}
  \end{equation}
  Then the set $\{f_{n}\}$ forms an oracle, whose corresponding language is the
  set of all binary representations of odd numbers.
\end{example}

% TODO: Example for oracle

An oracle is not particularly interesting mathematical object on its own (after
all, it is simply a set of arbitrary Boolean functions); its utility comes from
when it interacts with a Turing machine. A normal Turing machine does not have
the facilities to interact with an oracle, so we need to define a small
extension to a standard Turing machine to allow for this.

\begin{defn}[{\cite[Def.\ 3.6]{AB09}}]\label{def:tm-oracle}\index{Turing machine!with oracle}
  A \emph{Turing machine with an oracle} is a Turing machine with an additional
  tape, called the \emph{oracle tape}, as well as three special states:
  $q_{\text{query}}$, $q_{\text{yes}}$, and $q_{\text{no}}$. Further, each
  machine is associated with an oracle $A$. During the execution of the machine,
  if it ever moves into the state $q_{\text{query}}$, the machine then (in one
  step) takes the output of $A$ on the contents of the oracle tape, moving into
  $q_{\text{yes}}$ if the answer is 1 and $q_{\text{no}}$ if the answer is 0.
\end{defn}

Of course, the question now becomes how we can effectively use an oracle in an
algorithm. The previously-mentioned conception of an oracle as a set of strings
is useful here. If we consider the set of strings as being a \emph{language} in
its own right, then querying the oracle is the same as determining whether a
string is in the langauge, just in one step. If the language is computationally
hard, this means our machine can get a significant power boost from the right
oracle.

\begin{defn}[{\cite[Def.\ 2.1]{AW09}}]\label{def:oracle-class}
  For any complexity class $\mathcal{C}$, the complexity class $\mathcal{C}^{A}$ is the class of all
  languages determinable by a Turing machine with access to $A$ in the number of
  steps defined for $\mathcal{C}$.
\end{defn}

We will be using this definition in many places, so we should take a moment to
look at it in more depth. First, it is important to realize that $\mathcal{C}^{A}$ is a
set of \emph{languages}, not \emph{machines}: despite the notation, augmenting
$\mathcal{C}$ with an oracle does not modify any languages, it just adds new ones that are
computable. Second, since a machine can always ignore its oracle, it follows
that adding an oracle can only increase the number of languages in the class,
never decrease it.

\begin{lemma}\label{thm:relativizing-increases}
  For any complexity class $\mathcal{C}$ and oracle $A$, $\mathcal{C} \subseteq \mathcal{C}^{A}$.
\end{lemma}

\begin{proof}
  Let $L \in \mathcal{C}$ and $M$ be a machine that determines $L$. Then the oracle machine
  $M'$ that simulates $M$ on its input and makes no queries to the oracle will
  also accept exactly $L$. Since $M'$ is a $\mathcal{C}^{A}$ machine for any oracle $A$,
  it follows that $L \in \mathcal{C}^{A}$ and hence $\mathcal{C} \in \mathcal{C}^{A}$.
\end{proof}

While the above lemma tells us that $\mathcal{C} \subseteq \mathcal{C}^{A}$ always, another interesting
question is when $\mathcal{C} = \mathcal{C}^{A}$. We do have a notion for this, called
\emph{lowness}. Lowness can be defined for both individual languages and
complexity classes; we will define both here.

% TODO: Cite all these
\begin{defn}\label{def:low-class}\index{low}
  A language $L$ is \emph{low} for a class $\mathcal{C}$ if $\mathcal{C}^{L} = \mathcal{C}$.
\end{defn}

\begin{defn}\label{def:low-lang}\index{low}
  A complexity class $\mathcal{D}$ is \emph{low} for a class $\mathcal{C}$ if each language in $\mathcal{D}$
  is low for $\mathcal{C}$.
\end{defn}

Of particular interest to us will be classes that are low for \emph{themselves}.
We care about these classes because they can use other problems from the same
class as a subroutine without issue; in particular recursion and iteration both
work here. Thankfully, both $\P$ and $\PSPACE$ are low for themselves (it turns
out $\NP$ is probably not); this allows us to easily write algorithms that
recurse for classes in both of our most common classes.

\begin{thm}\label{thm:p-low}
  $\P$ is low for itself.
\end{thm}

\begin{proof}
  % TODO: Rewrite?
  Let $L \in \P$ and let $K \in \P^{L}$. Let $M(L)$ be the determiner of $L$ and
  $M(K)$ be the determiner of $K$. Further, let $\hat{M}(K)$ be the determiner
  of $K$ but with access to $L$ as an oracle. We aim to show $K \in \P$. Let
  $p_{L}(n)$ be a polynomial upper bound of the runtime of $M(L)$ on an input of
  length $n$, and let $p_{\hat{K}}(n)$ be similar. Since $M(K)$ can call $M(L)$ no
  more than $p_{\hat{K}}(n)$ times, it follows that
  $p_{K}(n) \le p_{\hat{K}}(p_{L}(n))$. Hence, the runtime of $M(K)$ is bounded
  above by a polynomial, and thus $K \in P$.
\end{proof}

\begin{thm}\label{thm:pspace-low}
  $\PSPACE$ is low for itself.
\end{thm}

\begin{proof}
  The proof is very similar to that for \cref{thm:p-low}, but with space instead
  of time.
  % TODO
\end{proof}

\section{Defining relativization}

We are now ready to define what relativization is. First, note that
relativization is a statement about a \emph{result}: we talk about inclusions
relativizing, not sets themselves.

% TODO: Cite
\begin{defn}\label{def:relativization}\index{relativization}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \subseteq \mathcal{D}$. We say the result
  $\mathcal{C} \subseteq \mathcal{D}$ \emph{relativizes} if $\mathcal{C}^{A} \subseteq \mathcal{D}^{A}$ for all oracles $A$. Conversely,
  if there exists $A$ such that $\mathcal{C} \nsubseteq \mathcal{D}$, we say that the result $\mathcal{C} \subseteq \mathcal{D}$
  \emph{does not relativize}.
\end{defn}

\begin{defn}\label{def:relativization-ne}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \nsubseteq \mathcal{D}$. We say the result
  $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{relativizes} if $\mathcal{C}^{A} \nsubseteq \mathcal{D}^{A}$ for all oracles $A$. Conversely,
  if there exists $A$ such that $\mathcal{C} \subseteq \mathcal{D}$, we say that the result $\mathcal{C} \nsubseteq \mathcal{D}$
  \emph{does not relativize}.
\end{defn}

We start with a very straightforward example of a relativizing result.

\begin{lemma}\label{lem:pa-subset-npa}
  For any oracle $A$, $\P^{A} \subseteq \NP^{A}$. Equivalently, the result $\P \subseteq \NP$
  relativizes.
\end{lemma}

\begin{proof}
  Since any deterministic Turing machine is also a nondeterministic machine, it
  follows that a machine that solves a $\P^{A}$ problem is also an $\NP^{A}$
  machine. Hence, $\P^{A} \subseteq \NP^{A}$.
\end{proof}

This result tells us that not \emph{everything} is weird in the world of
relativization (although we will soon do our best to find all the weird bits):
if we have a machine that can do more operations without an oracle, it can still
do more operations with an oracle. Further, for the question of $\P$ vs.\ $\NP$
that we will discuss in \cref{sec:rel-p-np}, this means that the question we
care about is whether $\NP \subseteq^{?} \P$ relativizes. As such, the question we are
asking simplifies to determining where $\P^{A} = \NP^{A}$ and where
$\P^{A} \subsetneq \NP^{A}$.

Now that we have talked about set inclusions relativizing, we need to define the
other side of the coin: \emph{proofs} can relativize as well as results.
Unfortunately, this needs to be a somewhat informal definition as formally
delineating different types of proof is far beyond the scope of this paper.
However, the definition we offer here will be sufficient for our purposes.

\begin{defn}\label{def:relativizing-result}
  We say a \emph{proof relativizes} if it is not made invalid if the relevant
  classes are replaced with oracle classes, i.e., a proof that $\mathcal{C} \subseteq \mathcal{D}$
  \emph{relativizes} if the same proof can be used to show $\mathcal{C}^{A} \subseteq \mathcal{D}^{A}$ for
  all oracles $A$ with minimal modifications.
\end{defn}

This gives us a reason to care about relativization as a concept: if our proofs
are relativizing then we know not to try to use them to prove nonrelativizing
results. In particular, we will show in \cref{sec:rel-p-np} that the famous $\P$
vs.\ $\NP$ problem will not relativize regardless of the outcome, and then in
\cref{sec:diag-relativizes} we will show that the common proof technique of
diagonalization \emph{does} in fact relativize.

Now that we have given ourselves a reason to care about oracles and how they
interact with Turing machines, we now turn to the question of how a machine can
gain information about the oracle it queries. We will do this with the notion of
\emph{query complexity}.

\section{Query complexity}\label{sec:query-complexity}

The goal of query complexity is to ask questions about some Boolean function
$A: \{0, 1\}^{n} \rightarrow \{0, 1\}$ by querying $A$ itself. For this, we will
interchangeably think of $A$ as a \emph{function} as well as a bit string of
length $N = 2^{n}$, where each string element is $A$ applied to the $i$th string
of length $n$, arranged in some lexicographical order. % TODO: Better way to phrase this
We can further think of the property itself as being a Boolean function; a
function that takes as input the bit-string representation of $A$ and outputs
whether or not $A$ has the given property. We will call the function
representing the property $f$. When viewed like this, $f$ is a function from
$\{0, 1\}^{N}$ to $\{0, 1\}$. We define three types of query complexity for
three of the most common types of computing paradigms: deterministic,
randomized, and quantum. Nondeterministic query complexity is interesting, but
it is outside the scope of this paper.
% TODO: Why on earth does this paper not define nondeterministic query complexity?

% TODO: Find better source for these definitions
% Perhaps rephrase in the style of AW09 Def. 4.1?
\begin{defn}[{\cite[17]{AW09}}]\label{def:det-qc}\index{query complexity!deterministic}
  Let $f: \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{deterministic query complexity} of $f$, which we write $D(f)$, is the
  minimum number of queries made by any deterministic algorithm with access to
  an oracle $A$ that determines the value of $f(A)$.
  % TODO: I don't quite understand the phrasing here; perhaps rephrase
\end{defn}

To make this more clear, let us give an example problem.

\begin{defn}\label{def:or-problem}\index{OR@$\mathsf{OR}$}
  The $\mathsf{OR}$ problem is the following oracle problem:
  \begin{quote}
    Let $A: \{0, 1\}^{n} \rightarrow \{0, 1\}$ be an oracle. The function $\mathsf{OR}(A)$
    returns 1 if there exists a string on which $A$ returns 1, and $0$
    otherwise.
  \end{quote}
\end{defn}

The question is then what the deterministic query complexity of the
$\mathsf{OR}$ function is.

\begin{thm}
  The $\mathsf{OR}$ problem has a deterministic query complexity of $2^{n}$.
\end{thm}

\begin{proof}
  First, note that any algorithm that determines the $\mathsf{OR}$ problem can
  stop as soon as it queries $A$ and gets an output of $1$. Hence, for any
  algorithm $M$, let $\{s_{i}\}$ be the sequence of queries $M$ makes to $A$ on
  the assumption that it always recieves a response of $0$. If
  $\abs{\{s_{i}\}} \le 2^{n}$, there exists some $s \in \{0, 1\}^{n}$ not queried.
  In that case, $M$ will not be able to distinguish the zero oracle from the
  oracle that outputs $1$ only when given $s$. Hence, $M$ must query every
  string of length $n$ and thus the query complexity is $2^{n}$.
\end{proof}

From this, we get that the $\mathsf{OR}$ problem cannot be solved any better
than by enumerative checking. This makes intuitive sense because none of the
results we get by querying $A$ imply anything about what $A$ will do on other
values, since $A$ can be an arbitrary function. Later on (in
\cref{sec:alg-query-complexity}), we will look at what happens when we give
ourselves access to a \emph{polynomial}, where querying one point could tell us
information about others.

For the next two definitions, since their Turing machines include some element
of randomness, we only require that they succeed with a $2/3$ probability. This
is in line with most definitions of complexity classes involving random
computers.

\begin{defn}[{\cite[17]{AW09}}]\label{def:rand-qc}\index{query complexity!randomized}
  Let $f: \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{randomized query complexity} of $f$, which we write $D(f)$, is the
  minimum number of queries made by any randomized algorithm with access to an
  oracle $A$ that evaluates $f(A)$ with probability at least $2/3$.
\end{defn}

% TODO: Talk about how quantum oracles are weird?

\begin{defn}[{\cite[17]{AW09}}]\label{def:quant-qc}\index{query complexity!quantum}
  Let $f: \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{quantum query complexity} of $f$, which we write $D(f)$, is the
  minimum number of queries made by any quantum algorithm with access to an
  oracle $A$ that evaluates $f(A)$ with probability at least $2/3$.
\end{defn}
% TODO: Examples

\section{Relativization of $\P$ vs.\ $\NP$}\label{sec:rel-p-np}

% TODO? move to relativization section as an example?
An important example of relativization is that of $\P$ and $\NP$. While the
question of if $\P = \NP$ is still open, we aim to show that \emph{regardless of
the answer}, the result does not algebrize. To do this, we show that there are
some oracles $A$ where $\P^{A} = \NP^{A}$, and some where $\P^{A} \ne \NP^{A}$.

Additionally, it should be noted that the similarity of relativization to
algebrization means that the structure of these proofs will return in
\cref{sec:alg-p-np} when we show the algebrization of $\P$ and $\NP$.

\subsection{Equality}

The more straightforward of the two proofs is the oracle where
$\P^{A} = \NP^{A}$, so we shall begin with that.

\begin{thm}[{\cite[Theorem 2]{BGS75}}]\label{thm:p-np-rel}
  There exists an oracle $A$ such that $\P^{A} = \NP^{A}$.
\end{thm}

\begin{proof}
  For this, we can let $A$ be any $\PSPACE$-complete language. By letting our
  machine in $\P$ be the reducer from $A$ to any other language in $\PSPACE$, we
  therefore get that $\PSPACE \subseteq \P^{A}$. Similarly, if we have a problem in
  $\NP^{A}$, we can verify it in polynomial space without talking to $A$ at all
  (by having our machine include a determiner for $A$). Hence, we have that
  $\NP^{A} \subseteq \NPSPACE$. Further, a celebrated result of Savitch~\cite{Sav70}
  (which we briefly discussed as \cref{thm:savitch}) is that
  $\PSPACE = \NPSPACE$. Combining all these results, we get the chain
  \begin{equation}
    \NP^{A} \subseteq \NPSPACE = \PSPACE \subseteq \P^{A} \subseteq \NP^{A}.
  \end{equation}
  This is a circular chain of subset relations, which means everything in the
  chain must be equal. Hence, $\P^{A} = \NP^{A} = \PSPACE$.
\end{proof}

For a slightly more intuitive view of what this proof is doing, what we have
done is found an oracle that is so powerful that it dwarfs any amount of
computation our actual Turing machine can do. Hence, the power of our machine is
really just the same as the power of our oracle, and since we have given both
the $\P$ and $\NP$ machine the same oracle, they have the same power.

\subsection{Inequality}

Having shown that an oracle exists where $\P^{A} = \NP^{A}$, we now endeavor to
find one where $\P^{A} \ne \NP^{A}$. This piece of the proof is less simple than
the previous section, and it uses a diagonalization argument to construct the
oracle. Before we dive in to the main proof, however, we need to define a few
preliminaries.

\begin{defn}[{\cite[436]{BGS75}}]\label{def:l(x)}\index{L(X)@$L(X)$}
  Let $X$ be an oracle. The language $L(X)$ is the set
  \begin{equation*}
    L(X) = \{x \mid \text{there is } y \in X \text{ such that } \abs{y} = \abs{x}\}.
  \end{equation*}
\end{defn}

\begin{example}\label{ex:l(x)-simple}
  Consider the language $X = \{0, 11, 0100\}$. The language $L(X)$ is the
  language consisting of all strings of length $1$, $2$, and $4$.
\end{example}

Our eventual goal will be to construct a language $X$ such that
$L(X) \in \NP^{X} \setminus \P^{X}$. Of particular note is that we can rather nicely put a
upper bound on the complexity of $L(X)$ when given $X$ as an oracle, regardless
of the value of $X$. This fact is what gives us the freedom to construct $X$ in
such a way that $L(X)$ will not be in $\P^{X}$.

\begin{lemma}[{\cite[436]{BGS75}}]\label{lem:l(x)-in-np}
  For any oracle $X$, $L(X) \in \NP^{X}$.
\end{lemma}

\begin{proof}
  Let $S$ be a string of length $n$. If $S \in L(X)$, then a witness for $S$ is
  any string $S'$ such that $\abs{S} = \abs{S'}$ and $S' \in X$. Since a machine
  with query access to $X$ can query whether $S'$ is in $X$ in one step, it
  follows that we can verify that $S \in L(X)$ in polynomial time.
\end{proof}

With this lemma as a base, we can now move on to our main theorem.

\begin{thm}[{\cite[Theorem 3]{BGS75}}]\label{thm:p-np-nrel}
  There exists an oracle $A$ such that $\P^{A} \ne \NP^{A}$.
\end{thm}

\begin{proof}
  Our goal is to construct a set $B$ such that $L(B) \notin \P^{B}$. We shall
  construct $B$ in an interative manner. We do this by taking a sequence
  $\{P_{i}\}$ of all machines that recongize some language in $\P^{A}$, and then
  constructing $B$ such that for each machine in the sequence, there is some
  part of $L(B)$ it cannot recognize. This technique is called
  \emph{diagonalization}, and it is used in many places in computer science
  theory.\footnote{This argument style is named after \emph{Cantor's diagonal
      argument}, which was originally used to prove that the real numbers are
    uncountable~\cite[Thm. 2.14]{Ru76}.} Additionally, we define $p_{i}(n)$ to
  be the maximum running time of $P_{i}$ on an input of length $n$. We give the
  following algorithm to construct $B$:

  \begin{algorithm}[H]
    % FIXME: Do the P_i need to be P machines? Everybody is unclear on this
    \KwIn{A sequence of $\P$ oracle machines $\{P_{i}\}_{i=1}^{\infty}$}
    \KwOut{A set $B$ such that $L(B) \notin \P^{B}$}
    % TODO: Define p_i(n)
    $B(0) \leftarrow \varnothing$\;
    $n_{0} \leftarrow 0$\;
    \For{$i$ starting at $1$}{
      Let $n > n_{i}$ be large enough that
      $p_{i}(n) < 2^{n}$\;\nllabel{line:def-n}
      Run $P_{i}^{B(i-1)}$ on input $0^{n}$\;\nllabel{line:computation}
      \If{$P_{i}^{B(i-1)}$ rejects $0^{n}$}{
        Let $x$ be a string of length $n$ not queried during the above
        computation\;\nllabel{line:not-queried}
        $B(i) \leftarrow B(i-1) \sqcup \{x\}$\;
      }
      $n_{i+1} \leftarrow 2^{n}$\;
    }
    $B \leftarrow \bigcup_{i}B(i)$\;
    \caption{An algorithm for constructing $B$}\label{alg:construct-b}
  \end{algorithm}

  Now that we have presented the algorithm, let us demonstrate its soundness.
  First, note that since $P_{i}$ runs in polynomial time, $p_{i}(n)$ is bounded
  above by a polynomial, and hence there will always exist an $n$ as defined in
  line~\ref{line:def-n}. Next, since there are $2^{n}$ strings of length $n$ and
  since $p_{i}(n) < 2^{n}$, we know that there must be some $x$ to make
  line~\ref{line:not-queried} well-defined. While our algorithm allows $x$ to be
  any string, if it is necessary to be explicit in which we choose, then picking
  $x$ to be the smallest string in lexicographic order is a standard choice.

  We should also briefly mention that this algorithm does not terminate. This is
  okay because we are only using it to construct the set $B$, which does not
  need to be bounded. If this were to be made practical, since the sequence of
  $n_{i}$s is monotonically increasing, the set could be constructed ``lazily''
  on each query by only running the algorithm until $n_{i}$ is greater than the
  length of the query.

  % FIXME: Should this "end goal" section be moved to before the algorithm?
  Next, we demonstrate that $L(B) \notin \P^{B}$. The end goal of our instruction is
  a set $B$ such that if $P_{i}^{B}$ accepts $0^{n}$ then there are no strings
  of length $n$ in $B$, and if $P_{i}^{B}$ rejects, then there is a string of
  length $n$ in $B$. This means that no $P_{i}$ accepts $L(B)$, and hence
  $L(B) \notin \NP^{B}$.

  The central idea behind the proper functioning of our algorithm is that adding
  strings to our oracle \emph{cannot change the output if they are not queried}.
  This is what we do in line~\ref{line:def-n}: we need our input length to be
  long enough to guarantee that a non-queried string exists. Since the number of
  queried strings is no greater than $p_{i}(n)$, and there are $2^{n}$ strings
  of length $n$, there must be some string not queried.

  Next, we run $P_{i}^{B(i-1)}$ on all the strings we have already added. If it
  accepts, then we want to make sure that no string of length $n$ is in $B$;
  that is, $0^{n}$ is not in $L(B)$. Hence, in this particular loop we add
  nothing to $B(i)$. If $P_{i}^{B(i-1)}$ rejects, we then need to make sure that
  $0^{n} \in L(B)$ but in a way that does not affect the output of
  $P_{i}^{B(i-1)}$. Hence, we find a string that $P_{i}^{B(i-1)}$ did not query
  (and thus will not affect the result) and add it to $B(i)$.

  Having done this, we then set $n_{i+1}$ to be $2^{n}$. Since
  $p_{i}(n) < 2^{n}$, it follows that no previous machine could have queried any
  strings of length $n_{i+1}$.\footnote{A word of caution: we only care about
    what $P_{i}$ does on input $n_{i}$, \emph{not any other input}. This is
    because we only need each machine to be incorrect for some $i$, not all
    $i$.} This way, we ensure our previous machines do not accidentally have
  their output change due to us adding a string they queried.

  % TODO: Note about how it's fine that this doesn't actually halt b/c it's just
  % in the construction of the set
  Having run this over all polynomial-time Turing machines, we have a set $L(B)$
  such that no machine in $\P^{B}$ accepts it, which tells us $L(B) \notin \P^{B}$.
  But, \cref{lem:l(x)-in-np} already told us $L(B) \in \NP^{B}$. Hence,
  $\P^{B} \ne \NP^{B}$.
\end{proof}

\section{Diagonalization relativizes}\label{sec:diag-relativizes}

Of course, determining that $\P$ vs $\NP$ does not relativize is only important
if the proof techniques used in practice \emph{do} in fact relativize. Rather
unfortunately, it turns out that simple diagonalization is a relativizing
result.

% FIXME: Are there formal definitions of diagonalization?
While diagonalization itself does not have a formal definition, we can still
think about it informally. Looking at our construction of $B$, which we did
using diagonalization, notice that our definition never really cared about how
the $P_{i}$ worked, just about the results it produced. Hence, if it were to be
possible to modify \cref{alg:construct-b} to construct $B \in \NP \setminus \P$, the proof
would remain the same if we were to replace our sequence $\{P_{i}\}$ with a
sequence of machines in $\P^{A}$ for some $\PSPACE$-complete $A$. However, this
would lead to a contradiction, as we showed in \cref{thm:p-np-rel} that in that
case, $\P^{A} = \NP^{A}$! This tells us that a simple diagonalization argument
would not suffice to determine separation between $\P$ and $\NP$.

\chapter{Algebrization}\label{chap:algebrization}

Algebrization, originally described by Aaronson and Wigderson~\cite{AW09}, is an
extension of relativization. While relativization deals with oracles that are
Boolean functions, algebrization extends oracles to be a collection of
polynomials over finite fields. Since any field contains the set $\{0, 1\}$, we
can think about our new oracles as \emph{extending} some specific oracle $A$, so
that both oracles agree on the set $\{0, 1\}^{n} \subseteq \mathbb{F}^{n}$. We formalize
this notion below.

\begin{defn}[{\cite[Def.\ 2.2]{AW09}}]\label{def:ext-oracle}\index{extension oracle}
  % TODO: Reuse definition of extension polynomial from earlier
  Let $A_{m}: \{0, 1\}^{m} \rightarrow \{0, 1\}$ be a Boolean function and let
  $\mathbb{F}$ be a finite field. Then an \emph{extension} of $A_{m}$ over
  $\mathbb{F}$ is a polynomial
  $\tilde{A}_{m,\mathbb{F}}: \mathbb{F}^{m} \rightarrow \mathbb{F}$ such that
  $\tilde{A}_{m,\mathbb{F}}(x) = A_{m}(x)$ whevever $x \in \{0, 1\}^{m}$. Also,
  given an oracle $A = (A_{m})$, an \emph{extension} $\tilde{A}$ of $A$ is a
  collection of polynomials
  $\tilde{A}_{m,\mathbb{F}}: \mathbb{F}^{m} \rightarrow \mathbb{F}$, one for each positive
  integer $m$ and finite field $\mathbb{F}$, such that
  \begin{enumerate}
    \item $\tilde{A}_{m,\mathbb{F}}$ is an extension of $A_{m}$ for all
          $m,\mathbb{F}$, and
    \item there exists a constant $c$ such that
          $\mdeg(\tilde{A}_{m,\mathbb{F}}) \le c$ for all $m, \mathbb{F}$.
          % TODO: Rephrase point 2 in terms of F[x_{1,...,n}^{<= c}]
  \end{enumerate}
\end{defn}

Take note that an oracle can have many different extension oracles, since one
can construct an infinite number of polynomials that go through a set of points.
For this reason, when dealing with oracles in practice, we will also often be
interested in oracles of a particular multidegree, which limits our options for
oracles in potentially-interesting ways.

\begin{example}\label{ex:oracle-function-ext}
  Consider the function we defined in \cref{ex:oracle-function}:
  \begin{equation}
    \begin{aligned}
      f: \{0, 1\}^{3} &\rightarrow \{0, 1\} \\
      abc &\mapsto b.
    \end{aligned}
  \end{equation}
  An extension of that function is the polynomial
  \begin{equation}
    \begin{aligned}
      \tilde{f}: \mathbb{F}^{3} &\rightarrow \mathbb{F}^{3} \\
      (a,b,c) &\mapsto b.
    \end{aligned}
  \end{equation}
  While this is a relatively trivial polynomial, there are more non-trivial
  ones, for example
  \begin{equation}
    \begin{aligned}
      \tilde{f}: \mathbb{F}^{3} &\rightarrow \mathbb{F}^{3} \\
      (a,b,c) &\mapsto a^{3}c^{3} + b^{2} - ac.
    \end{aligned}
  \end{equation}
  Notice that on $\{0, 1\}$, $x^{2} = x$, which allows us to see that
  $\tilde{f}$ is a valid extension of $f$.
\end{example}

% TODO: Example of an extension oracle

\begin{defn}[{\cite[Def.\ 2.2]{AW09}}]\label{def:ext-oracle-class}
  For any complexity class $\mathcal{C}$ and extension oracle $\tilde{A}$, the complexity
  class $\mathcal{C}^{\tilde{A}}$ is the class of all languages determinable by a Turing
  machine with access to $\tilde{A}$ with the requirements for $\mathcal{C}$.
\end{defn}

Next, we need to formally define what algebrization is.

\begin{defn}[{\cite[Def.\ 2.3]{AW09}}]\label{def:algebrization}\index{algebrization}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \subseteq \mathcal{D}$. We say the result
  $\mathcal{C} \subseteq \mathcal{D}$ \emph{algebrizes} if $\mathcal{C}^{A} \subseteq \mathcal{D}^{\tilde{A}}$ for all oracles $A$ and
  finite field extensions $\tilde{A}$ of $A$. Conversely, if there exists $A$
  and $\tilde{A}$ such that $\mathcal{C} \nsubseteq \mathcal{D}$, we say that the result $\mathcal{C} \subseteq \mathcal{D}$ \emph{does
    not algebrize}.
\end{defn}

\begin{defn}[{\cite[Def.\ 2.3]{AW09}}]\label{def:algebrization-neq}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \nsubseteq \mathcal{D}$. We say the result
  $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{algebrizes} if $\mathcal{C}^{A} \nsubseteq \mathcal{D}^{\tilde{A}}$ for all oracles $A$ and
  finite field extensions $\tilde{A}$ of $A$. Conversely, if there exists $A$
  and $\tilde{A}$ such that $\mathcal{C} \subseteq \mathcal{D}$, we say that the result $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{does
    not algebrize}.
\end{defn}

\section{Algebraic query complexity}\label{sec:alg-query-complexity}

Similarly to how we defined query complexity in \cref{sec:query-complexity}, our
notion of algebrization requires a definition of \emph{algebraic} query
complexity. % TODO: More (connect to previous section)

\begin{defn}[{\cite[Def. 4.1]{AW09}}]\label{def:aqc}\index{query complexity!algebraic}
  Let $f: \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function, $\mathbb{F}$ be a
  field, and $c$ be a positive integer. Also, let $\mathbb{M}$ be the set of
  deterministic algorithms $M$ such that $M^{\tilde{A}}$ outputs $f(A)$ for
  every oracle $A: \{0, 1\}^{n} \rightarrow \{0, 1\}$ and every finite field extension
  $\tilde{A}: \mathbb{F}^{n} \rightarrow \mathbb{F}$ of $A$ with $\mdeg(\tilde{A}) \le c$.
  Then, the deterministic algebraic query complexity of $f$ over $\mathbb{F}$ is
  defined as
  \begin{equation}
    \tilde{D}_{\mathbb{F}, c}(f) = \min_{M \in \mathcal{M}}\mleft(
      \max_{A, \tilde{A}: \mdeg(\tilde{A}) \le c}T_{M}(\tilde{A})
    \mright),
  \end{equation}
  where $T_{M}(\tilde{A})$ is the number of queries to $\tilde{A}$ made by
  $M^{\tilde{A}}$.
  % TODO: Can this be made more intelligible?
\end{defn}

Our goal here is to find the \emph{worst}-case scenario for the \emph{best}
algorithm that calculates the property $f$. The difference between this and
\cref{def:det-qc} is twofold: first, our algorithm $M$ has access to
an extension oracle of $A$, and second, that we can limit our $\tilde{A}$ in
its maximum multidegree. For the most part, we will focus on equations with
multidegree 2, which is enough to get the results we want.

As an example, let us look at the same $\mathsf{OR}$ problem we defined in
\cref{def:or-problem}.

% FIXME:
\begin{thm}[{\cite[Thm.\ 4.4]{AW09}}]\label{thm:or-algebraic}
  $\tilde{D}_{\mathbb{F},2}(\mathsf{OR}) = 2^{n}$ for every field $\mathbb{F}$.
\end{thm}

\begin{proof}
  % TODO
\end{proof}

This gives us a potentially counterintuitive property of algebraic query
complexity: while it would seem that giving our machine a polynomial (and a
polynomial of multidegree only 2, at that) would give us the ability to solve
the hardest problems more quickly, that turns out not to be the case.

Now, while this is true for polynomials of multidegree 2, it turns out that if
we restrict our oracles to being simply \emph{multilinear} polynomials, we do
get a speedup.

\begin{thm}[{\cite[Thm. 3]{JKRS09}}]\label{thm:or-multilinear}
  $\tilde{D}_{\mathbb{F},1}(\mathsf{OR}) = 1$ for every field $\mathbb{F}$ with
  characteristic not equal to $2$.
\end{thm}

\begin{proof}
  % TODO
  Let $A: \{0, 1\}^{n} \rightarrow \{0, 1\}$ and $\tilde{A}$ be our extension polynomial.
  Consider the value of $p(1/2, \ldots, 1/2)$. We aim to show that this value is
  equal to $0$ if and only if $A$ is the zero oracle.

  Consider the function
  \begin{equation}
    p'(x_{1}, \ldots, x_{n}) = p(1 - 2x_{1}, \ldots, 1 - 2x_{n}).
  \end{equation}
  Since $1 - 2x$ is a linear polynomial, it follows that $p'$ is itself a
  multilinear polynomial. Further, since the sum over $\{1, -1\}^{n}$ of a
  non-constant multilinear monomial is 0 as per \cref{lem:monomial-sum}, it
  follows that
  \begin{equation}
    \sum_{b \in \{-1, 1\}^{n}}p'(b) = p'(0, \ldots, 0),
  \end{equation}
  i.e., the constant term of $p'$. Further, from our definition of $p'$, we have
  that $p'(0, \ldots, 0) = p(1/2, \ldots, 1/2)$. Hence, we have
  \begin{equation}
    \sum_{b \in \{0, 1\}^{n}}p(b) = p(1/2, \ldots, 1/2).
  \end{equation}
  Since $p(b) \ge 0$ for all $b \in \{0, 1\}^{n}$, it follows that $p(1/2, \ldots, 1/2)$
  is 0 if and only if $p(b) = 0$ for all $b \in \{0, 1\}^{n}$, i.e. exactly when
  $A$ is the zero function.
\end{proof}

% TODO: Deterministic & nondeterministic AQC
% TODO: Work through the result of Juma et al. (JKRS09) that the OR problem can
% be solved with 1 query for multilinear polynomials? More generally, perhaps
% use the JKRS proofs instead of the AW proofs if they're cleaner?
% Ok yeah, having read it, I definitely want JKRS09 Thm 3 b/c it's so slick

\section{Algebrization of $\P$ vs.\ $\NP$}\label{sec:alg-p-np}

As with relativization, an important application of algebrization is in regards
to the $\P$ vs.\ $\NP$ problem.

\begin{defn}[{\cite[Def.\ 6.1]{BFL90}}]\label{def:pspace-robust}\index{PSPACE-robust@$\PSPACE$-robust}
  A language $L$ is \emph{$\PSPACE$-robust} if $\P^{L} = \PSPACE^{L}$.
\end{defn}

% TODO: Move up above thm:p-np-rel so I can reference it there?
\begin{lemma}\label{lem:complete-is-robust}
  Any $\PSPACE$-complete language is also $\PSPACE$-robust.
\end{lemma}

\begin{proof}
  First, we know from \cref{thm:relativizing-increases} that
  $\P^{L} \subseteq \PSPACE^{L}$. Next, let $M \in \PSPACE^{L}$, and we aim to show
  $M \in \P^{L}$. Since $L \in \PSPACE$ and $\PSPACE$ is low for itself, we know
  $M \in \PSPACE$. As such, we know there is a polynomial-time reduction $f$ from
  $M$ to $L$. Hence, we can compute $M$ by running $f$ on the input and then
  testing if that output is in $L$ (using the oracle). Hence, $M \in \P^{L}$ and
  thus $\P^{L} = \PSPACE^{L}$.
\end{proof}

\begin{lemma}[{\cite[Lemma 6.2]{BFL90}}]\label{lem:multilinear-is-pspace}
  Let $L$ be a $\PSPACE$-robust language, with corresponding oracle $A$. Let
  $\tilde{A}$ be the unique multilinear extension oracle of $A$. Then the
  language
  \begin{equation}
    \tilde{L} = \bigcup_{n \in \mathbb{N}}\{(x_{1}, \ldots, x_{n}, z) \in \mathbb{F}^{n+1} \mid \tilde{A}(x_{1}, \ldots, x_{n}) = z\}
  \end{equation}
  is polynomially-equivalent to $L$; that is, $\tilde{L} \in \P^{L}$ and
  $L \in \P^{\tilde{L}}$.
\end{lemma}

The proof of this statement originally given in~\cite{BFL90} has some apparent
problems; we discuss these more thoroughly later on in \cref{app:bug-in-pspace}.
Instead, we present our own proof of the above lemma.

\begin{proof}
  First, we provide a polynomial-time reduction from $L$ to $\tilde{L}$. Since
  for all $x \in \{0, 1\}^{n}$, $\tilde{A}(x) = 1$ if and only if $x \in L$, it
  follows that
  \begin{equation}
    \begin{aligned}
      f: \Sigma^{*} &\rightarrow \Sigma^{*} \\
      x &\mapsto (x, 1)
    \end{aligned}
  \end{equation}
  is a polynomial-time reduction from $L$ to $L'$.

  \begin{algorithm}[H]
    \KwIn{$(x_{1}, \ldots, x_{n}, z) \in \mathbb{F}^{n+1}$}
    \KwOut{Whether $\tilde{A}(x_{1}, \ldots, x_{n}) = z$}
    $z' \leftarrow 0$\;
    \For{$k \in \{0, 1\}^{n}$}{\nllabel{line:for-z-prime}
      Simulate $L$ on input $k$\;
      \If{$k \in L$}{
        \tcp{Compute $d_{k}(x)$}
        $d \leftarrow 1$\;
        \For{$i$ from $1$ to $n$}{\nllabel{line:for-d}
          \eIf{$k_{i} = 1$}{
            $d \leftarrow d \cdot x_{i}$\;
          }{
            $d \leftarrow d \cdot (1 - x_{i})$\;
          }
        }
        $z' \leftarrow z' + d$\;
      }
    }
    \Return{whether $z = z'$}\;
    \caption{Determiner for $\tilde{L}$}\label{alg:l-tilde-det}
  \end{algorithm}

  This algorithm simply calculates the value of $\tilde{A}(x_{1}, \ldots, x_{n})$
  directly, from the explicit definition we gave in
  \cref{cor:low-degree-boolean}, and then compares it to the value of $z$.

  First, we demonstrate that the above algorithm runs in $\P^{L}$. From the
  definition of $\PSPACE$-robustness, we know that we only need to show that the
  algorithm runs in $\PSPACE^{L}$, a much weaker bound. The inner for-loop runs
  in polynomial \emph{time}, hence it must run in polynomial space. The outer
  for-loop runs for $2^{n}$ iterations, so determining that it is in $\P^{L}$ is
  non-trivial. Beyond the inner loop (which we have already discussed), the only
  thing we do in the outer loop is simulate $L$, which can be done in one step
  with access to an oracle for $L$.

  The only memory we need to simulate this oracle (beyond that for the input) is
  space for $d$ and $z'$. We have already shown $d$ needs polynomial space, so
  what remains is $z'$. Since $A(x_{1}, \ldots, x_{n}) \in \{0, 1\}$, each term in the
  sum in \cref{eqn:low-deg-ext-small} is bounded above by $\delta_{\beta}(x)$. This means
  that the value of $z'$ that we compute is bounded above by
  \begin{equation}
    2^{n}\max_{k \in \{0, 1\}^{n}}\delta_{k}(x).
  \end{equation}
  Since each $\delta_{k}(x)$ can be written in polynomial space, and $2^{n}$ can be
  \emph{written} in polynomial space, it follows that $z'$ can as well. Hence,
  \cref{alg:l-tilde-det} is in $\PSPACE^{L}$, and thus is in $\P^{L}$.

  Next, we show that \cref{alg:l-tilde-det} determines $\tilde{L}$. As mentioned
  earlier, our algorithm computes $\tilde{A}$ directly through the equations
  given in \cref{cor:low-degree-boolean}. First, we show the inner loop
  (beginning on line~\ref{line:for-d}) computes $\delta_{k}(x)$. We compute $\delta$
  directly, through the formula described at \cref{eqn:delta-poly-small}. We do
  this by simply iterating through each $i$ and then multiplying $d$ by either
  $x_{i}$ or $1-x_{i}$, as appropriate.

  Second, in this case \cref{eqn:low-deg-ext-small} simplifies to
  \begin{equation}
    \tilde{A}_{n}(x_{1}, \ldots, x_{n}) = \sum_{\beta \in L}\delta_{\beta}(x_{1}, \ldots, x_{n}).
  \end{equation}
  This is exactly what our outer loop does: computes the sum directly through
  iteration.
  Hence, the only thing the above algorithm does is calculate
  $\tilde{A}_{n}(x_{1}, \ldots, x_{n})$ and then compares it to the value we were
  given. As such, it determines $\tilde{L}$.

  Since there is a reduction from $L$ to $\tilde{L}$, we know that $L$ is no
  harder than $\tilde{L}$, and \cref{alg:l-tilde-det} demonstrates that
  $\tilde{L} \in \PSPACE$. Hence, $\tilde{L}$ is $\PSPACE$-complete.
\end{proof}

With that as a base, we can now move on to the main theorem. As before, the more
straightforward proof is the oracle where $\P^{\tilde{A}} = \NP^{A}$, so we
begin with that.

\begin{thm}[{\cite[Theorem 5.1]{AW09}}]\label{thm:p-np-alg}
  There exist $A$, $\tilde{A}$ such that $\NP^{A} = \P^{\tilde{A}}$.
\end{thm}

\begin{proof}
  % TODO: Pull the lemma from BFL into this paper
  For this theorem, we use the same technique we did in our proof of
  \cref{thm:p-np-rel}: find a $\PSPACE$-complete language $A$ and work from
  there. If we let $\tilde{A}$ be the unique multilinear extension of $A$,
  \cref{lem:multilinear-is-pspace} tells us $\tilde{A}$ is $\PSPACE$-complete.
  Hence, reusing our argument from \cref{thm:p-np-rel}, we have
  \begin{equation}
    \NP^{\tilde{A}} = \NP^{\PSPACE} = \PSPACE = \P^{A}.
  \end{equation}
  % TODO: Don't just say "reusing our argument": actually spell it out
  % NOTE: This uses BFL90 which introduces concepts of MIP & such; how do we
  % work that in with the MIP work later in the thesis?
\end{proof}

Now it is time for the other case.

\begin{thm}[{\cite[Theorem 5.3]{AW09}}]\label{thm:p-np-nalg}
  There exist $A$, $\tilde{A}$ such that $\NP^{A} \ne \P^{\tilde{A}}$.
\end{thm}

\begin{proof}
  Like in \cref{thm:p-np-nrel}, we aim to ``diagonalize'': iterate over all
  $\P^{\tilde{A}}$ machines to construct a language that none of them can
  recognize. Also like before, we will do this by constructing an oracle
  extension $\tilde{A}$ such that $L(A) \notin \P^{\tilde{A}}$. Since we only give an
  algebraic extension to $\P$ and not $\NP$, we can resuse the result from
  \cref{lem:l(x)-in-np} that $L(B) \in \NP^{A}$. We shall construct $\tilde{A}$
  using the following algorithm:
  % FIXME: Mention that until set, A_{n,F} outputs 0
  \begin{algorithm}[H]
    % FIXME: Do the P_i need to be P machines? Everybody is unclear on this
    \KwIn{A sequence of $\P$ oracle machines $\{P_{i}\}_{i=1}^{\infty}$}
    \KwOut{An extension oracle $\tilde{A}$ such that $L(A) \notin \P^{\tilde{A}}$}
    % TODO: Define p_i(n)
    $\tilde{A} \leftarrow \varnothing$\;
    $n_{0} \leftarrow 0$\;
    \For{$i$ starting at $1$}{
      Let $n > n_{i}$ be large enough that
      $p_{i}(n) < 2^{n}$\;
      % TODO: Think of this as a collection of oracles instead of indices?
      $T_{j} \leftarrow \bigcup_{j < i}S_{j}$\;
      Run $P_{i}^{\tilde{A}}$ on input $0^{n}$\;
      \eIf{$P_{i}^{B(i-1)}$ rejects $0^{n}$}{
        Let $\mathcal{Y}_{\mathbb{F}}$ be the set of all $y \in \mathbb{F}^{n_{i}}$ queried
        during the above computation\;
        \tcp{See \cref{lem:multiquad-adversary} for why we can do this}
        Let $w \in \{0, 1\}^{n}$ such that the following
        works\;\nllabel{line:def-w}
        \For{all $\mathbb{F}$}{
          Set $\tilde{A}_{n_{i},\mathbb{F}}$ to be a multiquadratic polynomial
          such that $\tilde{A}_{n_{i},\mathbb{F}}(w) = 1$ and
          $\tilde{A}_{n_{i},\mathbb{F}}(y) = 0$ for all
          $y \in \mathcal{Y}_{\mathbb{F}} \cup (\{0, 1\}^{n_{i}} \setminus \{w\})$\;\nllabel{line:set-a}
          % FIXME: Be more clear about what this means
        }
      }{
        Set $\tilde{A}_{n_{i},\mathbb{F}} = 0$ for all $\mathbb{F}$\;
      }
      $n_{i+1} \leftarrow 2^{n}$\;
    }
    $B \leftarrow \bigcup_{i}B(i)$\;
    \caption{An algorithm for constructing $\tilde{A}$}\label{alg:construct-a-tilde}
  \end{algorithm}
  As before, we will start by demonstrating soundness and then move on to why
  the constructed oracle provides the separation we seek.
  % TODO
\end{proof}

\begin{appendices}

\chapter{More on extension polynomials}\label[appendix]{app:ext-poly}

In this appendix, we will work through some of the algebra we mentioned but did
not go into detail about in \cref{sec:polynomial}.

\section{A proof of \cref{eqn:delta-is-delta}}\label{sec:delta-is-delta}

% TODO: Restate preliminaries
Our goal is to demonstrate the following:
\begin{equation}\label{eqn:delta-is-iverson}
  [x = y] = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
    \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
  \mright)\mright)
\end{equation}
for all $x, y \in H^{n}$. We will do this in two cases: one where $x = y$ and one
where $x \ne y$.

First, assume $x \ne y$ (so we want to show $\delta_{y}(x) = 0$). In this case, there
exists at least one $i$ where $x_{i} \ne y_{i}$. For this $i$, for each $\omega$ there
exists some $\gamma \in H \setminus \{\omega\}$ such that either $x_{i} = \gamma$ or
$y_{i} = \gamma$.\footnote{This piece fails in the case where $x_{i} = y_{i}$, since
  if $\omega = x_{i} = y_{i}$ neither of the terms will ever be zero.} As
such, it follows that either $(x_{i} - \gamma) = 0$ or $(\gamma_{i} - \gamma) = 0$. Hence, for
this $i$ the sum will be entirely over zero terms (since there will be at least
one zero term in the product for each $\omega$). As such, this means that the $i$th
term of our outermost product is $0$, and hence the entire product is $0$, as
desired.

When $x = y$ (and so we want to show $\delta_{y}(x) = 1$), the above equation
simplifies to
\begin{equation}
  [x = y] = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
    \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)^{2}}{(\omega - \gamma)^{2}}
  \mright)\mright)
\end{equation}
Whenever $\omega \ne x_{i}$, the innermost product becomes $0$ since there will be a
term where $\gamma = x_{i}$. Hence, we can simplify this further to
\begin{equation}
  [x = y] = \prod_{i=1}^{m}\mleft(\prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)^{2}}{(x_{i} - \gamma)^{2}}\mright).
\end{equation}
Since $\gamma \ne x_{i}$, we can simplify the fraction to $1$; since we have two nested
products it follows that the equation as a whole simplifies to $1$.

\section{Algebra behind \cref{eqn:delta-poly-small}}\label{sec:delta-poly-small}

Our goal is to show that the equation in \cref{eqn:delta-poly} simplifies to
that of \cref{eqn:delta-poly-small} when $H = \{0, 1\}^{n}$.
% TODO

As a refresher, our starting equation has the form
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
  \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
  \mright)\mright).
\end{equation}
We start by manually substituting the outer sum:
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft(\mleft(
    \prod_{\gamma \in \{0, 1\} \setminus \{0\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
    \mright) + \mleft(
    \prod_{\gamma \in \{0, 1\} \setminus \{1\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
    \mright)
  \mright).
\end{equation}
Next, notice that the inner products are actually each over one term, so we can
manually substitute there:
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft(
    \frac{(x_{i} - 1)(y_{i} - 1)}{(0 - 1)^{2}} +
    \frac{(x_{i} - 0)(y_{i} - 0)}{(1 - 0)^{2}}
  \mright).
\end{equation}
Next, we simplify, taking note that the denominator of both fractions is $1$:
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft((x_{i} - 1)(y_{i} - 1) + x_{i}y_{i}\mright).
\end{equation}
From here, we take advantage of the fact that $y \in \{0, 1\}^{n}$; here we split
our product into two smaller products: one where $y_{i} = 0$ and one where
$y_{i} = 1$.
\begin{equation}
  \delta_{y}(x) = \mleft(\prod_{i:y_{i}=0}(x_{i} - 1)(0 - 1) + 0x_{i}\mright)
  \mleft(\prod_{i:y_{i}=1}(x_{i} - 1)(1 - 1) + x_{i}1\mright).
\end{equation}
Finally, we simplify, bringing us to \cref{eqn:delta-poly-small}.
\begin{equation}
  \delta_{y}(x) = \mleft(\prod_{i:y_{i}=0}(1 - x_{i})\mright)\mleft(\prod_{i:y_{i}=1}x_{i}\mright).
\end{equation}

\chapter{More on \cref{lem:multilinear-is-pspace}}\label[appendix]{app:bug-in-pspace}

\begin{defn}\label{def:alternating-tm}\index{Turing machine!alternating}
  An \emph{alternating Turing machine} is
\end{defn}

\begin{proof}[{Proof of \cref{lem:multilinear-is-pspace} as written in~\cite{BFL90}}]
  Let $L$ be a $\PSPACE$-robust language. Let $g_{n}(x_{1}, \ldots, x_{n})$ be the
  multilinear extension of the characteristic function of
  $L_{n} = L \cap \{0, 1\}^{n}$. Clearly, $L \in \P^{g}$, where
  $g = \{g_{n} \mid n \ge 0\}$. We will describe an alternating polynomial-time
  Turing machine with access to $L$ computing $g$. First guess the value
  $z = g_{n}(x_{1}, \ldots, x_{n})$. Then existentially guess the linear function
  $h_{1}(y) = g(y, x_{2}, \ldots, x_{n})$ and verify that $h_{1}(x_{1}) = z$. Then
  universally choose $t_{1} \in \{0, 1\}$ and existentially guess the linear
  function $h_{2}(y) = g(t_{1}, y, x_{3}, \ldots, x_{n})$. Keep repeating this
  process until we have specified $t_{1}, \ldots, t_{n}$ and then verify
  $t_{1}, \ldots, t_{n} \in L$. Since a $\PSPACE$ machine can simulate an alternating
  polynomial-time Turing machine, if $L$ is $\PSPACE$-robust then $g$ is
  Turing-reducible to $L$.
\end{proof}
\end{appendices}

\printbibliography[heading=bibintoc]{}

\printindex{}

\end{document}
