\documentclass[english,12pt]{reedthesis}

\usepackage[T1]{fontenc}
\usepackage{babel}

\usepackage{appendix}
\usepackage{amsfonts, amscd, amssymb, amsthm, amsmath}
\usepackage{mathtools} %xmapsto etc
\usepackage{pdfsync} %leaves makers for tex searching
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{complexity}
\usepackage{mleftright}
\usepackage{xcolor}
\usepackage{soul}

\usepackage[linesnumbered,algochapter]{algorithm2e}
\usepackage{biblatex}
\usepackage{imakeidx}
\usepackage{microtype}
\usepackage{tikz}
\usepackage{csquotes}
\usepackage{fewerfloatpages}

% FIXME: Really bad hack to get cleveref working
% This is unnecessary on literally every other LaTeX setup except mine
% When working again, will need to reset theorem defs to point to the thm counter
\usepackage{aliascnt}

\usepackage[colorlinks]{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}

\usetikzlibrary{arrows.meta}
\usetikzlibrary{hobby}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{through}
\usetikzlibrary{trees}

\setlength{\headheight}{14.5pt}

\hyphenation{prob-a-bilis-ti-cal-ly}

%%% Theorems %%%---------------------------------------------------------
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newaliascnt{lemma}{thm}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\newaliascnt{prop}{thm}
\newtheorem{prop}[prop]{Proposition}
\aliascntresetthe{prop}
\newaliascnt{cor}{thm}
\newtheorem{cor}[cor]{Corollary}
\aliascntresetthe{cor}
\theoremstyle{definition}
\newtheorem*{def*}{Definition}
\newaliascnt{defn}{thm}
\newtheorem{defn}[defn]{Definition}
\aliascntresetthe{defn}
\theoremstyle{remark}
\newtheorem{example}{Example}[thm]
\newtheorem{remark}[thm]{Remark}
\newtheorem{subrem}[example]{Remark}

\definecolor{plum}{HTML}{8105C1}
\definecolor{pumpkin}{HTML}{E47604}
\definecolor{rose}{HTML}{C10091}
\definecolor{dgreen}{HTML}{25A75B}
\definecolor{dblue}{HTML}{0066FF}
\definecolor{cornflower}{HTML}{3256C3}
\definecolor{viridian}{HTML}{099A97}
\definecolor{alert}{HTML}{3256C3}

\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\comp}{comp}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dimn}{dim}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\ev}{ev}
\def\f{\varphi}
\def\half{\hbox{$\frac12$}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\img}{img}
\DeclareMathOperator{\Inn}{Inn}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\mdeg}{mdeg}
\DeclareMathOperator{\out}{out}
\DeclareMathOperator{\oin}{in}
\DeclareMathOperator{\rad}{rad}
\DeclareMathOperator{\Rep}{Rep}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\rk}{rank}
\def\normeq{\trianglelefteq}
\DeclareMathOperator{\nul}{nullity}
\DeclareMathOperator{\per}{per}
\DeclareMathOperator{\Sim}{Sim}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Syl}{Syl}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\View}{View}
\def\vep{\varepsilon}
\DeclareMathOperator{\lcm}{lcm}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\ang}{\langle}{\rangle}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\DeclarePairedDelimiter\bra{\langle}{\rvert}
\DeclarePairedDelimiter\ket{\lvert}{\rangle}
\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1\,\delimsize\vert\,\mathopen{}#2}
\DeclarePairedDelimiterX\braopket[3]{\langle}{\rangle}{#1\,\delimsize\vert\,\mathopen{}#2\,\delimsize\vert\,\mathopen{}#3}

\newcommand{\middlemid}{%
  \ensuremath{\;\middle\vert\;}
}

\newcommand{\dblang}[1]{%
  \ensuremath{\left\langle\!\left\langle#1\right\rangle\!\right\rangle}
}

\newcommand{\comment}[1]{%
  \text{\phantom{(#1)}} \tag{#1}%
}

\newcommand{\commath}[1]{%
  \phantom{(#1)} \tag{#1}%
}

\newcommand{\pcpp}[4]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{rand.\ complexity:} & #1 \\
    \text{query complexity:} & #2 \\
    \text{prox.\ param.:} & #3 \\
    \text{soundness error:} & #4
  \end{array}}\mright]%
}

\newcommand{\ipcp}[5]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{round complexity:} & #1 \\
    \text{PCP length:} & #2 \\
    \text{comm.\ complexity:} & #3 \\
    \text{query complexity:} & #4 \\
    \text{soundness error:} & #5
  \end{array}}\mright]%
}

\newcommand{\pzkipcp}[6]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{round complexity:} & #1 \\
    \text{PCP length:} & #2 \\
    \text{comm.\ complexity:} & #3 \\
    \text{query complexity:} & #4 \\
    \text{query bound:} & #5 \\
    \text{soundness error:} & #6
  \end{array}}\mright]%
}

\newcommand{\pzkpcp}[4]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{rand.\ complexity:} & #1 \\
    \text{query complexity:} & #2 \\
    \text{query bound:} & #3 \\
    \text{soundness error:} & #4
  \end{array}}\mright]%
}

\newcommand{\pzkpcpr}[6]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{rand.\ complexity:} & #1 \\
    \text{query complexity:} & #2 \\
    \text{query bound:} & #3 \\
    \text{soundness error:} & #4 \\
    \text{RS error:} & #5 \\
    \text{robustness param:} & #6
  \end{array}}\mright]%
}

\newcommand{\ldipcp}[6]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{round complexity:} & #1 \\
    \text{PCP length:} & #2 \\
    \text{comm.\ complexity:} & #3 \\
    \text{query complexity:} & #4 \\
    \text{oracle:} & #5 \\
    \text{soundness error:} & #6
  \end{array}}\mright]%
}

\newcommand{\mipstar}[4]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{number of provers:} & #1 \\
    \text{round complexity:} & #2 \\
    \text{comm.\ complexity:} & #3 \\
    \text{soundness error:} & #4
  \end{array}}\mright]%
}

\newcommand{\pcpr}[4]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{query complexity:} & #1 \\
    \text{random complexity:} & #2 \\
    \text{robustness parameter:} & #3 \\
    \text{robust-soundness error:} & #4
  \end{array}}\mright]%
}

\newclass{\CktVal}{CktVal}
\newclass{\IPCP}{IPCP}
\makeatletter % complexity doesn't support hyphens by default...
\newcommand{\PZKPCP}{%
  \PZK\complexity@hyphenleft\PCP
}
\newcommand{\PZKIPCP}{%
  \PZK\complexity@hyphenleft\IPCP
}
\newcommand{\PZKMIP}{%
  \PZK\complexity@hyphenleft\MIP
}
\makeatother

\newlang{\CktSAT}{CktSAT}
\newlang{\ECC}{ECC}
\newlang{\GNI}{GNI}
\newlang{\OR}{OR}
\newlang{\OSAT}{O3SAT}
\newlang{\PCPP}{PCPP}
\newlang{\Prime}{Prime}
\newlang{\Sum}{Sum}

\newcommand{\shrug}[1][]{%
  \begin{tikzpicture}[baseline,x=0.8\ht\strutbox,y=0.8\ht\strutbox,line width=0.125ex,#1]
    \def\arm{(-2.5,0.95) to (-2,0.95) (-1.9,1) to (-1.5,0) (-1.35,0) to (-0.8,0)};
    \draw \arm;
    \draw[xscale=-1] \arm;
    \def\headpart{(0.6,0) arc[start angle=-40, end angle=40,x radius=0.6,y radius=0.8]};
    \draw \headpart;
    \draw[xscale=-1] \headpart;
    \def\eye{(-0.075,0.15) .. controls (0.02,0) .. (0.075,-0.15)};
    \draw[shift={(-0.3,0.8)}] \eye;
    \draw[shift={(0,0.85)}] \eye;
    % draw mouth
    \draw (-0.1,0.2) to [out=15,in=-100] (0.4,0.95);
  \end{tikzpicture}
}

\newcommand{\TODO}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\FIXME}[1]{\textcolor{red}{\ul{FIXME: #1}}}

\addbibresource{bibliography.bib}

\makeindex[intoc]

\title{Extending Zero-Knowledge PCPs Beyond NP}
\author{Patrick Norton}

\approvedforthe{Committee}
\thedivisionof{The Established Interdisciplinary Committee for \\}
\division{Mathematics and Computer Science}
\department{Mathematics and Computer Science}
\advisor{Zajj Daugherty}
\altadvisor{Adam Groce}

\begin{document}

\maketitle

\tableofcontents

\listofalgorithms

% TODO: Abstract

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

The $\P$ vs $\NP$ problem is perhaps the most important open problem in
complexity theory.

\chapter{Preliminaries}

\section{Turing machines}

% TODO: Do we even need to formally define a TM?

Central to our definitions of complexity is that of a Turing machine. This is
the most common mathematical model of a computer, and is the jumping-off point
for mant variants. There are many ways to think of a Turing machine, but the
most common is that of a small machine that can read and write to an
arbitrarily-long ``tape'' according to some finite set of rules. We give a more
formal definition below, and then we will attempt to take this definition into a
more manageable form.

\begin{figure}[htbp]
  \centering
  \TODO{}
  \begin{tikzpicture}
  \end{tikzpicture}
  \caption{A Turing machine}\label{fig:tm}
\end{figure}

\begin{defn}[{\cite[Def.\ 3.1]{Sip97}}]\label{def:TM}\index{Turing machine}
  A \emph{Turing machine} (abbreviated TM) is a 7-tuple
  $(Q, \Sigma, \Gamma, \delta, q_{0}, q_{a}, q_{r})$ where $Q$, $\Sigma$, and $\Gamma$ are all finite
  sets and
  \begin{enumerate}
    \item $Q$ is the set of \emph{states},
    \item $\Sigma$ is the \emph{input alphabet},
    \item $\Gamma$ is the \emph{tape alphabet},
    \item $\delta\colon Q \times \Gamma \rightarrow Q \times \Gamma \times \{L, R\}$ is the \emph{transition function},
    \item $q_{0} \in Q$ is the \emph{start state},
    \item $q_{a} \in Q$ is the \emph{accept state},
    \item $q_{r} \in Q$ is the \emph{reject state}, with $q_{a} \ne q_{r}$.
  \end{enumerate}
\end{defn}

While we have this formalism here as a useful reference, even here we will most
frequently refer to Turing machines in a more intuitionisitc form. There are
several ways we will think about Turing machines.

The first way to think about a Turing machine is as a little computing box with
a tape. We let the box read and write to the tape, and each step it can move the
tape one space in either direction. At some point, the machine can decide it is
done, in which case we say it ``halts''; however it does not necessarily need to
halt. For this paper, we will only think about machines that \emph{do} halt, and
in particular we will care about how many it takes us to get there. Further, we
will use this informalism as a base from which we can define our Turing machine
variants intuitively, without needing to deal with the (potentially extremely
convoluted) formalism.

Another way we think about a Turing machine is as an algorithm. Perhaps the
foundational paper of modern computer science theory, the \emph{Church-Turing
  thesis}~\cite{Tur36}, states that any actually-computable algorithm has an
equivalent Turing machine, and vice versa. We will use this fact liberally; in
many cases we will simply describe an algorithm and not deal with putting it
into the context of a Turing machine. If we have explained the algorithm well
enough that a reader can execute it (as we endeavor to do), then we know a
Turing machine must exist.

\begin{figure}[htbp]
  \centering
  \TODO{}
  \begin{tikzpicture}
  \end{tikzpicture}
  \caption{A nondeterministic Turing machine}\label{fig:ntm}
\end{figure}

\begin{defn}\label{def:nondeterministic-tm}\index{Turing machine!nondeterministic}
  A \emph{nondeterministic Turing machine} is % TODO
\end{defn}

\begin{defn}\label{def:multitape-tm}\index{Turing machine!multitape}
  A \emph{multitape Turing machine} is
\end{defn}

\begin{defn}\label{def:prob-tm}\index{Turing machine!probabilistic}
  A \emph{probabilistic Turing machine} is
\end{defn}

\section{Complexity classes}

% TODO: Do I need to define big-O for this?

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \pgfmathsetmacro{\s}{sqrt(2)/4};
    \draw (0, 0) circle [radius=0.5];
    \node at (0, 0) {$\P$};
    \draw (\s,\s) ellipse [x radius=1,y radius=0.75,rotate=45];
    \node[anchor=south west] at (\s,\s) {$\NP$};
    \draw (-\s,\s) ellipse [x radius=1,y radius=0.75,rotate=-45];
    \node[anchor=south east] at (0.1-\s,\s) {$\BPP$};
    \draw (0, 1) circle [radius=1.5];
    \node at (0, 1.75) {$\PSPACE$};
    \draw (0, 1.5) circle [radius=2];
    \node at (0, 3) {$\EXP$};
    \draw (0, 2) circle [radius=2.5];
    \node at (0, 4) {$\NEXP$};
    \draw (0, 2.5) circle [radius=3];
    \node at (0, 5) {$\RE$};
  \end{tikzpicture}
  \caption{The relationship between the complexity classes for this
    paper}\label{fig:comp-class}
\end{figure}

Complexity classes are the main way we think about the hardness of problems in
computer science. A complexity class\index{complexity class} is a collection of
languages that all share a common level of difficulty.

We start with a relatively straightforward example of a complexity class: the
class of languages that a Turing machine can recognize. First, we need to
define what recognition is in order to make a complexity class out of it.

\begin{defn}[{\cite[Def.\ 3.2]{Sip97}}]\label{def:recognition}\index{recognize}
  A language $L$ is \emph{recognized} by a Turing machine $M$ if for all strings
  $s \in L$, $M$ halts in the accept state when given $s$ as input.
\end{defn}

Now, since our complexity classes are about \emph{languages}, we naturally wish
to extend our notion of recognition to a statistic on languages.

\begin{defn}\label{def:turing-recognizable}\index{Turing-recognizable language}
  A language $L$ is \emph{Turing-recognizable} (frequently just
  \emph{recognizable}) if it is recognized by some Turing machine.
\end{defn}

Now that we have a property of languages, it is straightforward for us to turn
it into a complexity class.

\begin{defn}\label{def:re}\index{RE@$\RE$}
  The class $\RE$ is the class of all Turing-recognizable languages.
\end{defn}
% TODO: Example (and non-example)

For most other classes, we want our Turing machines to halt on \emph{all}
inputs, not just those in the class. From a practical perspective, this is
useful because it tells us that we can be certain about whether any given string
is in the given language. From here on, we will generally care about how much of
some resource our machines take when making their decision, as opposed to
whether or not they can.

\subsection{Time complexity}\index{time complexity}

The most intuitive (and most important) notion of complexity is that of time
complexity. Time complexity is the answer of the question of how long it takes
to solve a problem. We begin with an abstract base for our time classes, and
will then introduce some specific ones that we care about.

\begin{defn}[{\cite[Def.\ 1.19]{AB09}}]\label{def:dtime}\index{DTIME@$\DTIME$}
  % FIXME: Reword to make more clear what $n$ is
  Let $f\colon \mathbb{N} \rightarrow \mathbb{N}$ be a function. The class $\DTIME(f(n))$ is the class of all
  problems computable by a deterministic Turing machine in $O(f(n))$ steps for
  some constant $c > 0$.
\end{defn}

While $\DTIME$ is a useful base to start from, it is rare that we deal with
$\DTIME$ classes directly. % TODO

\begin{defn}[{\cite[Def.\ 1.20]{AB09}}]\label{def:p}\index{P@$\P$}
  The complexity class $\P$ is the class
  \[
    \P = \bigcup_{c > 0}\DTIME(n^{c}).
  \]
\end{defn}

The class $\P$ is perhaps the most important complexity class. Mathematically,
we care about $\P$ because it is closed under composition: a polynomial-time
algorithm iterated a polynomial number of times is still in $\P$. Further, $\P$
turns out to generally be invariant under change of (deterministic) computation
model, which allows us to reason about $\P$ problems easily without needing to
resort to the formal definition of a Turing machine. More philosophically, $\P$
generally represents the set of ``efficient'' algorithms in the real
world.\footnote{It is worth mentioning that this is a \emph{mathematical}
  efficiency---there are plenty of algorithms in $\P$ that a real-world computer
  scientist would never dare to call efficient.}

\begin{example}\label{ex:polynomial-is-p}
  The language
  \[
    \{(p, x, y) \mid p \text{ a polynomial and } p(x) = y\}
  \]
  is in $\P$. We can calculate whether a string is in this language by
  calculating $p(x)$ (which we can do efficiently), and then comparing it to
  $y$.
\end{example}

As we have defined $\P$ in terms of $\DTIME$, the question arises of whether
there is an equivalent in terms of $\NTIME$. Naturally, there is, and we call it
$\NP$.

\begin{defn}[{\cite[Cor.\ 7.22]{Sip97}}]\label{def:np}\index{NP@$\NP$}
  The complexity class $\NP$ is the class
  \[
    \NP = \bigcup_{c > 0}\NTIME(n^{c}).
  \]
\end{defn}

While this definition demonstrates how $\NP$ is similar to $\P$, there are other
equivalent ones that we can use. In particular, we very often like to think of
$\NP$ in terms of deterministic \emph{verifiers}. Since nondeterministic
machines do not exist in real life, this definition gives a practical meaning to
$\NP$.

\begin{example}\label{ex:sat-is-np}
  The language $\SAT$ is the language of Boolean formulas with at least one
  solution. $\SAT$ is in $\NP$: we can nondeterministically pick a potential
  solution and then evaluate our formula (which can be done efficiently); there
  will be an accepting path if and only if a solution to the formula exists.
\end{example}

\begin{thm}[{\cite[Def.\ 7.19]{Sip97}}]\label{thm:np-verifier}
  $\NP$ is exactly the class of all languages verifiable by a $\P$-time Turing
  machine.
\end{thm}

% TODO: Should I prove this?

\begin{example}\label{ex:sat-np-verifier}
  The language $\SAT$ we defined in \cref{ex:sat-is-np} can be verified
  efficiently, where the certificate is an accepting set of variables. Since we
  can evaluate a Boolean formula efficiently, if we already have an accepting
  set of variables we can therefore verify it in $\P$.
\end{example}

The next step up from polynomial complexities is that of exponential
complexities. For these, instead of having the classes bounded above by a
polynomial, we have the classes bounded above by $2$ to the power of a
polynomial. While we use $2$ as the base, the value of the base turns out not to
matter since for any $a, b > 1$,
\begin{equation}
  a^{n^{c}} = b^{n^{c}\log_{b}(a)} \in O\mleft(b^{n^{c+1}}\mright).
\end{equation}

\begin{defn}[{\cite[\defaultS 2.6.2]{AB09}}]\label{def:exp}\index{EXP@$\EXP$}
  The complexity class $\EXP$ is the class
  \[
    \EXP = \bigcup_{c > 0}\DTIME\mleft(2^{n^{k}}\mright).
  \]
\end{defn}

\begin{defn}[{\cite[\defaultS 2.6.2]{AB09}}]\label{def:nexp}\index{NEXP@$\NEXP$}
  The complexity class $\NEXP$ is the class
  \[
    \NEXP = \bigcup_{c > 0}\NTIME\mleft(2^{n^{k}}\mright).
  \]
\end{defn}

It is immediate that $\P \subseteq \EXP$ and $\NP \subseteq \NEXP$ (since the exponential
classes allow the use of more of the same resource). Of slightly less-trivial
interest is the relationship between $\NP$ and $\NEXP$.

\begin{thm}\label{thm:np-exp}
  $\NP \subseteq \EXP$.
\end{thm}

\begin{proof}
  If a nondeterministic machine solves a problem in $p(n)$ steps, it follows
  that the total number of branches is less than $a^{p(n)}$, where $a$ is the
  maximum number of branches for a node within the machine. Hence, we can
  simulate the machine deterministically by simply enumerating every branch,
  giving us a total computation time of $p(n)a^{p(n)}$, which is in
  $O(2^{q(n)})$ for some other polynomial $q(n)$. Hence any $\NP$ problem is in
  $\EXP$.
\end{proof}

It is perhaps illustrative to see an example of a problem in $\NEXP$. % TODO

\begin{defn}[{\cite[Def.\ 14.1]{CFGS22}}]\label{def:oracle-3sat}\index{O3SAT@$\OSAT$}
  The \emph{oracle 3-satisfiability problem}, denoted $\OSAT$, is the language
  of all triplets $(r, s, B)$, where $r, s \in \mathbb{N}^{+}$ and
  $B\colon \{0, 1\}^{r+3s+3} \rightarrow \{0, 1\}$ a boolean function, such that there
  exists a boolean function $A\colon \{0, 1\}^{s} \rightarrow \{0, 1\}$ having
  the property that for all $z \in \{0, 1\}^{r}$ and
  $b_{1}, b_{2}, b_{3} \in \{0, 1\}^{s}$,
  \begin{equation*}
    B(z, b_{1}, b_{2}, b_{3}, A(b_{1}), A(b_{2}), A(b_{3})) = 1.
  \end{equation*}
\end{defn}

% TODO: Cite
\begin{thm}\label{thm:o3sat-in-nexp}
  $\OSAT \in \NEXP$.
\end{thm}

\begin{proof}
  We present the following non-deterministic algorithm to determine if
  $(r, s, B) \in \OSAT$:

  \begin{algorithm}[H]
    \KwIn{A triplet $(r, s, B)$}
    \KwOut{Whether or not $(r, s, B) \in \OSAT$}
    Nondeterministically choose a function $A\colon \{0, 1\}^{s} \rightarrow \{0, 1\}$\;
    \For{$z \in \{0, 1\}^{r}$}{
      \For{$b_{1}, b_{2}, b_{3} \in \{0, 1\}^{s}$}{
        Compute $B(z, b_{1}, b_{2}, b_{3}, A(b_{1}), A(b_{2}), A(b_{3}))$\;
        \If{the above is not $1$}{
          \KwRet{0}\;
        }
      }
    }
    \KwRet{1}\;
    \caption{A $\NEXP$-time algorithm for determining $\OSAT$}\label{alg:osat-nexp}
  \end{algorithm}

  First, we need to show that \cref{alg:osat-nexp} is in $\NEXP$.
  Nondeterministically choosing a function from $\{0, 1\}^{s}$ can be done in
  time $2^{s}$; and the two loops will run a total of $2^{r}2^{s}$ times,
  respectively. Computation of a function can be done in polynomial time
  relative to its length; hence the runtime of this function is in exponential
  time relative to $r + s$.

  One might be tempted to think that since we are given a function $B$ as input,
  that our function $A$ can be no longer than $\poly(\abs{B})$, but this is not
  necessarily true. We are given $B$ in $3\SAT$ form, and thus there are
  expressions of $B$ that are polynomial with respect to $r + s$. Despite this,
  there are polynomial-length $B$ instances whose $A$ is \emph{not} polynomial
  in length (since that is an arbitrary function); since runtime complexity is
  about the worst case there thus exist inputs that cannot be computed in
  polynomial time relative to their length.

  \TODO{}
\end{proof}

\subsection{Space complexity}\index{space complexity}

In addition to time complexity, the an additional notion of complexity is that
of space complexity. Space complexity is the question of how much space on its
memory tape a machine needs in order to compute a problem. In many ways, our
definitions of space complexity are analagous to those for time complexity that
we have already defined. In particular, $\DSPACE$ will correspond nicely to
$\DTIME$, and $\NSPACE$ to $\NTIME$.

\begin{defn}[{\cite[Def.\ 4.1]{AB09}}]\label{def:dspace}\index{DSPACE@$\DSPACE$}
  Let $f\colon \mathbb{N} \rightarrow \mathbb{N}$ be a function. A language $L$ is in $\DSPACE(f(n))$ if
  there exists a deterministic Turing machine $M$ such that the number of
  locations on the tape that are non-blank at some point during the execution of
  $M$ is in $O(f(n))$.
\end{defn}

In the same way as we have defined $\DSPACE$ for deterministic machines, we now
need to define $\NSPACE$ for nondeterministic machines.

\begin{defn}[{\cite[Def. 4.1]{AB09}}]\label{def:nspace}\index{NSPACE@$\NSPACE$}
  Let $f\colon \mathbb{N} \rightarrow \mathbb{N}$ be a function. A language $L$ is in $\NSPACE(f(n))$ if there
  exists a nondeterministic Turing machine $M$ such that the number of locations
  on the tape that are non-blank at some point during the execution of $M$ is in
  $O(f(n))$.
\end{defn}

Analagously to $\P$ and $\NP$, our two main classes of space complexity are
$\PSPACE$ and $\NPSPACE$.

\begin{defn}[{\cite[Def.\ 4.5]{AB09}}]\label{def:pspace}\index{PSPACE@$\PSPACE$}
  The complexity class $\PSPACE$ is the class
  \[
    \PSPACE = \bigcup_{c > 0}\DSPACE(n^{c}).
  \]
\end{defn}

\begin{defn}[{\cite[Def.\ 4.5]{AB09}}]\label{def:npspace}\index{NPSPACE@$\NPSPACE$}
  The complexity class $\NPSPACE$ is the class
  \[
    \NPSPACE = \bigcup_{c > 0}\NSPACE(n^{c}).
  \]
\end{defn}

Unlike with $\P$ and $\NP$, the relationship between $\PSPACE$ and $\NPSPACE$ is
well known. Due to the complexity of the proof of the theorem, we will not prove
it here, as it is mostly not relevant to what we will be doing.

\begin{thm}[{Savitch's theorem;~\cite{Sav70}}]\label{thm:savitch}\index{Savitch's theorem}
  $\PSPACE = \NPSPACE$.
\end{thm}

Upon seeing this, one might ask why it is that we believe $\P \ne \NP$ if we know
that $\PSPACE = \NPSPACE$, given they are defined analogously. The answer to
this question boils down to the fact that we are able to reuse space, while we
are not able to reuse time. Space on the tape that is no longer needed can be
overwritten, while time that is no longer needed is gone forever.

Since $\PSPACE$ and $\NPSPACE$ are equal classes, it is relatively rare to see
$\NPSPACE$ referred to. Here, we will only refer to it when it makes a class
relationship clearer; most frequently when comparing $\NPSPACE$ to some other
nondeterministic class.

\begin{example}\label{ex:regex-is-pspace}
  The language
  \[
    \{(x, y) \mid x, y \text{ regexes that accept the same set of strings}\}
  \]
  is in $\PSPACE$. % TODO: Explain why
\end{example}

\subsection{Completeness}

% TODO: Not a huge fan of this paragraph
Even within a complexity class, not all problems are created equal. The notion
of \emph{completeness} gives us a mathematically-rigorous way to talk about
which problems in a class are the hardest. Since putting upper bounds on hard
problems naturally puts those same bounds on any easier problems, complete
problems can be useful in reasoning about the relationship between complexity
classes.

\begin{defn}[{\cite[Def.\ 7.29]{Sip97}}]\label{def:p-reduction}\index{polynomial-time reduction}
  A language $A$ is \emph{polynomial-time reducible} to a language $B$ if a
  polynomial-time computable function $f\colon \Sigma^{*} \rightarrow \Sigma^{*}$ exists
  such that for all $w \in \Sigma^{*}$, $w \in A$ if and only if $f(w) \in B$.
\end{defn}

Polynomial-time reductions are important because they give us a way to say that
$A$ is \emph{no harder} than $B$. In particular, if we have an algorithm $M$
that determines $B$, we can construct the following algorithm that determines
$A$ with only a polynomial amount of additional work:

\begin{algorithm}[H]
  \KwIn{A string $w \in \Sigma^{*}$}
  \KwOut{Whether $w \in A$}
  Compute $f(w)$\;
  Use $M$ to check whether $f(w) \in B$\;
  \KwRet{the result of $M$}\;
  \caption{An algorithm to reduce $A$ to $B$}
\end{algorithm}

\begin{defn}[{\cite[Def.\ 7.34]{Sip97}}]\label{def:np-complete}\index{NP-complete@$\NP$-complete}
  A language $L$ is $\NP$-complete if $L \in \NP$ and every $A \in \NP$ is
  polynomial-time reducible to $L$.
\end{defn}

This is a practical use of our polynomial-time reductions: since an
$\NP$-complete language has a reduction from every other language in $\NP$, it
follows that it is \emph{at least as hard} as any other language in $\NP$. Of
particular interest to complexity theorists is the fact that $\P = \NP$ if and
only if \emph{any} $\NP$-complete language is in $\P$.

\begin{example}\label{ex:sat-is-complete}\index{Cook-Levin theorem}
  A famous result of Cook~\cite{Cook71}, also proved around the same time by
  Levin and thus called the \emph{Cook-Levin theorem}, is that the $\SAT$
  problem we defined earlier in \cref{ex:sat-is-np} is $\NP$-complete.
\end{example}

The notion of completeness is very important to complexity theorists. Since
these are the ``hardest'' problems in $\NP$, this means that if we can do
anything interesting to an $\NP$-complete problem, we can leverage these
reductions to do that interesting thing to \emph{any} other problem in $\NP$
with only a little (i.e.\ polynomial) more effort. This will come in especially
handy when we want to prove that complexity classes are equal or that $\NP$ is a
subset of some other complexity class---since most complexity classes allow for
things to change polynomially, we only need to prove that a single
$\NP$-complete element is in the other class for the subset relation to follow.

% TODO: Cite NEXP not being closed under EXP reductions
Along with completeness for $\NP$, we have a notion of completeness for $\NEXP$.
While you might expect that the reducibility constraints might loosen (i.e.\
allow more complex reductions) since $\NEXP$ is more complex for $\NP$, but this
turns out not to be the case. In particular, while it might initially seem
logical to allow for $\EXP$-reductions, it turns out that $\NEXP$ is not closed
under $\EXP$-reductions, which makes a notion of completeness challenging.
Despite this, we can still learn interesting things about $\NEXP$ by studing
completeness under polynomial reductions.

\begin{defn}\label{def:nexp-complete}\index{NEXP-complete@$\NEXP$-complete}
  A language $L$ is $\NEXP$-complete if $L \in \NEXP$ and every $A \in \NEXP$ is
  polynomial-time reducible to $\NEXP$.
\end{defn}

$\NEXP$-completeness has many of the same nice properties of $\NP$-completeness.
Of particular interest to us will again be the ease with which
$\NEXP$-completeness allows us to determine subset relations, simply by proving
the inclusion of a single complete language.

\begin{thm}[{\cite[Proposition 4.2]{BFL90}}]\label{thm:o3sat-nexp-complete}
  The language $\OSAT$ (as defined in \cref{def:oracle-3sat}) is
  $\NEXP$-complete.
\end{thm}

\begin{proof}
  We demonstrated in \cref{thm:o3sat-in-nexp} that $\OSAT \in \NEXP$, so all that
  remains is to prove that reductions exist for every $\NEXP$ language. Let
  $L \in \NEXP$, and let $x \in \{0, 1\}^{n}$. We aim to construct an algorithm in
  $\P^{\OSAT}$ that computes $L$.

  \TODO{}
\end{proof}

Just as we have $\NP$-completeness and $\NEXP$-completeness for time complexity,
we also have notions of completeness for space complexity. Since
$\PSPACE = \NPSPACE$, instead of calling the class $\NPSPACE$-complete, we call
it $\PSPACE$-complete.

\begin{defn}[{\cite[Def.\ 8.8]{Sip97}}]\label{def:pspace-complete}\index{PSPACE-complete@$\PSPACE$-complete}
  A language $L$ is $\PSPACE$-complete if $L \in \PSPACE$ and every $A \in \PSPACE$
  is polynomial-time reducible to $\NP$.
\end{defn}

While this definition is mostly analagous to that of $\NP$-completeness, one
might wonder why we use a time complexity for our reduction when $\PSPACE$ is a
space-complexity class. This is because if we were to use space complexity, we
would want to use $\PSPACE$-reductions, but that would make every language in
$\PSPACE$ trivially $\PSPACE$-complete. Since that is not a useful definition,
we instead restrict ourselves to polynomial-time reductions.

\begin{example}
  A result of Stockmeyer and Meyer~\cite{SM73} is that the language we defined
  in \cref{ex:regex-is-pspace} is $\PSPACE$-complete.
\end{example}

\subsection{Randomized complexity}\label{sec:rand-complexity}

\TODO{}

\begin{defn}\label{def:bpp}\index{BPP@$\BPP$}
  A language $L$ is in $\BPP$ if there exists a probabilistic Turing machine $M$
  such that
  \begin{enumerate}
    \item $M$ runs in polynomial time,
    \item for all $x \in L$, $M$ accepts $x$ with probability at least $2/3$,
    \item for all $x \notin L$, $M$ rejects $x$ with probability at least $2/3$.
  \end{enumerate}
\end{defn}

\begin{thm}\label{thm:p-subset-bpp}
  $\P \subseteq \BPP$.
\end{thm}

\begin{thm}\label{thm:bpp-subset-pspace}
  $\BPP \subseteq \PSPACE$.
\end{thm}

\subsection{Counting complexity}\index{counting complexity}

So far, all of the problems we have seen are boolean problems, where the answer
is either yes or no. There exist many interesting questions, however, where we
would like more than two distinct answers. In particular, there are lots of
problems where the most interesting answer is a \emph{count} of something: in
particular, questions of the type ``How many objects are there such that some
condition is true?'' will be of interest to us.

\begin{defn}\label{def:counting-problem}\index{counting problem}
  A \emph{counting problem} is a function from $\{0, 1\}^{*}$ to $\mathbb{N}$.
\end{defn}

Now that we have this definition, we should define a complexity class of
function problems. The simplest complexity class for function problems is
$\#\P$, the set of function problems that can be computed in polynomial time.

\begin{defn}[{\cite[Def.\ 9.2]{AB09}}]\label{def:hash-p}\index{P#@$\#\P$}
  The class $\#\P$ is the class of functions $f\colon \{0, 1\}^{*} \rightarrow \mathbb{N}$ such
  that there exists a polynomial $p\colon \mathbb{N} \rightarrow \mathbb{N}$ and a polynomial-time
  Turing machine $M$ such that for every $x \in \{0, 1\}^{*}$,
  \begin{equation}\label{eqn:hash-p}
    f(x) = \abs*{\mleft\{y \in \{0, 1\}^{p(\abs{x})} \middlemid M(x, y) = 1\mright\}}.
  \end{equation}
\end{defn}

As mentioned earlier, questions of the type ``How many objects are there such
that some condition is true?'' are of the most interest of us, and here we
actually define $\#\P$ in terms of these problems. \Cref{eqn:hash-p} simply says
that $f(x)$ needs to be equal to the number of inputs $y$ on which some
polynomial Turing machine accepts when given both $x$ and $y$. In this case, the
Turing machine $M$ provides our condition, and a function problem is in $\#\P$
exactly when we can compute this condition efficiently.

\TODO{Introduce $\#\P$-completeness}

\begin{defn}\label{def:hash-p-complete}\index{P#-complete@$\#\P$-complete}
  A language is $\#\P$-complete if \TODO{Do I want to use the definition with $\FP$?}
\end{defn}

\begin{defn}\label{def:hash-sat}\index{SAT#@$\#\SAT$}
  The function $\#\SAT$ is the function that, given a Boolean formula $\phi$,
  returns the number of distinct assignments such that $\phi$ is true.
\end{defn}

\begin{thm}\label{thm-hash-sat-is-complete}
  $\#\SAT$ is $\#\P$-complete.
\end{thm}

\section{Polynomials}\label{sec:polynomial}

\TODO{}

\begin{defn}[{\cite{Knu92}}]\label{def:iverson-bracket}\index{Iverson bracket}
  Let $P$ be a mathematical statement. The function $[P]$ is the the function
  \begin{equation}\label{eqn:iverson-bracket}
    [P] = \begin{cases}
      1 & P \text{ is true} \\
      0 & \text{otherwise.}
    \end{cases}
  \end{equation}
  This is called the \emph{Iverson bracket}, after its inventor Kenneth Iverson,
  who originally included it in the programming language APL\footnote{The
    original notation used parentheses, but square brackets are much less
    ambiguous, so that has become the standard and what we will use
    here.}~\cite[11]{APL}.
\end{defn}

\begin{example}
  Using the Iverson bracket, the Kronecker delta function\index{Kronecker delta
    function} can be defined as
  \[
    \delta_{ij} = [i = j].
  \]
\end{example}

Much of our work will deal with multivariate polynomials. For a given field
$\mathbb{F}$, we will denote the set of $m$-variable polynomials over
$\mathbb{F}$ with $\mathbb{F}[x_{1, \ldots, m}]$.

\begin{defn}[{\cite[8]{AW09}}]\label{def:mdeg}\index{multidegree}
  The \emph{multidegree} of a multivariate polynomial $p$, written $\mdeg(d)$,
  is the maximum degree of any variable $x_{i}$ of $p$.
\end{defn}

It is worth noting that for monovariate polynomials, multidegree and degree
coincide. The difference between multidegree and degree is subtle, but
important. We shall illustrate the difference with a simple example.

\begin{example}
  Consider the polynomial $x_{1}^{2}x_{2} + x_{2}^{2}$. The multidegree of this
  polynomial is 2, while its degree is 3.
\end{example}

We denote by $\mathbb{F}[x_{1, \ldots, m}^{\le d}]$ the subset of
$\mathbb{F}[x_{1, \ldots, m}]$ of polynomials with multidegree at most $d$. We also
need two special cases of these polynomials, which we will want to quickly be
able to reference throughout the paper.

\begin{defn}[{\cite[8]{AW09}}]\label{def:mlin}\index{multilinear}\index{multiquadratic}
  A polynomial is \emph{multilinear} if it has multidegree at most 1. Similarly,
  a polynomial is \emph{multiquadratic} if it has multidegree at most 2.
\end{defn}

\begin{example}
  The polynomial $x_{1}x_{2} + 4x_{2}x_{3} + x_{1}x_{2}x_{3}$ is multilinear. The
  polynomial $x_{1}^{2}x_{2}x_{3} - 2x_{1}x_{3} + 3x_{2}^{2}$ is multiquadratic.
\end{example}

From here, we need to define the notion of an \emph{extension polynomial}. This
gives the ability to take an arbitrary multivariate function defined on a subset
of a field and extend it to be a multivariate polynomial over the \emph{whole}
field.

\begin{defn}[{\cite[8]{AW09}}]\label{def:ext-poly}\index{extension polynomial}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f\colon H^{m} \rightarrow \mathbb{F}$ be a function. An \emph{extension polynomial} of
  $f$ is any polynomial $f' \in \mathbb{F}[x_{1, \ldots, m}]$ such that $f(h) = f'(h)$
  for all $h \in H$.
\end{defn}

\begin{example}
  Define $H = \{0, 1\}^{3} \subseteq \mathbb{R}^{3}$. Further define
  \begin{align*}
    f\colon H^{3} &\rightarrow \mathbb{R} \\
    (a, b, c) &\mapsto a \oplus b \oplus c,
  \end{align*}
  where $\oplus$ is the xor function; equivalently addition mod $2$. Then an
  extension polynomial of $f$ is the function
  \begin{equation*}
    f'(x, y, z) = xyz - (x - y)(y - z)(z - x).
  \end{equation*}
  A second extension polynomial of $f$ is the function
  \begin{equation*}
    f''(x, y, z) = x + y + z - 2xy - 2yz - 2xz + 4xyz.
  \end{equation*}
\end{example}

There are (at least) two important things to be gleaned from this example.
First, extension polynomials are not unique: $f'$ and $f''$ are not equal to
each other (they are not even of the same multidegree). Second, $f''$ is in fact
multilinear, which might be a somewhat lower multidegree than expected given we
need to interpolate 8 different points. It turns out that this is not
particularly unusual: while our choice of $H$ is particularly nice, functions
from this particular $H$ still happen to be quite nice in general. Even for
less-nice values of $H$, extension polynomials need only to be of a surprisingly
low multidegree. Since polynomials of lower degree are generally easier to
compute, we would like to see exactly what these low-degree extension
polynomials look like and how they work.

\begin{defn}[{\cite[\defaultS 5.1]{CFGS22}}]\label{def:low-deg-ext}\index{low-degree extension}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f\colon H^{m} \rightarrow \mathbb{F}$ be a function. A \emph{low-degree extension}
  $\tilde{f}$ of $f$ is an extension of $f$ with multidegree at most
  $\abs{H} - 1$.
\end{defn}

% TODO: Cite these statements
It turns out that this is the minimum possible degree of any extension
polynomial. Further, it turns out that for any $f$, there is a \emph{unique}
low-degree extension. Neither of these statements are particularly important for
our further work, so we will not endeavor to prove them here. Something of
practical use to us is an explicit formula for the low-degree extension, which
we shall now calculate.

\begin{thm}[{\cite[\defaultS 5.1]{CFGS22}}]\label{thm:low-deg-ext-exists}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f\colon H^{m} \rightarrow \mathbb{F}$. Then a low-degree extension $\tilde{f}$ of $f$
  is the function
  \begin{equation}
    \tilde{f}(x) = \sum_{\beta \in H^{m}}\delta_{\beta}(x)f(\beta),
  \end{equation}
  where $\delta$ is the polynomial
  % TODO? Flip x and y here
  \begin{equation}\label{eqn:delta-poly}
    \delta_{x}(y) = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
        \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
      \mright)\mright).
  \end{equation}
\end{thm}

\begin{proof}
  First, we must show $\tilde{f}$ has multidegree $\abs{H} - 1$. First, note
  that $\tilde{f}$ is a linear combination of some $\delta_{x}$es; hence asking about
  the multidegree of $\tilde{f}$ is really just asking about the multidegree of
  $\delta_{x}$. Looking at $\delta_{x}$, the innermost product has $\abs{H} - 1$ terms,
  each with the same $y_{i}$; thus those terms have multidegree $\abs{H} - 1$.
  Summing terms preserves their multidegree, and the outer product iterates over
  the variables, thus it preserves multidegree as well. Thus, $\delta_{x}$ has
  multidegree $\abs{H} - 1$.

  To understand why $\tilde{f}(x)$ agrees with $f(x)$ on $H$, we first should
  look at $\delta_{\beta}(x)$. In particular, for all $x, y \in H^{m}$,
  \begin{equation}\label{eqn:delta-is-delta}
    \delta_{y}(x) = [x = y] = \delta_{xy}.
  \end{equation}
  This can be shown through some algebra which we have worked through in full
  detail in \cref{app:ext-poly}. This is the reason why we have named the
  polynomial in \cref{eqn:delta-poly} as we have; it functions as the Kronecker
  delta function over the set $H^{m}$.

  Taking the above statement, we get that for all $x \in H^{m}$, the only nonzero
  term of $\tilde{f}(x)$ is the term where $\beta = x$; thus $\tilde{f}(x) = f(x)$.
  Hence, $\tilde{f}$ is a low-degree extension of $f$.
\end{proof}

Of particular interest to us will be the case of low-degree extensions where
$H = \{0, 1\}$. Since every field contains both $0$ and $1$, this will allow us
to construct a set consisting of an extension for \emph{every} field. Further,
since $\abs{H} = 2$ here, it means our low-degree extensions will be
multilinear. Not only do we thus constrain our polynomial to have a very low
multidegree, the $\delta$ function also dramatically simplifies in this case, which
makes it much easier to reason about.

\begin{cor}[{\cite[\defaultS 4.1]{AW09}}]\label{cor:low-degree-boolean}
  Let $\mathbb{F}$ be a finite field, $m \in \mathbb{N}$ a number, and
  $f\colon \{0, 1\}^{m} \rightarrow \mathbb{F}$. Then
  \begin{equation}\label{eqn:low-deg-ext-small}
    \tilde{f}(x) = \sum_{\beta \in \{0, 1\}^{m}}\delta_{\beta}(x)f(\beta)
  \end{equation}
  is a low-degree extension of $f$, where $\delta$ is the polynomial
  \begin{equation}\label{eqn:delta-poly-small}
    \delta_{y}(x) = \mleft(\prod_{i:y_{i}=1}x_{i}\mright)\mleft(\prod_{i:y_{i}=0}(1 - x_{i})\mright).
  \end{equation}
\end{cor}
Note that in the product bound $i:y_{i} = 1$, we mean the product over all
numbers $i$ such that $y_{i} = 1$.
% TODO: Prove?

As we can see, the form of $\delta$ in \cref{eqn:delta-poly-small} is much more
manageable than the form in \cref{eqn:delta-poly}, and it is perhaps more
immediately apparent here why $\delta$ has the property it does. Further, since this
equation has no division, it turns out that it is valid for arbitrary
(non-trivial) rings, while the more complex equation is only valid for fields.
We show the algebra that brings us from the first to the second in
\cref{app:ext-poly}.

The form of $\delta_{y}$ defined in \cref{eqn:delta-poly-small} has further use to us
than just being simpler. In particular, these $\delta_{y}$ form a basis of
multilinear polynomials (and hence a generating set for the ring of all
polynomials). This is a particularly useful basis because it allows us to reason
about multilinear polynomials based solely on their outcomes on the Boolean
cube.\footnote{As an aside, this fact provides a relatively slick proof of the
  special case of our unproven statement earlier that low-degree extensions are
  both of minimal degree and unique.}

\begin{thm}[{\cite[\defaultS 4.1]{AW09}}]\label{thm:delta-poly-basis}
  For any field $\mathbb{F}$, the set $\{\delta_{x} \mid x \in \{0, 1\}^{n}\}$ forms a
  basis for the vector space of multilinear polynomials
  $\mathbb{F}^{n} \rightarrow \mathbb{F}$.
\end{thm}

\begin{proof}
  Since $\delta_{y}(x) = 0$ for all $y \ne x \in \{0, 1\}^{n}$, it follows that the only
  way to get
  \begin{equation}
    \sum_{y \in \{0, 1\}^{n}}a_{y}\delta_{y} = 0
  \end{equation}
  is to have each $a_{y} = 0$. Hence the set of $\delta_{x}$ is linearly independent.
  Further, the vector space of multilinear polynomials has $2^{n}$ dimensions;
  since there are $2^{n}$ distinct $\delta_{x}$ polynomials, it follows that they
  form a basis.
\end{proof}

Now, we can use this fact to prove some cases where our low-degree extensions
turn out to have a particularly low degree. Unfortunately, these do have a lot
of qualifiers to them, but they will be useful in later theorems (in particular
\cref{lem:multiquad-adversary}).

\begin{thm}[{\cite[Theorem 4.3]{AW09}}]\label{thm:multiquad-extension}
  Let $\mathbb{F}$ be a field and $Y \subseteq \mathbb{F}^{n}$ be a set of $t$ points
  $y_{1}, \ldots, y_{t}$. Then for at least $2^{n} - t$ Boolean points
  $w \in \{0, 1\}^{n}$, there exists a multiquadratic extension polynomial
  $p\colon \mathbb{F}^{n} \rightarrow \mathbb{F}$ such that
  \begin{enumerate}
    \item $p(y_{i}) = 0$ for all $i \in [t]$,
    \item $p(w) = 1$,
    \item $p(z) = 0$ for all Boolean $z \ne w$.
  \end{enumerate}
\end{thm}

\begin{proof}
  \TODO{}
  % NOTE: AW09 Lemma 4.2 is *almost* a corollary of our Theorem 1.3.5, except
  % for the fact that not all our points are in \{0, 1\}^n
\end{proof}

% TODO: Do we want this here?
% TODO: Add preceding lemmas
% TODO: Come up with descriptive names for these things (adversary polynomials?)
\begin{lemma}[{\cite[Lemma 4.5]{AW09}}]\label{lem:multiquad-adversary}
  Let $\mathcal{F}$ be a collection of fields. Let $f\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ be a
  Boolean function, and for every $\mathbb{F} \in \mathcal{F}$, let
  $p_{\mathbb{F}}\colon \mathbb{F}^{n} \rightarrow \mathbb{F}$ be a multiquadratic
  polynomial over $\mathbb{F}$ extending $f$. Also let
  $\mathcal{Y}_{\mathbb{F}} \in \mathbb{F}^{n}$ for each $\mathbb{F} \in \mathcal{F}$, and define
  $t = \sum_{\mathbb{F} \in \mathcal{F}}\abs{\mathcal{Y}_{\mathbb{F}}}$.

  Then, there exists a subset $B \subseteq \{0, 1\}^{n}$, with $\abs{B} \le t$, such that
  for all Boolean functions $f'\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ that agree with
  $f$ on $B$, there exist multiquadratic polynomials
  $p_{\mathbb{F}}'\colon \mathbb{F}_{n} \rightarrow \mathbb{F}$ (one for each
  $\mathbb{F} \in \mathcal{F}$) such that
  \begin{enumerate}
    \item $p_{\mathbb{F}}'$ extends $f'$, and
    \item $p_{\mathbb{F}}'(y) = p_{\mathbb{F}}(y)$ for all $y \in \mathcal{Y}_{\mathbb{F}}$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Call $z \in \{0, 1\}^{n}$ \emph{good} if for every $\mathbb{F} \in \mathcal{F}$ there exists
  a multiquadratic poylnomial
  $u_{\mathbb{F},z}\colon \mathbb{F}^{n} \rightarrow \mathbb{F}$ such that
  \begin{enumerate}[label=\alph*.]
    \item\label{item:zero-in-y} $u_{\mathbb{F},z}(y) = 0$ for all
          $y \in \mathcal{Y}_{\mathbb{F}}$,
    \item\label{item:delta-one} $u_{\mathbb{F},z}(z) = 1$, and
    \item\label{item:delta-zero} $u_{\mathbb{F},z} = 0$ for all
          $w \in \{0, 1\}^{n} \setminus \{z\}$.
  \end{enumerate}
  % TODO: Rephrase w/more explanation
  From \cref{thm:multiquad-extension}, each $\mathbb{F} \in \mathcal{F}$ can prevent at most
  $\abs{\mathcal{Y}_{\mathbb{F}}}$ points from being good. Since
  $t = \abs{\mathcal{Y}_{\mathbb{F}}}$, there are at least $2^{n} - t$ good points.

  Let $G$ be the set of good points, and thus let $B = \{0, 1\}^{n} \setminus G$ be the
  set of not-good points. Define
  \begin{equation}\label{eqn:p-prime}
    p'_{\mathbb{F}}(x) = p_{\mathbb{F}}(x) + \sum_{z \in G}(f'(z) - f(z))u_{\mathbb{F},z}(x).
  \end{equation}
  Now, all we need is to show that $p'_{\mathbb{F}}(x)$ satisfies the two
  conditions from the theorem statement.

  First, we show that $p_{\mathbb{F}}'$ extends $f'$; that is,
  $p_{\mathbb{F}}'(x) = f'(x)$ for all $x \in \{0, 1\}^{n}$. There are two cases
  here: $x \in G$ and $x \in B$. If $x \in B$, then the sum term of \cref{eqn:p-prime}
  is $0$; hence $p'_{\mathbb{F}}(x) = p_{\mathbb{F}}(x)$. Since
  $p_{\mathbb{F}}(x)$ extends $f(x)$, and since $f(x) = f'(x)$ on $B$, this
  means $p'_{\mathbb{F}}(x) = f'(x)$. If $x \in G$, then the only term of the sum
  where $u_{\mathbb{F},z}(x)$ is nonzero is where $x = G$, as per
  \cref{item:delta-one,item:delta-zero} above. Hence, we have
  \[
    p'_{\mathbb{F}}(x) = p_{\mathbb{F}}(x) + f'(x) - f(x),
  \]
  and since $p_{\mathbb{F}}(x) = f(x)$, it follows that
  $p'_{\mathbb{F}}(x) = f'(x)$.

  Next, we show that $p_{\mathbb{F}}'(y)$ and $p_{\mathbb{F}}(y)$ agree for all
  $y \in \mathcal{Y}_{\mathbb{F}}$. Since by \cref{item:zero-in-y} above, we have that
  $u_{\mathbb{F},z}(y) = 0$, it follows that the entire sum term is zero.
  Therefore, $p'_{\mathbb{F}}(y) = p_{\mathbb{F}}(y)$ for all
  $y \in \mathcal{Y}_{\mathbb{F}}$.

  As such, we have constructed a polynomial $p'_{\mathbb{F}}$ and a set $B$ that
  satisfy our conditions of the theorem.
\end{proof}

\TODO{Unpack all that}

% NOTE: JKRS09 is actually even stronger than this (we need it only to be linear
% in at least one variable)
\begin{lemma}[{\cite[Lemma 7]{JKRS09}}]\label{lem:monomial-sum}
  Let $m(x_{1}, \ldots, x_{n})$ be a multilinear monomial. Over a field of
  characteristic other than 2, we have
  \begin{equation}
    \sum_{b \in \{-1, 1\}}m(b) = 0.
  \end{equation}
\end{lemma}

\begin{proof}
  For some $x_{i}$, we can write $m = x_{i} \cdot m'$, where the degree of $x_{i}$
  in $m'$ is 0. Then
  \begin{align*}
    \sum_{b \in \{1, -1\}^{n}}m(b)
    &= \sum_{a \in \{-1, 1\}}\sum_{b' \in \{1, -1\}^{n-1}}a \cdot m'(b') \\
    &= \sum_{a \in \{-1, 1\}} a \cdot \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) \\
    &= \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) - \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) \\
    &= 0.
  \end{align*}
\end{proof}

\section{Statistics}

In this paper, we will be dealing quite a bit with computers that have access to
randomness. Because these computers now have access to randomness, their outputs
are no longer deterministic: they can return different results depending on the
exact rolls of their random dice.

\begin{defn}\label{def:random-var}\index{random variable}
  A \emph{random variable} is % TODO
\end{defn}

\begin{defn}\label{def:stat-indep}\index{statistical independence}
  Two random variables are \emph{statistically independent} if
\end{defn}

% TODO: Lots more probably needs to go here

% TODO: Talk about "linear independence equals statistical independence" as an intro

\begin{thm}[{\cite[Claim 2]{CFGS22}}]\label{thm:lin-indep-stat-indep}
  Let $\mathbb{F}$ be a finite field and $D$ a finite set. Let
  $V \subseteq \mathbb{F}^{D}$ be a vector space, and let $v$ be a uniform random
  variable over $V$. For any subdomains $S, S' \subseteq D$, the restrictions $v|_{S}$
  and $v|_{S'}$ are statistically dependent if and only if there exist constants
  $c \in \mathbb{F}^{S}$ and $d \in \mathbb{F}^{S'}$ such that
  \begin{enumerate}
    \item There exists $w \in V$ such that $c \cdot w \ne 0$, and
    \item For all $w \in V$, $c \cdot w = d \cdot w$.
  \end{enumerate}
\end{thm}

\begin{proof}
  % TODO
\end{proof}

\chapter{Relativization}

% TODO: Examples

An important prerequisite to understanding algebrization is the similar, but
simpler, concept of \emph{relativization}, also called \emph{oracle separation}.
To do this, we first must define an \emph{oracle}.
\begin{defn}[{\cite[Def.\ 2.1]{AW09}}]\label{def:oracle}\index{oracle}
  An \emph{oracle} $A$ is a collection of Boolean functions
  $A_{m}\colon \{0, 1\}^{m} \rightarrow \{0, 1\}$, one for each natural number $m$.
\end{defn}
There are several ways to think of an oracle; this will extend the most
naturally when it comes time to define an extension oracle in
\cref{def:ext-oracle}. Another way to think of an oracle is as a subset
$A \subseteq \{0, 1\}^{*}$. This allows us to think of $A$ as a language. Since we can
do this, it gives us the ability to think of the complexity of the oracle. If we
want to think about the subset in terms of our functions, we can write $A$ as
\begin{equation}
  A = \bigcup_{m \in \mathbb{N}}\mleft\{x \in \{0, 1\}^{m} \mid A_{m}(x) = 1\mright\}.
\end{equation}
% FIXME: Will we actually do this?
We will use the Iverson bracket defined in \cref{def:iverson-bracket} for this
purpose: allowing us to think of $A$ as the set and $[A]$ as the function.

\begin{example}\label{ex:oracle-function}
  Let $m = 3$. The function
  \begin{equation}
    \begin{aligned}
      f\colon \{0, 1\}^{3} &\rightarrow \{0, 1\} \\
      abc &\mapsto b
    \end{aligned}
  \end{equation}
  is an oracle function. We can think of $f$ as corresponding to the set
  $\{010, 011, 110, 111\}$.
\end{example}

\begin{example}\label{ex:oracle-full}
  For each $n \in \mathbb{N}$, define
  \begin{equation}
    \begin{aligned}
      f_{n}\colon \{0, 1\}^{n} &\rightarrow \{0, 1\} \\
      a_{1}a_{2} \cdots a_{n} &\mapsto a_{n}.
    \end{aligned}
  \end{equation}
  Then the set $\{f_{n}\}$ forms an oracle, whose corresponding language is the
  set of all binary representations of odd numbers.
\end{example}

% TODO: Example for oracle

An oracle is not particularly interesting mathematical object on its own (after
all, it is simply a set of arbitrary Boolean functions); its utility comes from
when it interacts with a Turing machine. A normal Turing machine does not have
the facilities to interact with an oracle, so we need to define a small
extension to a standard Turing machine to allow for this.

\begin{defn}[{\cite[Def.\ 3.6]{AB09}}]\label{def:tm-oracle}\index{Turing machine!with oracle}
  A \emph{Turing machine with an oracle} is a Turing machine with an additional
  tape, called the \emph{oracle tape}, as well as three special states:
  $q_{\text{query}}$, $q_{\text{yes}}$, and $q_{\text{no}}$. Further, each
  machine is associated with an oracle $A$. During the execution of the machine,
  if it ever moves into the state $q_{\text{query}}$, the machine then (in one
  step) takes the output of $A$ on the contents of the oracle tape, moving into
  $q_{\text{yes}}$ if the answer is 1 and $q_{\text{no}}$ if the answer is 0.
\end{defn}

Of course, the question now becomes how we can effectively use an oracle in an
algorithm. The previously-mentioned conception of an oracle as a set of strings
is useful here. If we consider the set of strings as being a \emph{language} in
its own right, then querying the oracle is the same as determining whether a
string is in the langauge, just in one step. If the language is computationally
hard, this means our machine can get a significant power boost from the right
oracle.

\begin{defn}[{\cite[Def.\ 2.1]{AW09}}]\label{def:oracle-class}
  For any complexity class $\mathcal{C}$, the complexity class $\mathcal{C}^{A}$ is the class of all
  languages determinable by a Turing machine with access to $A$ in the number of
  steps defined for $\mathcal{C}$.
\end{defn}

We will be using this definition in many places, so we should take a moment to
look at it in more depth. First, it is important to realize that $\mathcal{C}^{A}$ is a
set of \emph{languages}, not \emph{machines}: despite the notation, augmenting
$\mathcal{C}$ with an oracle does not modify any languages, it just adds new ones that are
computable. Second, since a machine can always ignore its oracle, it follows
that adding an oracle can only increase the number of languages in the class,
never decrease it.

\begin{lemma}\label{thm:relativizing-increases}
  For any complexity class $\mathcal{C}$ and oracle $A$, $\mathcal{C} \subseteq \mathcal{C}^{A}$.
\end{lemma}

\begin{proof}
  Let $L \in \mathcal{C}$ and $M$ be a machine that determines $L$. Then the oracle machine
  $M'$ that simulates $M$ on its input and makes no queries to the oracle will
  also accept exactly $L$. Since $M'$ is a $\mathcal{C}^{A}$ machine for any oracle $A$,
  it follows that $L \in \mathcal{C}^{A}$ and hence $\mathcal{C} \in \mathcal{C}^{A}$.
\end{proof}

While the above lemma tells us that $\mathcal{C} \subseteq \mathcal{C}^{A}$ always, another interesting
question is when $\mathcal{C} = \mathcal{C}^{A}$. We do have a notion for this, called
\emph{lowness}. Lowness can be defined for both individual languages and
complexity classes; we will define both here.

% TODO: Cite all these
\begin{defn}\label{def:low-class}\index{low}
  A language $L$ is \emph{low} for a class $\mathcal{C}$ if $\mathcal{C}^{L} = \mathcal{C}$.
\end{defn}

\begin{defn}\label{def:low-lang}\index{low}
  A complexity class $\mathcal{D}$ is \emph{low} for a class $\mathcal{C}$ if each language in $\mathcal{D}$
  is low for $\mathcal{C}$.
\end{defn}

Of particular interest to us will be classes that are low for \emph{themselves}.
We care about these classes because they can use other problems from the same
class as a subroutine without issue; in particular recursion and iteration both
work here. Thankfully, both $\P$ and $\PSPACE$ are low for themselves (it turns
out $\NP$ is probably not); this allows us to easily write algorithms that
recurse for classes in both of our most common classes.

\begin{thm}\label{thm:p-low}
  $\P$ is low for itself.
\end{thm}

\begin{proof}
  % TODO: Rewrite?
  Let $L \in \P$ and let $K \in \P^{L}$. Let $M(L)$ be the determiner of $L$ and
  $M(K)$ be the determiner of $K$. Further, let $\hat{M}(K)$ be the determiner
  of $K$ but with access to $L$ as an oracle. We aim to show $K \in \P$. Let
  $p_{L}(n)$ be a polynomial upper bound of the runtime of $M(L)$ on an input of
  length $n$, and let $p_{\hat{K}}(n)$ be similar. Since $M(K)$ can call $M(L)$ no
  more than $p_{\hat{K}}(n)$ times, it follows that
  $p_{K}(n) \le p_{\hat{K}}(p_{L}(n))$. Hence, the runtime of $M(K)$ is bounded
  above by a polynomial, and thus $K \in P$.
\end{proof}

\begin{thm}\label{thm:pspace-low}
  $\PSPACE$ is low for itself.
\end{thm}

\begin{proof}
  The proof is very similar to that for \cref{thm:p-low}, but with space instead
  of time. Since memory usage is bounded above by some polynomial, and
  polynomials are closed under composition, it follows that $\PSPACE$ is low for
  itself.
\end{proof}

\section{Defining relativization}

We are now ready to define what relativization is. First, note that
relativization is a statement about a \emph{result}: we talk about inclusions
relativizing, not sets themselves.

% TODO: Cite
\begin{defn}\label{def:relativization}\index{relativization}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \subseteq \mathcal{D}$. We say the result
  $\mathcal{C} \subseteq \mathcal{D}$ \emph{relativizes} if $\mathcal{C}^{A} \subseteq \mathcal{D}^{A}$ for all oracles $A$. Conversely,
  if there exists $A$ such that $\mathcal{C} \nsubseteq \mathcal{D}$, we say that the result $\mathcal{C} \subseteq \mathcal{D}$
  \emph{does not relativize}.
\end{defn}

\begin{defn}\label{def:relativization-ne}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \nsubseteq \mathcal{D}$. We say the result
  $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{relativizes} if $\mathcal{C}^{A} \nsubseteq \mathcal{D}^{A}$ for all oracles $A$. Conversely,
  if there exists $A$ such that $\mathcal{C} \subseteq \mathcal{D}$, we say that the result $\mathcal{C} \nsubseteq \mathcal{D}$
  \emph{does not relativize}.
\end{defn}

We start with a very straightforward example of a relativizing result.

\begin{lemma}\label{lem:pa-subset-npa}
  For any oracle $A$, $\P^{A} \subseteq \NP^{A}$. Equivalently, the result $\P \subseteq \NP$
  relativizes.
\end{lemma}

\begin{proof}
  Since any deterministic Turing machine is also a nondeterministic machine, it
  follows that a machine that solves a $\P^{A}$ problem is also an $\NP^{A}$
  machine. Hence, $\P^{A} \subseteq \NP^{A}$.
\end{proof}

This result tells us that not \emph{everything} is weird in the world of
relativization (although we will soon do our best to find all the weird bits):
if we have a machine that can do more operations without an oracle, it can still
do more operations with an oracle. Further, for the question of $\P$ vs.\ $\NP$
that we will discuss in \cref{sec:rel-p-np}, this means that the question we
care about is whether $\NP \subseteq^{?} \P$ relativizes. As such, the question we are
asking simplifies to determining where $\P^{A} = \NP^{A}$ and where
$\P^{A} \subsetneq \NP^{A}$.

Now that we have talked about set inclusions relativizing, we need to define the
other side of the coin: \emph{proofs} can relativize as well as results.
Unfortunately, this needs to be a somewhat informal definition as formally
delineating different types of proof is far beyond the scope of this paper.
However, the definition we offer here will be sufficient for our purposes.

\begin{defn}\label{def:relativizing-result}
  We say a \emph{proof relativizes} if it is not made invalid if the relevant
  classes are replaced with oracle classes, i.e., a proof that $\mathcal{C} \subseteq \mathcal{D}$
  \emph{relativizes} if the same proof can be used to show $\mathcal{C}^{A} \subseteq \mathcal{D}^{A}$ for
  all oracles $A$ with minimal modifications.
\end{defn}

This gives us a reason to care about relativization as a concept: if our proofs
are relativizing then we know not to try to use them to prove nonrelativizing
results. In particular, we will show in \cref{sec:rel-p-np} that the famous $\P$
vs.\ $\NP$ problem will not relativize regardless of the outcome, and then in
\cref{sec:diag-relativizes} we will show that the common proof technique of
diagonalization \emph{does} in fact relativize.

Now that we have given ourselves a reason to care about oracles and how they
interact with Turing machines, we now turn to the question of how a machine can
gain information about the oracle it queries. We will do this with the notion of
\emph{query complexity}.

\section{Query complexity}\label{sec:query-complexity}

The goal of query complexity is to ask questions about some Boolean function
$A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ by querying $A$ itself. For this, we will
interchangeably think of $A$ as a \emph{function} as well as a bit string of
length $N = 2^{n}$, where each string element is $A$ applied to the $i$th string
of length $n$, arranged in some lexicographical
order. % TODO: Better way to phrase this
We can further think of the property itself as being a Boolean function; a
function that takes as input the bit-string representation of $A$ and outputs
whether or not $A$ has the given property. We will call the function
representing the property $f$. When viewed like this, $f$ is a function from
$\{0, 1\}^{N}$ to $\{0, 1\}$. We define three types of query complexity for
three of the most common types of computing paradigms: deterministic,
randomized, and quantum. Nondeterministic query complexity is interesting, but
it is outside the scope of this paper.
% TODO: Why on earth does this paper not define nondeterministic query complexity?

% TODO: Find better source for these definitions
% Perhaps rephrase in the style of AW09 Def. 4.1?
\begin{defn}[{\cite[17]{AW09}}]\label{def:det-qc}\index{query complexity!deterministic}
  Let $f\colon \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{deterministic query complexity} of $f$, which we write $D(f)$, is the
  minimum number of queries made by any deterministic algorithm with access to
  an oracle $A$ that determines the value of $f(A)$.
  % TODO: I don't quite understand the phrasing here; perhaps rephrase
\end{defn}

To make this more clear, let us give an example problem.

\begin{defn}\label{def:or-problem}\index{OR@$\OR$}
  The $\OR$ problem is the following oracle problem:
  \begin{quote}
    Let $A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ be an oracle. The function $\OR(A)$
    returns 1 if there exists a string on which $A$ returns 1, and $0$
    otherwise.
  \end{quote}
\end{defn}

The question is then what the deterministic query complexity of the $\OR$
function is.

\begin{thm}
  The $\OR$ problem has a deterministic query complexity of $2^{n}$.
\end{thm}

\begin{proof}
  First, note that any algorithm that determines the $\OR$ problem can stop as
  soon as it queries $A$ and gets an output of $1$. Hence, for any algorithm
  $M$, let $\{s_{i}\}$ be the sequence of queries $M$ makes to $A$ on the
  assumption that it always recieves a response of $0$. If
  $\abs{\{s_{i}\}} \le 2^{n}$, there exists some $s \in \{0, 1\}^{n}$ not queried.
  In that case, $M$ will not be able to distinguish the zero oracle from the
  oracle that outputs $1$ only when given $s$. Hence, $M$ must query every
  string of length $n$ and thus the query complexity is $2^{n}$.
\end{proof}

From this, we get that the $\OR$ problem cannot be solved any better than by
enumerative checking. This makes intuitive sense because none of the results we
get by querying $A$ imply anything about what $A$ will do on other values, since
$A$ can be an arbitrary function. Later on (in \cref{sec:alg-query-complexity}),
we will look at what happens when we give ourselves access to a
\emph{polynomial}, where querying one point could tell us information about
others.

For the next two definitions, since their Turing machines include some element
of randomness, we only require that they succeed with a $2/3$ probability. This
is in line with most definitions of complexity classes involving random
computers.

\begin{defn}[{\cite[17]{AW09}}]\label{def:rand-qc}\index{query complexity!randomized}
  Let $f\colon \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{randomized query complexity} of $f$, which we write $D(f)$, is the
  minimum number of queries made by any randomized algorithm with access to an
  oracle $A$ that evaluates $f(A)$ with probability at least $2/3$.
\end{defn}

% TODO: Talk about how quantum oracles are weird?

\begin{defn}[{\cite[17]{AW09}}]\label{def:quant-qc}\index{query complexity!quantum}
  Let $f\colon \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{quantum query complexity} of $f$, which we write $D(f)$, is the minimum
  number of queries made by any quantum algorithm with access to an oracle $A$
  that evaluates $f(A)$ with probability at least $2/3$.
\end{defn}
% TODO: Examples

\section{Relativization of $\P$ vs.\ $\NP$}\label{sec:rel-p-np}

% TODO? move to relativization section as an example?
An important example of relativization is that of $\P$ and $\NP$. While the
question of if $\P = \NP$ is still open, we aim to show that \emph{regardless of
the answer}, the result does not algebrize. To do this, we show that there are
some oracles $A$ where $\P^{A} = \NP^{A}$, and some where $\P^{A} \ne \NP^{A}$.

Additionally, it should be noted that the similarity of relativization to
algebrization means that the structure of these proofs will return in
\cref{sec:alg-p-np} when we show the algebrization of $\P$ and $\NP$.

\subsection{Equality}

The more straightforward of the two proofs is the oracle where
$\P^{A} = \NP^{A}$, so we shall begin with that.

\begin{thm}[{\cite[Theorem 2]{BGS75}}]\label{thm:p-np-rel}
  There exists an oracle $A$ such that $\P^{A} = \NP^{A}$.
\end{thm}

\begin{proof}
  For this, we can let $A$ be any $\PSPACE$-complete language. By letting our
  machine in $\P$ be the reducer from $A$ to any other language in $\PSPACE$, we
  therefore get that $\PSPACE \subseteq \P^{A}$. Similarly, if we have a problem in
  $\NP^{A}$, we can verify it in polynomial space without talking to $A$ at all
  (by having our machine include a determiner for $A$). Hence, we have that
  $\NP^{A} \subseteq \NPSPACE$. Further, a celebrated result of Savitch~\cite{Sav70}
  (which we briefly discussed as \cref{thm:savitch}) is that
  $\PSPACE = \NPSPACE$. Combining all these results, we get the chain
  \begin{equation}
    \NP^{A} \subseteq \NPSPACE = \PSPACE \subseteq \P^{A} \subseteq \NP^{A}.
  \end{equation}
  This is a circular chain of subset relations, which means everything in the
  chain must be equal. Hence, $\P^{A} = \NP^{A} = \PSPACE$.
\end{proof}

For a slightly more intuitive view of what this proof is doing, what we have
done is found an oracle that is so powerful that it dwarfs any amount of
computation our actual Turing machine can do. Hence, the power of our machine is
really just the same as the power of our oracle, and since we have given both
the $\P$ and $\NP$ machine the same oracle, they have the same power.

\subsection{Inequality}

Having shown that an oracle exists where $\P^{A} = \NP^{A}$, we now endeavor to
find one where $\P^{A} \ne \NP^{A}$. This piece of the proof is less simple than
the previous section, and it uses a diagonalization argument to construct the
oracle. Before we dive in to the main proof, however, we need to define a few
preliminaries.

\begin{defn}[{\cite[436]{BGS75}}]\label{def:l(x)}\index{L(X)@$L(X)$}
  Let $X$ be an oracle. The language $L(X)$ is the set
  \begin{equation*}
    L(X) = \{x \mid \text{there is } y \in X \text{ such that } \abs{y} = \abs{x}\}.
  \end{equation*}
\end{defn}

\begin{example}\label{ex:l(x)-simple}
  Consider the language $X = \{0, 11, 0100\}$. The language $L(X)$ is the
  language consisting of all strings of length $1$, $2$, and $4$.
\end{example}

Our eventual goal will be to construct a language $X$ such that
$L(X) \in \NP^{X} \setminus \P^{X}$. Of particular note is that we can rather nicely put a
upper bound on the complexity of $L(X)$ when given $X$ as an oracle, regardless
of the value of $X$. This fact is what gives us the freedom to construct $X$ in
such a way that $L(X)$ will not be in $\P^{X}$.

\begin{lemma}[{\cite[436]{BGS75}}]\label{lem:l(x)-in-np}
  For any oracle $X$, $L(X) \in \NP^{X}$.
\end{lemma}

\begin{proof}
  Let $S$ be a string of length $n$. If $S \in L(X)$, then a witness for $S$ is
  any string $S'$ such that $\abs{S} = \abs{S'}$ and $S' \in X$. Since a machine
  with query access to $X$ can query whether $S'$ is in $X$ in one step, it
  follows that we can verify that $S \in L(X)$ in polynomial time.
\end{proof}

With this lemma as a base, we can now move on to our main theorem.

\begin{thm}[{\cite[Theorem 3]{BGS75}}]\label{thm:p-np-nrel}
  There exists an oracle $A$ such that $\P^{A} \ne \NP^{A}$.
\end{thm}

\begin{proof}
  Our goal is to construct a set $B$ such that $L(B) \notin \P^{B}$. We shall
  construct $B$ in an interative manner. We do this by taking a sequence
  $\{P_{i}\}$ of all machines that recongize some language in $\P^{A}$, and then
  constructing $B$ such that for each machine in the sequence, there is some
  part of $L(B)$ it cannot recognize. This technique is called
  \emph{diagonalization},\index{diagonalization} and it is used in many places
  in computer science theory.\footnote{This argument style is named after
    \emph{Cantor's diagonal argument}, which was originally used to prove that
    the real numbers are uncountable~\cite[Thm. 2.14]{Ru76}.} Additionally, we
  define $p_{i}(n)$ to be the maximum running time of $P_{i}$ on an input of
  length $n$. We aim to show that \cref{alg:construct-b} constructs $B$.

  \begin{algorithm}[htbp]
    % FIXME: Do the P_i need to be P machines? Everybody is unclear on this
    \KwIn{A sequence of $\P$ oracle machines $\{P_{i}\}_{i=1}^{\infty}$}
    \KwOut{A set $B$ such that $L(B) \notin \P^{B}$}
    % TODO: Define p_i(n)
    $B(0) \leftarrow \varnothing$\;
    $n_{0} \leftarrow 0$\;
    \For{$i$ starting at $1$}{
      Let $n > n_{i}$ be large enough that
      $p_{i}(n) < 2^{n}$\;\nllabel{line:def-n}
      Run $P_{i}^{B(i-1)}$ on input $0^{n}$\;\nllabel{line:computation}
      \If{$P_{i}^{B(i-1)}$ rejects $0^{n}$}{
        Let $x$ be a string of length $n$ not queried during the above
        computation\;\nllabel{line:not-queried}
        $B(i) \leftarrow B(i-1) \sqcup \{x\}$\;
      }
      $n_{i+1} \leftarrow 2^{n}$\;
    }
    $B \leftarrow \bigcup_{i}B(i)$\;
    \caption{An algorithm for constructing $B$}\label{alg:construct-b}
  \end{algorithm}

  To begin, let us demonstrate the algorithm's soundness. First, note that since
  $P_{i}$ runs in polynomial time, $p_{i}(n)$ is bounded above by a polynomial,
  and hence there will always exist an $n$ as defined in line~\ref{line:def-n}.
  Next, since there are $2^{n}$ strings of length $n$ and since
  $p_{i}(n) < 2^{n}$, we know that there must be some $x$ to make
  line~\ref{line:not-queried} well-defined. While our algorithm allows $x$ to be
  any string, if it is necessary to be explicit in which we choose, then picking
  $x$ to be the smallest string in lexicographic order is a standard choice.

  We should also briefly mention that this algorithm does not terminate. This is
  okay because we are only using it to construct the set $B$, which does not
  need to be bounded. If this were to be made practical, since the sequence of
  $n_{i}$s is monotonically increasing, the set could be constructed ``lazily''
  on each query by only running the algorithm until $n_{i}$ is greater than the
  length of the query.

  % FIXME: Should this "end goal" section be moved to before the algorithm?
  Next, we demonstrate that $L(B) \notin \P^{B}$. The end goal of our instruction is
  a set $B$ such that if $P_{i}^{B}$ accepts $0^{n}$ then there are no strings
  of length $n$ in $B$, and if $P_{i}^{B}$ rejects, then there is a string of
  length $n$ in $B$. This means that no $P_{i}$ accepts $L(B)$, and hence
  $L(B) \notin \NP^{B}$.

  The central idea behind the proper functioning of our algorithm is that adding
  strings to our oracle \emph{cannot change the output if they are not queried}.
  This is what we do in line~\ref{line:def-n}: we need our input length to be
  long enough to guarantee that a non-queried string exists. Since the number of
  queried strings is no greater than $p_{i}(n)$, and there are $2^{n}$ strings
  of length $n$, there must be some string not queried.

  Next, we run $P_{i}^{B(i-1)}$ on all the strings we have already added. If it
  accepts, then we want to make sure that no string of length $n$ is in $B$;
  that is, $0^{n}$ is not in $L(B)$. Hence, in this particular loop we add
  nothing to $B(i)$. If $P_{i}^{B(i-1)}$ rejects, we then need to make sure that
  $0^{n} \in L(B)$ but in a way that does not affect the output of
  $P_{i}^{B(i-1)}$. Hence, we find a string that $P_{i}^{B(i-1)}$ did not query
  (and thus will not affect the result) and add it to $B(i)$.

  Having done this, we then set $n_{i+1}$ to be $2^{n}$. Since
  $p_{i}(n) < 2^{n}$, it follows that no previous machine could have queried any
  strings of length $n_{i+1}$.\footnote{A word of caution: we only care about
    what $P_{i}$ does on input $n_{i}$, \emph{not any other input}. This is
    because we only need each machine to be incorrect for some $i$, not all
    $i$.} This way, we ensure our previous machines do not accidentally have
  their output change due to us adding a string they queried.

  % TODO: Note about how it's fine that this doesn't actually halt b/c it's just
  % in the construction of the set
  Having run this over all polynomial-time Turing machines, we have a set $L(B)$
  such that no machine in $\P^{B}$ accepts it, which tells us $L(B) \notin \P^{B}$.
  But, \cref{lem:l(x)-in-np} already told us $L(B) \in \NP^{B}$. Hence,
  $\P^{B} \ne \NP^{B}$.
\end{proof}

\section{Diagonalization relativizes}\label{sec:diag-relativizes}

Of course, determining that $\P$ vs $\NP$ does not relativize is only important
if the proof techniques used in practice \emph{do} in fact relativize. Rather
unfortunately, it turns out that simple diagonalization is a relativizing
result.

% FIXME: Are there formal definitions of diagonalization?
While diagonalization itself does not have a formal definition, we can still
think about it informally. Looking at our construction of $B$, which we did
using diagonalization, notice that our definition never really cared about how
the $P_{i}$ worked, just about the results it produced. Hence, if it were to be
possible to modify \cref{alg:construct-b} to construct $B \in \NP \setminus \P$, the proof
would remain the same if we were to replace our sequence $\{P_{i}\}$ with a
sequence of machines in $\P^{A}$ for some $\PSPACE$-complete $A$. However, this
would lead to a contradiction, as we showed in \cref{thm:p-np-rel} that in that
case, $\P^{A} = \NP^{A}$! This tells us that a simple diagonalization argument
would not suffice to determine separation between $\P$ and $\NP$.

\section{Arithmetization does not relativize}\label{sec:arith-non-rel}

While we know that diagonalization relativizes, in the years since the Baker,
Gill, and Solovay paper researchers have discovered proof techniques that do not
in fact relativize. One of these techniques is
\emph{arithmetization}\index{arithmetization}, introduced by~\cite{BF91}.

The idea behind arithmetization is that we want to be able to reduce
computational problems to algebraic ones. More specifically, we would like to
reduce our problems to ones involving low-degree polynomials over a finite field
(such as those seen in \cref{sec:polynomial}). In this paper, we will care about
arithmetization for two reasons: because it is a non-relativizing technique (as
we are about to see) and because we will be using it later on in this paper as
an important part of several proofs. % TODO: List proofs

% TODO

\chapter{Algebrization}\label{chap:algebrization}

Algebrization, originally described by Aaronson and Wigderson~\cite{AW09}, is an
extension of relativization. While relativization deals with oracles that are
Boolean functions, algebrization extends oracles to be a collection of
polynomials over finite fields. Since any field contains the set $\{0, 1\}$, we
can think about our new oracles as \emph{extending} some specific oracle $A$, so
that both oracles agree on the set $\{0, 1\}^{n} \subseteq \mathbb{F}^{n}$. We formalize
this notion below.

\begin{defn}[{\cite[Def.\ 2.2]{AW09}}]\label{def:ext-oracle}\index{extension oracle}
  % TODO: Reuse definition of extension polynomial from earlier
  Let $A_{m}\colon \{0, 1\}^{m} \rightarrow \{0, 1\}$ be a Boolean function and let
  $\mathbb{F}$ be a finite field. Then an \emph{extension} of $A_{m}$ over
  $\mathbb{F}$ is a polynomial
  $\tilde{A}_{m,\mathbb{F}}\colon \mathbb{F}^{m} \rightarrow \mathbb{F}$ such that
  $\tilde{A}_{m,\mathbb{F}}(x) = A_{m}(x)$ whevever $x \in \{0, 1\}^{m}$. Also,
  given an oracle $A = (A_{m})$, an \emph{extension} $\tilde{A}$ of $A$ is a
  collection of polynomials
  $\tilde{A}_{m,\mathbb{F}}\colon \mathbb{F}^{m} \rightarrow \mathbb{F}$, one for each
  positive integer $m$ and finite field $\mathbb{F}$, such that
  \begin{enumerate}
    \item $\tilde{A}_{m,\mathbb{F}}$ is an extension of $A_{m}$ for all
          $m,\mathbb{F}$, and
    \item there exists a constant $c$ such that
          $\mdeg(\tilde{A}_{m,\mathbb{F}}) \le c$ for all $m, \mathbb{F}$.
          % TODO: Rephrase point 2 in terms of F[x_{1,...,n}^{<= c}]
  \end{enumerate}
\end{defn}

Take note that an oracle can have many different extension oracles, since one
can construct an infinite number of polynomials that go through a set of points.
For this reason, when dealing with oracles in practice, we will also often be
interested in oracles of a particular multidegree, which limits our options for
oracles in potentially-interesting ways.

\begin{example}\label{ex:oracle-function-ext}
  Consider the function we defined in \cref{ex:oracle-function}:
  \begin{equation}
    \begin{aligned}
      f\colon \{0, 1\}^{3} &\rightarrow \{0, 1\} \\
      abc &\mapsto b.
    \end{aligned}
  \end{equation}
  An extension of that function is the polynomial
  \begin{equation}
    \begin{aligned}
      \tilde{f}\colon \mathbb{F}^{3} &\rightarrow \mathbb{F}^{3} \\
      (a,b,c) &\mapsto b.
    \end{aligned}
  \end{equation}
  While this is a relatively trivial polynomial, there are more non-trivial
  ones, for example
  \begin{equation}
    \begin{aligned}
      \tilde{f}\colon \mathbb{F}^{3} &\rightarrow \mathbb{F}^{3} \\
      (a,b,c) &\mapsto a^{3}c^{3} + b^{2} - ac.
    \end{aligned}
  \end{equation}
  Notice that on $\{0, 1\}$, $x^{2} = x$, which allows us to see that
  $\tilde{f}$ is a valid extension of $f$.
\end{example}

% TODO: Example of an extension oracle

\begin{defn}[{\cite[Def.\ 2.2]{AW09}}]\label{def:ext-oracle-class}
  For any complexity class $\mathcal{C}$ and extension oracle $\tilde{A}$, the complexity
  class $\mathcal{C}^{\tilde{A}}$ is the class of all languages determinable by a Turing
  machine with access to $\tilde{A}$ with the requirements for $\mathcal{C}$.
\end{defn}

Next, we need to formally define what algebrization is.

\begin{defn}[{\cite[Def.\ 2.3]{AW09}}]\label{def:algebrization}\index{algebrization}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \subseteq \mathcal{D}$. We say the result
  $\mathcal{C} \subseteq \mathcal{D}$ \emph{algebrizes} if $\mathcal{C}^{A} \subseteq \mathcal{D}^{\tilde{A}}$ for all oracles $A$ and
  finite field extensions $\tilde{A}$ of $A$. Conversely, if there exists $A$
  and $\tilde{A}$ such that $\mathcal{C} \nsubseteq \mathcal{D}$, we say that the result $\mathcal{C} \subseteq \mathcal{D}$ \emph{does
    not algebrize}.
\end{defn}

\begin{defn}[{\cite[Def.\ 2.3]{AW09}}]\label{def:algebrization-neq}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \nsubseteq \mathcal{D}$. We say the result
  $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{algebrizes} if $\mathcal{C}^{A} \nsubseteq \mathcal{D}^{\tilde{A}}$ for all oracles $A$ and
  finite field extensions $\tilde{A}$ of $A$. Conversely, if there exists $A$
  and $\tilde{A}$ such that $\mathcal{C} \subseteq \mathcal{D}$, we say that the result $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{does
    not algebrize}.
\end{defn}

\section{Algebraic query complexity}\label{sec:alg-query-complexity}

Similarly to how we defined query complexity in \cref{sec:query-complexity}, our
notion of algebrization requires a definition of \emph{algebraic} query
complexity. % TODO: More (connect to previous section)

\begin{defn}[{\cite[Def. 4.1]{AW09}}]\label{def:aqc}\index{query complexity!algebraic}
  Let $f\colon \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function, $\mathbb{F}$ be a
  field, and $c$ be a positive integer. Also, let $\mathbb{M}$ be the set of
  deterministic algorithms $M$ such that $M^{\tilde{A}}$ outputs $f(A)$ for
  every oracle $A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ and every finite field
  extension $\tilde{A}\colon \mathbb{F}^{n} \rightarrow \mathbb{F}$ of $A$ with
  $\mdeg(\tilde{A}) \le c$. Then, the deterministic algebraic query complexity of
  $f$ over $\mathbb{F}$ is defined as
  \begin{equation}
    \tilde{D}_{\mathbb{F}, c}(f) = \min_{M \in \mathcal{M}}\mleft(
      \max_{A, \tilde{A}: \mdeg(\tilde{A}) \le c}T_{M}(\tilde{A})
    \mright),
  \end{equation}
  where $T_{M}(\tilde{A})$ is the number of queries to $\tilde{A}$ made by
  $M^{\tilde{A}}$.
  % TODO: Can this be made more intelligible?
\end{defn}

Our goal here is to find the \emph{worst}-case scenario for the \emph{best}
algorithm that calculates the property $f$. The difference between this and
\cref{def:det-qc} is twofold: first, our algorithm $M$ has access to
an extension oracle of $A$, and second, that we can limit our $\tilde{A}$ in
its maximum multidegree. For the most part, we will focus on equations with
multidegree 2, which is enough to get the results we want.

As an example, let us look at the same $\OR$ problem we defined in
\cref{def:or-problem}.

% FIXME:
\begin{thm}[{\cite[Thm.\ 4.4]{AW09}}]\label{thm:or-algebraic}
  $\tilde{D}_{\mathbb{F},2}(\OR) = 2^{n}$ for every field $\mathbb{F}$.
\end{thm}

\begin{proof}
  First note that $2^{n}$ is an upper bound for the number of queries necessary
  since we can query every point in $\{0, 1\}^{n}$, of which there are $2^{n}$.

  Let $M$ be a deterministic algorithm and let $\mathcal{Y}$ be the set of points queried
  by $M$ in the case where $M$ always recieves $0$ as a response. So long as
  $\abs{\mathcal{Y}} < 2^{n}$, there exists by \cref{thm:multiquad-extension} a
  multiquadratic extension polynomial
  $\tilde{A}\colon \mathbb{F}^{n} \rightarrow \mathbb{F}$ such that $\tilde{A}(y) = 0$ for
  all $y \in \mathcal{Y}$ but $\tilde{A}(w) = 1$ for some $w \in \{0, 1\}^{n}$. As such, if
  $M$ queries less than $2^{n}$ points then it would not be able to tell the
  difference between $\tilde{A}$ and the zero function. However, $\OR(A) = 1$
  and $\OR(0) = 0$, so it would get the incorrect answer for one of them. Hence
  if $M$ queries fewer than $2^{n}$ points it cannot solve the $\OR$ problem.
\end{proof}

Note that this works even if $M$ is adaptive: if $M$ ever recieves a nonzero
response it (correctly) knows $\OR(A) = 1$, so it can accept immediately. As
such, we know that any contradiction must come when $M$ has only ever seen zeros
as responses.

This gives us a potentially counterintuitive property of algebraic query
complexity: while it would seem that giving our machine a polynomial (and a
polynomial of multidegree only 2, at that) would give us the ability to solve
the hardest problems more quickly, that turns out not to be the case.

Now, while this is true for polynomials of multidegree 2, it turns out that if
we restrict our oracles to being simply \emph{multilinear} polynomials, we do
get a speedup.

\begin{thm}[{\cite[Thm. 3]{JKRS09}}]\label{thm:or-multilinear}
  $\tilde{D}_{\mathbb{F},1}(\OR) = 1$ for every field $\mathbb{F}$ with
  characteristic not equal to $2$.
\end{thm}

\begin{proof}
  % TODO
  Let $A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ and $\tilde{A}$ be our extension
  polynomial. Consider the value of $p(1/2, \ldots, 1/2)$. We aim to show that this
  value is equal to $0$ if and only if $A$ is the zero oracle.

  Consider the function
  \begin{equation}
    p'(x_{1}, \ldots, x_{n}) = p(1 - 2x_{1}, \ldots, 1 - 2x_{n}).
  \end{equation}
  Since $1 - 2x$ is a linear polynomial, it follows that $p'$ is itself a
  multilinear polynomial. Further, since the sum over $\{1, -1\}^{n}$ of a
  non-constant multilinear monomial is 0 as per \cref{lem:monomial-sum}, it
  follows that
  \begin{equation}
    \sum_{b \in \{-1, 1\}^{n}}p'(b) = p'(0, \ldots, 0),
  \end{equation}
  i.e., the constant term of $p'$. Further, from our definition of $p'$, we have
  that $p'(0, \ldots, 0) = p(1/2, \ldots, 1/2)$. Hence, we have
  \begin{equation}
    \sum_{b \in \{0, 1\}^{n}}p(b) = p(1/2, \ldots, 1/2).
  \end{equation}
  Since $p(b) \ge 0$ for all $b \in \{0, 1\}^{n}$, it follows that $p(1/2, \ldots, 1/2)$
  is 0 if and only if $p(b) = 0$ for all $b \in \{0, 1\}^{n}$, i.e. exactly when
  $A$ is the zero function.
\end{proof}

% TODO: Deterministic & nondeterministic AQC

\section{Algebrization of $\P$ vs.\ $\NP$}\label{sec:alg-p-np}

As with relativization, an important application of algebrization is in regards
to the $\P$ vs.\ $\NP$ problem.

\begin{defn}[{\cite[Def.\ 6.1]{BFL90}}]\label{def:pspace-robust}\index{PSPACE-robust@$\PSPACE$-robust}
  A language $L$ is \emph{$\PSPACE$-robust} if $\P^{L} = \PSPACE^{L}$.
\end{defn}

% TODO: Move up above thm:p-np-rel so I can reference it there?
\begin{lemma}\label{lem:complete-is-robust}
  Any $\PSPACE$-complete language is also $\PSPACE$-robust.
\end{lemma}

\begin{proof}
  First, we know from \cref{thm:relativizing-increases} that
  $\P^{L} \subseteq \PSPACE^{L}$. Next, let $M \in \PSPACE^{L}$, and we aim to show
  $M \in \P^{L}$. Since $L \in \PSPACE$ and $\PSPACE$ is low for itself, we know
  $M \in \PSPACE$. As such, we know there is a polynomial-time reduction $f$ from
  $M$ to $L$. Hence, we can compute $M$ by running $f$ on the input and then
  testing if that output is in $L$ (using the oracle). Hence, $M \in \P^{L}$ and
  thus $\P^{L} = \PSPACE^{L}$.
\end{proof}

\begin{lemma}[{\cite[Lemma 6.2]{BFL90}}]\label{lem:multilinear-is-pspace}
  Let $L$ be a $\PSPACE$-robust language, with corresponding oracle $A$. Let
  $\tilde{A}$ be the unique multilinear extension oracle of $A$. Then the
  language
  \begin{equation}
    \tilde{L} = \bigcup_{n \in \mathbb{N}}\{(x_{1}, \ldots, x_{n}, z) \in \mathbb{F}^{n+1} \mid \tilde{A}(x_{1}, \ldots, x_{n}) = z\}
  \end{equation}
  is polynomially-equivalent to $L$; that is, $\tilde{L} \in \P^{L}$ and
  $L \in \P^{\tilde{L}}$.
\end{lemma}

The proof of this statement originally given in~\cite{BFL90} has some apparent
problems; we discuss these more thoroughly later on in \cref{app:bug-in-pspace}.
Instead, we present our own proof of the above lemma.

\begin{proof}
  First, we provide a polynomial-time reduction from $L$ to $\tilde{L}$. Since
  for all $x \in \{0, 1\}^{n}$, $\tilde{A}(x) = 1$ if and only if $x \in L$, it
  follows that
  \begin{equation}
    \begin{aligned}
      f\colon \Sigma^{*} &\rightarrow \Sigma^{*} \\
      x &\mapsto (x, 1)
    \end{aligned}
  \end{equation}
  is a polynomial-time reduction from $L$ to $L'$.

  \begin{algorithm}[htbp]
    \KwIn{$(x_{1}, \ldots, x_{n}, z) \in \mathbb{F}^{n+1}$}
    \KwOut{Whether $\tilde{A}(x_{1}, \ldots, x_{n}) = z$}
    $z' \leftarrow 0$\;
    \For{$k \in \{0, 1\}^{n}$}{\nllabel{line:for-z-prime}
      Simulate $L$ on input $k$\;
      \If{$k \in L$}{
        \tcp{Compute $d_{k}(x)$}
        $d \leftarrow 1$\;
        \For{$i$ from $1$ to $n$}{\nllabel{line:for-d}
          \eIf{$k_{i} = 1$}{
            $d \leftarrow d \cdot x_{i}$\;
          }{
            $d \leftarrow d \cdot (1 - x_{i})$\;
          }
        }
        $z' \leftarrow z' + d$\;
      }
    }
    \Return{whether $z = z'$}\;
    \caption{Determiner for $\tilde{L}$}\label{alg:l-tilde-det}
  \end{algorithm}

  Next, consider \cref{alg:l-tilde-det}. This algorithm simply calculates the
  value of $\tilde{A}(x_{1}, \ldots, x_{n})$ directly, from the explicit definition
  we gave in \cref{cor:low-degree-boolean}, and then compares it to the value of
  $z$. As such, this is a determiner for $L$.

  We now demonstrate that \cref{alg:l-tilde-det} runs in $\P^{L}$. From the
  definition of $\PSPACE$-robustness, we know that we only need to show that the
  algorithm runs in $\PSPACE^{L}$, a much weaker bound. The inner for-loop runs
  in polynomial \emph{time}, hence it must run in polynomial space. The outer
  for-loop runs for $2^{n}$ iterations, so determining that it is in $\P^{L}$ is
  non-trivial. Beyond the inner loop (which we have already discussed), the only
  thing we do in the outer loop is simulate $L$, which can be done in one step
  with access to an oracle for $L$.

  The only memory we need to simulate this oracle (beyond that for the input) is
  space for $d$ and $z'$. We have already shown $d$ needs polynomial space, so
  what remains is $z'$. Since $A(x_{1}, \ldots, x_{n}) \in \{0, 1\}$, each term in the
  sum in \cref{eqn:low-deg-ext-small} is bounded above by $\delta_{\beta}(x)$. This means
  that the value of $z'$ that we compute is bounded above by
  \begin{equation}
    2^{n}\max_{k \in \{0, 1\}^{n}}\delta_{k}(x).
  \end{equation}
  Since each $\delta_{k}(x)$ can be written in polynomial space, and $2^{n}$ can be
  \emph{written} in polynomial space, it follows that $z'$ can as well. Hence,
  \cref{alg:l-tilde-det} is in $\PSPACE^{L}$, and thus is in $\P^{L}$.

  Next, we show that \cref{alg:l-tilde-det} determines $\tilde{L}$. As mentioned
  earlier, our algorithm computes $\tilde{A}$ directly through the equations
  given in \cref{cor:low-degree-boolean}. First, we show the inner loop
  (beginning on line~\ref{line:for-d}) computes $\delta_{k}(x)$. We compute $\delta$
  directly, through the formula described at \cref{eqn:delta-poly-small}. We do
  this by simply iterating through each $i$ and then multiplying $d$ by either
  $x_{i}$ or $1-x_{i}$, as appropriate.

  Second, in this case \cref{eqn:low-deg-ext-small} simplifies to
  \begin{equation}
    \tilde{A}_{n}(x_{1}, \ldots, x_{n}) = \sum_{\beta \in L}\delta_{\beta}(x_{1}, \ldots, x_{n}).
  \end{equation}
  This is exactly what our outer loop does: computes the sum directly through
  iteration.
  Hence, the only thing the above algorithm does is calculate
  $\tilde{A}_{n}(x_{1}, \ldots, x_{n})$ and then compares it to the value we were
  given. As such, it determines $\tilde{L}$.

  Since there is a reduction from $L$ to $\tilde{L}$, we know that $L$ is no
  harder than $\tilde{L}$, and \cref{alg:l-tilde-det} demonstrates that
  $\tilde{L} \in \PSPACE$. Hence, $\tilde{L}$ is $\PSPACE$-complete.
\end{proof}

With that as a base, we can now move on to the main theorem. As before, the more
straightforward proof is the oracle where $\P^{\tilde{A}} = \NP^{A}$, so we
begin with that.

\begin{thm}[{\cite[Theorem 5.1]{AW09}}]\label{thm:p-np-alg}
  There exist $A$, $\tilde{A}$ such that $\NP^{A} = \P^{\tilde{A}}$.
\end{thm}

\begin{proof}
  For this theorem, we use the same technique we did in our proof of
  \cref{thm:p-np-rel}: find a $\PSPACE$-complete language $A$ and work from
  there. If we let $\tilde{A}$ be the unique multilinear extension of $A$,
  \cref{lem:multilinear-is-pspace} tells us $\tilde{A}$ is $\PSPACE$-complete.
  Hence, as mentioned before, we have
  $\NP^{\tilde{A}} \subseteq \NP^{\PSPACE} \subseteq \NPSPACE$, and since $\NPSPACE = \PSPACE$
  and we know from \cref{thm:p-np-rel} that $\PSPACE \subseteq \P^{A}$, it follows
  \begin{equation*}
    \NP^{\tilde{A}} = \NP^{\PSPACE} = \PSPACE = \P^{A}.
  \end{equation*}
\end{proof}

Now it is time for the other case.

\begin{thm}[{\cite[Theorem 5.3]{AW09}}]\label{thm:p-np-nalg}
  There exist $A$, $\tilde{A}$ such that $\NP^{A} \ne \P^{\tilde{A}}$.
\end{thm}

\begin{proof}
  Like in \cref{thm:p-np-nrel}, we aim to ``diagonalize'': iterate over all
  $\P^{\tilde{A}}$ machines to construct a language that none of them can
  recognize. Also like before, we will do this by constructing an oracle
  extension $\tilde{A}$ such that $L(A) \notin \P^{\tilde{A}}$. Since we only give an
  algebraic extension to $\P$ and not $\NP$, we can resuse the result from
  \cref{lem:l(x)-in-np} that $L(A) \in \NP^{A}$. We shall construct $\tilde{A}$
  using the following algorithm:
  \begin{algorithm}[htbp]
    \KwIn{A sequence of $\P$ oracle machines $\{P_{i}\}_{i=1}^{\infty}$}
    \KwOut{An extension oracle $\tilde{A}$ such that $L(A) \notin \P^{\tilde{A}}$}
    $\tilde{A} \leftarrow \varnothing$\;
    $n_{0} \leftarrow 0$\;
    \For{$i$ starting at $1$}{
      Let $n > n_{i}$ be large enough that
      $p_{i}(n) < 2^{n}$\;
      Run $P_{i}^{\tilde{A}}$ on input $0^{n}$\;
      \eIf{$P_{i}^{B(i-1)}$ rejects $0^{n}$}{
        Let $\mathcal{Y}_{\mathbb{F}}$ be the set of all $y \in \mathbb{F}^{n_{i}}$ queried
        during the above computation\;
        \tcp{See \cref{lem:multiquad-adversary} for why we can do this}
        Let $w \in \{0, 1\}^{n}$ such that the following
        works\;\nllabel{line:def-w}
        \For{all $\mathbb{F}$}{
          Set $\tilde{A}_{n_{i},\mathbb{F}}$ to be a multiquadratic polynomial
          such that $\tilde{A}_{n_{i},\mathbb{F}}(w) = 1$ and
          $\tilde{A}_{n_{i},\mathbb{F}}(y) = 0$ for all
          $y \in \mathcal{Y}_{\mathbb{F}} \cup (\{0, 1\}^{n_{i}} \setminus \{w\})$\;\nllabel{line:set-a}
          % FIXME: Be more clear about what this means
        }
      }{
        Set $\tilde{A}_{n_{i},\mathbb{F}} = 0$ for all $\mathbb{F}$\;
      }
      $n_{i+1} \leftarrow 2^{n}$\;
    }
    $B \leftarrow \bigcup_{i}B(i)$\;
    \caption{An algorithm for constructing $\tilde{A}$}\label{alg:construct-a-tilde}
  \end{algorithm}
  As before, we will start by demonstrating soundness and then move on to why
  the constructed oracle provides the separation we seek.

  Perhaps the least intuitive section of the above algorithm is the section
  beginning at line~\ref{line:def-w}. We want to leverage
  \cref{lem:multiquad-adversary} to show that such a solution exists. We know
  that $p_{i}(n) < 2^{n}$, and since $p_{i}(n)$ is an upper bound on the number
  of total queries, this tells us that there is at least one
  $w \in \{0, 1\}^{n_{i}}$ not queried. From the definition of $\mathcal{Y}_{\mathbb{F}}$,
  we also therefore know that $\sum_{\mathbb{F}}\mathcal{Y}_{\mathbb{F}} < 2^{n}$. Further
  setting up this lemma, we will let $f$ be the zero function and
  $p_{\mathbb{F}}$ be the zero polynomial.

  From the lemma, we know that there is some $B \in \{0, 1\}^{n}$ with
  $\abs{B} < 2^{n}$ such that for all $f'$ agreeing with $f$ there exists a
  series of $p_{\mathbb{F}}'$ extending $f'$ and agreeing with $p_{\mathbb{F}}$
  on $\mathcal{Y}_{\mathbb{F}}$. As such, if we pick any $w \in \{0, 1\}^{n} \setminus B$, then the
  function $f'(x) = [x = w]$ agrees with $f$ on $B$, and thus we know that there
  exists a series of $p_{\mathbb{F}}'$ that agree with the zero polynomial on
  $\mathcal{Y}_{\mathbb{F}}$ and each non-$w$ Boolean point.

  Now, we know that such a solution exists, and \cref{eqn:p-prime} gives us an
  explicit formula for our $A_{n_{i},\mathbb{F}}$; thus, we know that this is in
  fact computable. Since this algorithm is simply for \emph{constructing} the
  language, we do not care about time or space complexity, so the fact that it
  is computable is enough. In terms of finding the $w$ we need, we can simply
  iterate try the construction for each $w \in \{0, 1\}^{n}$ and stop as soon as
  we are able to construct each polynomial.

  The other component of soundness is determining how we can run $P_{i}$ with
  the extension oracle $\tilde{A}$ when $\tilde{A}$ is not yet fully
  constructed. What we do is when simulating $P_{i}$, we assume that any
  $\tilde{A}_{n_{i},\mathbb{F}}$ that we have not yet queried returns zero on
  all queried inputs. We then make sure that any time we set an
  $\tilde{A}_{n_{i},\mathbb{F}}$, it also returns zero on any point that we
  queried. Further, we ensure that each $n_{i}$ is large enough that no previous
  machine would have queried any string of length $n_{i}$ on its respective
  input; ergo modifying these polynomials would not have any affect on their
  output.

  Next, we show that $L(A)$ is not in $\P^{\tilde{A}}$. As we did in
  \cref{thm:p-np-nrel}, the idea is that for each polynomial-time machine
  $P_{i}$, that machine will return the incorrect result on the string
  $0^{n_{i}}$. We do this in \cref{alg:construct-a-tilde} by simulating $P_{i}$
  on the input, and then adjusting $\tilde{A}$ based on its output. We separate
  this into two cases: the case where $P_{i}^{\tilde{A}}$ rejects $0^{n_{i}}$,
  and the case where it accepts. We shall begin with the case where it accepts.

  When $P_{i}^{\tilde{A}}$ accepts, we want to ensure that no strings of length
  $n_{i}$ are in $A$. The unique low-degree extension of the zero function is
  the zero polynomial; hence, we set $\tilde{A}_{n_{i},\mathbb{F}}$ to be $0$
  for all $\mathbb{F}$. This ensures $\tilde{A}(x) = 0$ for all
  $x \in \{0, 1\}^{n_{i}}$, and thus $A \cap \{0, 1\}^{n_{i}} = \varnothing$. This
  means $0^{n_{i}} \notin L(A)$ and thus $P_{i}^{\tilde{A}}$ is incorrect.

  When $P_{i}^{\tilde{A}}$ accepts, we want to make sure that there is at least
  some string $w \in A \cap \{0, 1\}^{n_{i}}$, but also to make sure that any
  polynomials we add have their values align with what $P_{i}^{\tilde{A}}$
  already saw. As we mentioned earlier, we know that such a polynomial exists,
  and thus we construct it. Since our constructed polynomials tell us that
  $w \in A$, it follows that $0^{n_{i}} \in L(A)$ and hence $P_{i}^{\tilde{A}}$ is
  incorrect there as well.

  Since our argument earlier told us that none of the $P_{i}$ machines would
  have their output affected by any of the polynomials modified outside of the
  corresponding iteration $i$, it follows that no machine $P_{i}$ could
  recognize $L(A)$. Since $P_{i}$ includes every machine recognizing a
  $\P^{\tilde{A}}$ language, it follows that $L(A) \notin \P^{\tilde{A}}$.
\end{proof}

\section{Arithmetization algebrizes}\label{sec:arith-algebrizes}

In \cref{sec:arith-non-rel}, we mentioned arithmetization as an example of a
technique that does not relativize. One might in fact hope that we can continue
that logic here as well, and show that arithmetization is non-algebrizing.
Unfortunately, this is not the case.\footnote{This is no
  coincidence---algebrization was created pretty much entirely to break this
  technique.}

% TODO

\chapter{Interactive proof systems}\label{chap:ips}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[every text node part/.style={align=center}]
    \draw[rounded corners] (-2.5, -1) rectangle (2.5, 1) node [midway]
    {$\PCP[\log(n), O(1)]$ \\ $\NP$};
    \draw[rounded corners] (-3, -1.5) rectangle (3, 2.5);
    \node[anchor=north] at (0, 2.25) {$\PCP[\poly(n), O(1)]$ \\ $\MIP$ \quad $\NEXP$};
    \draw[rounded corners] (-3.5, -2) rectangle (3.5, 3.5);
    \node[anchor=north] at (0, 3.25) {$\IP$ \quad $\PSPACE$};
    \draw[rounded corners] (-4, -2.5) rectangle (4, 4.5);
    \node[anchor=north] at (0, 4.25) {$\MIP*$ \quad $\RE$};
  \end{tikzpicture}
  \caption{The interactive-proof classes and their
    relationships}\label{fig:ip-class-venn}
\end{figure}

Interactive proof systems are models of computation that involve multiple Turing
machines exchanging messages between each other. In general, the machines are
split into two categories: those that are computationally unbounded but
untrustworthy (the provers), and those that are bounded but trustworthy (the
verifiers). The ``goal'' of the system is to convince the verifier of whether or
not the string is in the language. These systems almost always use randomness as
part of their design: for this reason, almost all of the bounds are ``with high
probability'' bounds and not complete mathematical certainty.

Interactive proof systems turn out to be surprisingly powerful---while the
verifier only runs in polynomial time, it turns out that the interaction with
the untrustworthy computer is still enough to boost the power significantly. The
``classic'' interactive proof model involves exactly two computers (one prover,
one verifier), but many variants exist, all with distinct and interesting
complexity-theoretic characteristics. In this chapter, we will introduce a good
number of these variants and a few of their interesting properties; the goal of
the remaining chapters will be to prove several of the more modern interesting
results involving these systems.

\section{Interactive Turing machines}\label{sec:interactive-tm}

The central mechanism underlying all of the interactive proof systems we will
work with is the interactive Turing machine. This machine is a variant of a
standard Turing machine, but it has the ability to communicate with another
machine as part of its work. When multiple interactive machines work together,
they can produce a joint computation in the same way that a single
non-interactive machine can. From there, an interactive proof is just a pair of
interactive machines working together, with some particular constraints on what
they are allowed to do.

\begin{defn}[{\cite[Def.\ 4.2.1]{Go01}}]\label{def:interactive-tm}\index{Turing machine!interactive}
  An \emph{interactive Turing machine} is a deterministic multi-tape Turing
  machine with the following tapes:
  \begin{itemize}
    \item Input tape (read-only)
    \item Output tape (write-only)
    \item Two communication tapes (one read-only, one write-only)
    \item One-cell switch tape (read-write)
    \item Work tape (read-write)
  \end{itemize}
  In addition to these tapes, an interactive TM has a single bit $\sigma \in \{0, 1\}$
  associated with it, called its \emph{identity}.\index{Turing machine!identity}
  When the content of the switch tape is not equal to the machine's identity,
  the machine performs no computation and is called \emph{idle}.

  In most cases, we will also give the interactive machines a source of
  randomness as well that they can read from. Since this is so common we will
  treat it as the default; if we ever want a machine to not have a source of
  randomness we will explicitly state as such.
\end{defn}

On its own, a single interactive Turing machine is not worth much: in order to
do work with these we need to define how a pair of them interact. The chief
mechanism of interacting Turing machines is that of \emph{shared
  tapes}.\index{shared tape} Shared tapes are tapes where any modifications can
be seen by both Turing machines immediately. While the tapes themselves are
shared, the \emph{heads} are not: the two machines are perfectly capable of
looking at different entries at the same time.

\begin{defn}[{\cite[Def.\ 4.2.2]{Go01}}]\label{def:linked-tms}\index{Turing machine!linked pair}
  A pair of interactive Turing machines $(M, N)$ are \emph{linked} if the
  following are true:
  \begin{enumerate}
    \item The identity of $M$ is distinct from the identity of $N$.
    \item The switch tapes of $M$ and $N$ coincide (i.e., writing to one affects
          the value in both).
    \item The read-only communication tape of $M$ coincides with the write-only
          communication tape of $N$.
    \item The read-only communication tape of $N$ coincides with the write-only
          communication tape of $M$.
  \end{enumerate}
\end{defn}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[>={Stealth[scale=1.25]}]
    % TODO: Make pretty (esp. colors)
    \draw (0,0) rectangle ++(3,3) node [pos=0.5] {$\id = 0$};
    \draw (8,0) rectangle ++(3,3) node [pos=0.5] {$\id = 1$};

    \draw (5.25,4) rectangle ++(0.5,0.5) node [pos=0.5] {$\sigma$};
    \draw[<->] (5.25,4) -- (3,3);
    \draw[<->] (5.75,4) -- (8,3);

    \foreach \y in {0,...,4} {
      \pgfmathsetmacro{\yvar}{2.75 - 0.5 * \y}
      \draw (4.5,\yvar) rectangle ++(0.5,-0.5);
      \draw (6,\yvar) rectangle ++(0.5,-0.5);
    }
    \path (4.5,-0.25) rectangle ++(0.5,0.5) node [pos=0.5] {$\vdots$};
    \path (6,-0.25) rectangle ++(0.5,0.5) node [pos=0.5] {$\vdots$};
    \draw[->] (3,1.5) -- (4.5,1.5);
    \draw[->] (8,1.5) -- (6.5,1.5);
    \draw[<-] (3,2.5) to [bend left] (6,2.75);
    \draw[<-] (8,2.5) to [bend right] (5,2.75);

    \foreach \x in {0,...,9} {
      \pgfmathsetmacro{\xvar}{3 + 0.5 * \x}
      \draw (\xvar,5) rectangle ++(0.5,0.5);
    }
    \draw[->] (3,5) to (1.5,3);
    \draw[->] (8,5) to (9.5,3);

    \foreach \x in {0,...,9} {
      \pgfmathsetmacro{\xvar}{3 + 0.5 * \x}
      \draw (\xvar,-2.5) rectangle ++(0.5,0.5);
    }
    \draw[->] (1.5,0) to (3,-2);
    \draw[->] (9.5,0) to (8,-2);

    \foreach \y in {0,...,4} {
      \pgfmathsetmacro{\yvar}{2.75 - 0.5 * \y}
      \draw (-2,\yvar) rectangle ++(0.5,-0.5);
      \draw (12.5,\yvar) rectangle ++(0.5,-0.5);
    }
    \path (-2,-0.25) rectangle ++(0.5,0.5) node [pos=0.5] {$\vdots$};
    \path (12.5,-0.25) rectangle ++(0.5,0.5) node [pos=0.5] {$\vdots$};
    \draw[<->] (0,1.5) -- (-1.5,1.5);
    \draw[<->] (11,1.5) -- (12.5,1.5);

    \foreach \x in {0,10.25} {
      \draw[rounded corners] (\x,5) rectangle ++(0.75,0.75);
      \foreach \d in {1,2,3}{
        \pgfmathsetmacro{\offset}{\d*0.75/4}
        \pgfmathsetmacro{\circx}{\x+\offset}
        \pgfmathsetmacro{\circy}{5+\offset}
        \filldraw (\circx,\circy) circle (0.05);
      }
    }
    \draw[->] (0.375,5) to (1,3);
    \draw[->] (10.625,5) to (10,3);
  \end{tikzpicture}
  \caption{A linked pair of interactive Turing machines}\label{fig:linked-pair}
\end{figure}

We include a diagram of how a linked pair of Turing machines interact and share
tape as \cref{fig:linked-pair}. The arrows point in the direction data is able
to flow: read-only tapes have an arrow pointing from them and write-only tapes
have an arrow pointing to them.

\begin{defn}[{\cite[Def.\ 4.2.2]{Go01}}]\label{def:joint-comp}\index{joint computation}
  The \emph{joint computation} of a linked pair of interactive Turing machines
  $(M, N)$ is, on a common input string $x$, the series of computation states
  for both $M$ and $N$ when each is given $x$ as its initial input tape and when
  the initial value of the shared switch tape is $0$. The joint computation
  halts when either machine halts and the halting machine is not idle.
\end{defn}

\begin{figure}
  \TODO{}
  \caption{The flow of a joint computation of two interactive
    TMs}\label{fig:joint-comp-flow}
\end{figure}

We will denote the joint computation of machines $M$ and $N$ on input $x$ by
$\ang{M, N}(x)$. Since this output is not deterministic (it will depend on the
values of the random bits read by $P$ and $V$), it is important to note that
this is a random variable, not an individual value.

Finally, we need the concept of a ``view''. A view is in essence a record of
what a machine in a given interaction sees: it is an ordered list of everything
the machine reads in sequence. We will care quite a bit about views throughout
this thesis since views are a record of the ``public'' information of a proof:
when we begin to work with zero knowledge, we will be using the view of an
interaction in order to show that no information is leaked.

\begin{defn}\label{def:view}\index{view}
  The \emph{view} of $M$ in a joint computation on input string $x$ is the
  sequence $(x, r, m_{1}, \ldots, m_{n})$, where $x$ is the input string, $r$ is the
  sequence of random bits seen by $M$, and $m_{i}$ are the random messages
  recieved by $M$ from $N$. We will denote the view of $M$ by
  $\View_{M}^{N}(x)$.
\end{defn}

\section{Single-prover systems}\label{sec:single-prover}

Now that we have a model for letting two machines talk to each other, we can
define the requirements for an interactive proof. We will begin with the
simplest form---that where there is only one prover and one verifier. To make sure
our proof system is useful, we need three restrictions on the machines: to
restrict the complexity of the verifier (lest it simply compute the problem
itself without communication), to require the verifier to generally accept
whenever the input is in the language, and to require the verifier to generally
reject whenever the input is not in the language.

\begin{defn}[{\cite[Def.\ 4.2.4]{Go01}}]\label{def:ips}\index{interactive proof!single-prover}
  An \emph{interactive proof system} is a pair of interactive machines $(P, V)$
  such that $V$ is polynomial-time and the following holds:
  \begin{itemize}
    \item \emph{Completeness}: For every $x \in L$,
          \[
            \mathbb{P}[\ang{P, V}(x) = 1] \ge \frac{2}{3}.
          \]
    \item \emph{Soundness}: For every $x \notin L$ and every interactive machine $B$,
          \[
            \mathbb{P}[\ang{B, V}(x) = 1] \le \frac{1}{3}.
          \]
  \end{itemize}
\end{defn}

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=0.5]
    \draw (0,4) rectangle (4,8) node[midway] {$\mathbb{P} \ge \frac{2}{3}$};
    \draw (4,4) rectangle (8,8) node[midway] {\shrug{}};
    \draw (0,0) rectangle (4,4) node[midway] {$\mathbb{P} \le \frac{1}{3}$};
    \draw (4,0) rectangle (8,4) node[midway] {$\mathbb{P} \le \frac{1}{3}$};
    \node[anchor=east] at (0,6) {$x \in L$};
    \node[anchor=east] at (0,2) {$x \notin L$};
    % Needed to make vertical alignment work out (sigh)
    \node[anchor=south] at (2,8) {\makebox[0pt][l]{$B = P$}\phantom{$B \ne P$}};
    \node[anchor=south] at (6,8) {$B \ne P$};
  \end{tikzpicture}
  \caption{Probability matrix for IP acceptance given prover and input
    string}\label{fig:ip-accept-grid}
\end{figure}

While we require our system to be correct at least $2/3$ of the time, our choice
of probability is actually somewhat arbitrary, so long as it is at least $50\%$.
This is because with a greater than $50\%$ chance of success, we can simply run
the checker multiple times and take the majority vote, which will allow us to
get the probability arbitrarily high. Since this iteration is for a fixed number
of times, it will only linearly scale the runtime and thus it does not affect
whether our algorithm is an interactive proof system.

For the soundness clause, note that we require the inequality to hold for
\emph{any} interactive machine $B$, and not just our chosen machine $P$. This is
important---it says that our verifier cannot be fooled reliably by a dishonest
machine, so long as $x$ is not in the language $L$. In practice, what this means
is that if the verifier has reason to believe that the machine it is interacting
with is not $P$, then it should always reject immediately, as we do not care
what happens with an arbitrary machine when $x \in L$. A consequence of this is
that if $V$ ever recieves back improperly-formatted or nonsense input from its
prover, it will reject immediately. Similarly to what we do for ordinary Turing
machines parsing their input, we will not explicitly write out that $V$ should
reject if it recieves a poorly-formatted response, as it serves little but to
provide clutter.

We also do not care how $V$ fares if $x \in L$ and $P$ is not the correct
verifier. This is because neither insisting the protocol fail or insisting the
protocol succeed will be a reasonable restriction. Since $x \in L$, an alternative
$P'$ could give messages arbitrarily close to the correct $P$ (and in some cases
even send identical messages, which would be impossible to distinguish), so we
cannot insist $L$ reject, even with high probability. However, if $V$ could
accept even in the face of an arbitrarily malicious prover, or a prover that
sends no useful information whatsoever, it would mean that $V$ would have some
means of computing the problem on its own; hence we would only ever be able to
accept languages in $\BPP$ (since $L$ is a $\BPP$ machine).

As with all of our interactive-proof variants, we will also define a complexity
class corresponding to the set of languages with the given proof. Once we have a
complexity class, we will be able to work with it in the same way we have been
all the ``standard'' classes like $\P$ or $\NSPACE$.

\begin{defn}[{\cite[Def.\ 4.2.5]{Go01}}]\label{def:ip}\index{IP@$\IP$}
  The class $\IP$ is the class of all languages that have an interactive proof
  system.
\end{defn}

\TODO{}

\begin{defn}\label{def:view-ip}\index{view!IP}
  The \emph{view} of an interactive Turing machine $M_{1}$ communicating with
  $M_{2}$ is the tuple $(x, m_{1}, \ldots, m_{n})$, where $m_{i}$ is the $i$th
  message recieved from $M_{2}$.
\end{defn}

Now that we have seen the formal definition of an interactive proof, let us
illustrate the formality with an example. To do so, consider the language of
non-isomorphic graphs:

\TODO{Move $\GI$ and $\GNI$ definition to Chapter 1?}
\begin{defn}\label{def:gi}\index{GI@$\GI$}
  The language $\GI$ (for \emph{graph isomorphism}) is the set
  \[
    \GI = \{(G_{0}, G_{1}) \mid G_{0}, G_{1} \text{ graphs and } G_{0} \cong G_{1}\}.
  \]
\end{defn}

This language is interesting for reasons beyond the scope of this paper,
especially in that it is known to be in $\NP$ but is believed to be neither in
$\P$ nor $\NP$-complete.

\begin{defn}\label{def:gni}\index{GNI@$\GNI$}
  The language $\GNI$ (for \emph{graph non-isomorphism}) is the set
  \[
    \GNI = \{(G_{0}, G_{1}) \mid G_{0}, G_{1} \text{ graphs and } G_{0} \ncong G_{1}\}.
  \]
\end{defn}

Here, we will demonstrate that $\GNI$ has an interactive proof.

\FIXME{I think I want $\GI$ here (also in $\IP$ since I'm pretty sure
  $\IP = co\IP$); perhaps I should include both since the $\GI$ IP isn't
  particularly interesting ($\GI \in \NP$ so $P$ can just send the isomorphism)}
\begin{thm}\label{thm:gni-ip}
  The language $\GNI$ is in $\IP$.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{Two $n$-vertex graphs $G_{0}$ and $G_{1}$, and a security parameter $s$}
  \KwOut{Whether $G_{0} \ncong G_{1}$}
  \If{$\abs{V(G_{0})} \ne \abs{V(G_{1})}$ or $\abs{E(G_{0})} \ne \abs{E(G_{1})}$}{
    accept\;
  }
  \For{$i \in [s]$}{
    $V$: Pick a random $\sigma_{i} \in S^{n}$\;
    $V$: Pick a random $b_{i} \in \{0, 1\}$\;
    $V$: Compute $H_{i} \leftarrow \sigma_{i} \cdot G_{b_{i}}$\;
    $V$: Send $H_{i}$ to $P$\;
    $r_{i} \leftarrow$ \eIf{$H_{i} \cong G_{0}$}{
      $P$: Send $0$ to $S$\;
    }{
      $P$: Send $1$ to $S$\;
    }
    \If{$r_{i} \ne b_{i}$}{
      reject\;
    }
  }
  accept\;
  \caption{An interactive proof for the language $\GNI$}\label{alg:gni-ip}
\end{algorithm}

\begin{proof}
  We present an interactive protocol for $\GNI$ in \cref{alg:gni-ip}. In
  addition to the two graphs, we also give this algorithm one metaparameter $s$:
  the \emph{security parameter}\index{security parameter}. This parameter does
  not need to be dynamic; adjusting it only affects the number of rounds of the
  protocol and correspondingly the probability of outputting the correct value.

  First, we show $V$ runs in polynomial time. Picking a random permutation and
  single bit can be done in polynomial time, and computing the action of a
  permutation on a graph is also polynomial.

  Next, if $G_{0} \ncong G_{1}$ and $P$ is the honest prover, we show $V$ will accept
  with probability $\ge 2/3$. Since $G_{0} \ncong G_{1}$, it follows that
  $\sigma \cdot G_{0} \ncong \sigma' \cdot G_{1}$ for any permutations $\sigma$ and $\sigma'$. Hence, the honest
  prover will always answer with the correct $r_{i} = b_{i}$, and thus $V$ will
  always accept.

  If $G_{0} \cong G_{1}$, we show $V$ will reject with probability $\ge 2/3$,
  regardless of the prover. For any permutations $\sigma$ and $\sigma'$, we have that
  $\sigma \cdot G_{0} \cong \sigma' \cdot G_{1}$, by transitivity of isomorphisms. Further, we have
  that $S_{n}$ is exactly the class of isomorphisms on $n$-vertex labeled
  graphs, so for any isomorphic graph $G$, there exists exactly one $\sigma_{0}$ and
  $\sigma_{1}$ such that $\sigma_{0} \cdot G_{0} \cong G \cong \sigma_{1} \cdot G_{1}$. Hence, the odds of
  $b_{i}$ being $0$ are $1$ are equal for any given $G$. Thus, in each round the
  odds of $P$ guessing correctly are no more than $1/2$. Further, in each round
  the random picks are statistically independent; hence the overall odds of $P$
  fooling $V$ are no more than $2^{-s}$. For any $s > 1$, $2^{-s} < 1/3$; hence
  $V$ will reject with probability $\ge 2/3$.
\end{proof}

Once we have a complexity class, the question arises of how it relates to other
complexity classes. For $\IP$, Adi Shamir proved in 1992~\cite{Sha92} that a
language has a standard interactive protocol if and only if it is in $\PSPACE$.

\TODO{How much do I want to talk about this? (Scope creep)}
\begin{thm}[{\cite{Sha92}}]\label{thm:ip-is-pspace}
  $\IP = \PSPACE$.
\end{thm}

\begin{proof}
  \TODO{}
\end{proof}

\section{Multi-prover systems}\label{sec:multi-prover}

We have now seen quite a bit of single-prover interactive proofs. A natural
extension of the standard interactive proof format is to add more machines to
the interaction. Since our verifiers are trusted, increasing the number of
verifiers is not useful since any pair of verifiers could simply be simulated
with a single verifier working twice as hard (which would keep it polynomial).
However, increasing the number of provers to two turns out to give us more power
than we would get with a single prover.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[>=Stealth]
    \draw (0, 0) rectangle ++(1, 1) node [midway] {$V$};
    \draw (-2, 2) rectangle ++(1, 1) node [midway] {$P_{1}$};
    \draw (2, 2) rectangle ++(1, 1) node [midway] {$P_{1}$};
    \draw[<->] (0, 1) to (-1, 2);
    \draw[<->] (1, 1) to (2, 2);
    \draw[very thick,line cap=round] (0.5, 1.5) to (0.5, 3.5);
  \end{tikzpicture}
  \caption{A multi-prover interactive system}\label{fig:mip}
\end{figure}

\begin{defn}[{\cite[Def.\ 4.11.2]{Go01}}]\label{def:mps}\index{interactive proof!multi-prover}
  A \emph{multi-prover interactive proof system} is a triplet of interactive
  machines $(P_{1}, P_{2}, V)$ such that $P_{1}$ and $P_{2}$ cannot communicate,
  $V$ is probabilistic polynomial-time, and the following hold.
  \begin{itemize}
    \item \emph{Completeness}: For every $x \in L$,
          \[
            \mathbb{P}[\ang{P_{1}, P_{2}, V}(x) = 1] \ge \frac{2}{3}.
          \]
    \item \emph{Soundness}: For every $x \notin L$ and every pair of interactive
          machines $B_{1}$ and $B_{2}$,
          \[
            \mathbb{P}[\ang{B_{1}, B_{2}, V}(x) = 1] \le \frac{1}{3}.
          \]
  \end{itemize}
\end{defn}

The above definition should look rather similar to \cref{def:ips}; the only
difference is now we have two provers instead of just one. The fact that the two
provers cannot communicate is important: if they could, they would be able to
``strategize''; that is, agree on a joint plan to make sure that their responses
agree with each other. Since our provers are not required to be computationally
bounded, if they could communicate it would be no different than simply having
one prover. However, since the two provers cannot communicate, we gain some
information from times where they lie in \emph{different} ways: where each
prover individually could say something plausible, but in combination, the
provers' responses contradict.

Later on, we will see what can happen if the computers are allowed a limited
form of communication: the class $\MIP*$, which uses quantum provers that can
share entangled bits, will be defined in \cref{sec:quant-interactive} and then
explored in more detail in \cref{chap:ipcp-to-mip}.

\TODO{}

\begin{defn}\label{def:view-mip}\index{view!MIP}
  The \emph{view} of a multi-prover interactive system is \TODO{}
\end{defn}

\TODO{}

\begin{defn}\label{def:mip}\index{MIP@$\MIP$}
  The class $\MIP$ is the class of languages that have a multi-prover
  interactive proof system.
\end{defn}

The first question about a class like $\MIP$ is how it relates to other
complexity classes we have already seen. First, if we have a single-prover
system we can always convert it into a multi-prover system by simply having the
verifier ignore $P_{2}$ completely; thus we get the following.

\begin{lemma}\label{lem:ip-in-mip}
  $\IP \subseteq \MIP$.
\end{lemma}

\begin{proof}
  Let $(P, V)$ be an interactive proof system. Consider the system
  $(P_{1}, P_{2}, V')$, where $P_{1} = P$, and $V'$ simulates $V$ and sends all
  its messages to $P_{1}$. If $x \in L$, then $\ang{P_{1}, P_{2}, V'}(x)$ will
  accept with exactly the same probability as $\ang{P, V}(x)$. Since $(P, V)$ is
  an interactive proof system, it follows that it will accept with probability
  at least $2/3$. If $x \notin L$, then $\ang{P_{1}^{*}, P_{2}^{*}, V}$ will reject
  with exactly the same probability as $\ang{P_{1}^{*}, V}$ regardless of the
  value of $P_{2}^{*}$, since the interaction is exactly the same and since we
  ignore the second prover completely. Again, since $(P, V)$ is an interactive
  proof system, it follows that it will reject with probability at least $2/3$
  in this case. Hence, $(P_{1}, P_{2}, V')$ is a MIP system.
\end{proof}

A groundbreaking result by Babai, Fortnow, and Lund~\cite{BFL90} is that $\MIP$
is exactly equal to $\NEXP$. Since it is known that $\NP \ne \NEXP$~\cite{Cook73},
this tells us that adding multiple provers gives an actual boost in
computational power over just having one.

% https://www.math.toronto.edu/swastik/courses/rutgers/topics-S17/lec9.pdf
\begin{thm}[{\cite{BFL90}}]\label{thm:mip-is-nexp}
  $\MIP = \NEXP$.
\end{thm}

\begin{proof}
  \TODO{How much effort do I want to put into this? (Scope creep)}
  We showed in \cref{thm:o3sat-nexp-complete} that $\OSAT$ is $\NEXP$-complete,
  so all we need is to demonstrate that $\OSAT$ has a multi-prover system.
\end{proof}

Having seen how much more powerful systems become with two provers, one might
wonder what would happen if we were to add a third. Unfortunately, it turns out
that a third prover is no more powerful than just having two. We formalize this
below; because we do not get any benefit from three provers we will not work
with three-prover systems at all in this paper beyond this proof.

\begin{thm}[{\cite[Theorem 4]{BGKW88}}]\label{thm:mip-unchanged}
  If we redefine $\MIP$ to have $k$ provers instead of $2$, the class is unchanged.
\end{thm}

\begin{algorithm}[htbp]
  $\hat{V}$: Generate all random bits and send results to $\hat{P}_{1}$\;
  $\hat{P}_{1}$: Send transaction log between $(P_{1}, \ldots, P_{k}, V)$ with the
  chosen randomness\;
  \If{The simulated log is longer than the worst-case runtime of $V$}{
    reject\;
  }
  $\hat{V}$: Choose random $i \in [k]$\;
  $\hat{V}$ and $\hat{P}_{2}$: Simulate conversation between $V$ and $P_{i}$
  given coin tosses\;
  \eIf{the simulated conversation does not match the result of $\hat{P}_{1}$}{
    reject\;
  }{
    accept if and only if $V$ would accept with the given transcript\;
  }
  \caption{A 2-prover MIP simulating a $k$-prover MIP}\label{alg:2-prover-sim-k}
\end{algorithm}

\begin{proof}
  Let $(P_{1}, \ldots, P_{k}, V)$ be a $\MIP$ with $k$ provers. We show a simulator
  for $(P_{1}, \ldots, P_{k}, V)$ in \cref{alg:2-prover-sim-k}. Since $V$ runs in
  polynomial time and all $\hat{V}$ does is look at the simulated transaction
  log of $V$ (twice), it follows that $\hat{V}$ runs in polynomial time.

  Let $x \in L$. By definition, we know that
  \begin{equation}
    \mathbb{P}[\ang{P_{1}, \ldots, P_{k}, V}(x) = 1] \ge \frac{2}{3}.
  \end{equation}
  If $\hat{P}_{1}$ and $\hat{P}_{2}$ are honest, then the simulated
  conversations will match; hence $\hat{V}$ will reject if and only if $P$ would
  have. Thus, $\mathbb{P}[\ang{\hat{P}_{1}, \hat{P}_{2}, \hat{V}} = 1] \ge 2/3$.

  Let $x \notin L$, and let $\hat{P}_{1}^{*}$ and $\hat{P}_{2}^{*}$ be arbitrary
  verifiers. From the definition of MIP, for at least $2/3$ of the choices of
  randomness, the generated transcript would result in $V$ rejecting; hence
  $\hat{P}_{1}$ must deviate from it somewhere. Since $\hat{P}_{1}$ deviates
  from the protocol at least once, it follows there is at least a $1/k$ chance
  that the simulated conversation between $\hat{V}$ and $\hat{P}_{2}$ is
  different from what $\hat{V}$ recieved from $\hat{P}_{1}$. Hence, the
  probability of correctly rejecting here is at least $2/3k$.

  Note that if the two provers ever disagree, it must be that at least one of
  them is a cheating prover. Thus, if we run it at least $k^{2}$ times we will
  have at least one run where we catch the cheating with probability
  $2k^{2}/3k = 2/3$. Thus, we will correctly reject with probablity at least
  $2/3$ regardless of the cheating prover. Hence, $(P_{1}, P_{2}, V)$ is a valid
  MIP system.
\end{proof}

\section{Zero-knowledge proofs}\label{sec:zero-knowledge}

Zero-knowledge proofs are a variant of interactive proofs that have certain
cryptographic requirements. What we care about is the idea that zero-knowledge
proofs transmit \emph{no knowledge} other than precisely the statement trying to
be proved. As an example, if the statement that you are trying to prove is ``I
have an instance of $X$'', the conceptually-easiest way to prove it would be to
produce the aforementioned instance. However, this would not be zero-knowledge
since it also transmits the knowledge of exactly what your instance of $X$ is.

The way we mathematically define zero-knowledge is a little tricky. The way we
demonstrate that the proof is zero-knowledge is by creating a simulator $S_{V}$
for each possible verifier $V$: a machine in $\P$ that \emph{by itself} can
reproduce the entire message log between $P$ and $V$ for any input.

This definition shows that no knowledge has been released because we are able to
reproduce all the public information of the proof with relatively little work.
If non-trivial knowledge were released by the proof, we would not be able to
recreate the interaction faithfully without access to the
prover.\footnote{Exception: if $L \in \P$ then we can trivially recreate the
  interaction no matter what, but that case is not particularly interesting for
  the purpose of zero-knowledge proofs.}

Having said that, it is not particularly obvious that there are any languages
that are outside of $\P$ with perfect zero-knowledge proofs.\footnote{To some
  extent, showing that there are languages \emph{truly} outside of $\P$ would
  require a proof that $\P \ne \NP$ (which is unfortunately beyond the scope of
  this paper), but there are lots of languages strongly believed to be outside
  of $\P$ with zero-knowledge proofs.} It turns out, however, that these
languages do in fact exist (and are reasonably common). Abstractly, the idea
behind why many of these work is that the verifier can perform a transformation
on some random value, such that undoing the transformation and reliably
recovering the original value is only possible with knowledge of the language.
However, a simulator would have access to the randomly-chosen value, and thus it
could construct a response immediately with no reference to the problem to be
solved.

\begin{defn}[{\cite[Def.\ 4.3.1]{Go01}}]\label{def:zero-knowledge}%
  \index{perfect zero-knowledge}\index{perfect simulator}%
  \index{zero-knowledge proof!perfect}
  A proof system $(P, V)$ for a language $L$ is \emph{perfect zero-knowledge} if
  for each probabilistic polynomial-time interactive machine $V^{*}$ there
  exists a probabilistic polynomial-time ordinary machine $M^{*}$ such that for
  every $x \in L$ we have the following conditions hold:
  \begin{enumerate}
    \item With probability at most $1/2$ , on input $x$, machine $M^{*}$ outputs
          a special symbol denoted $\bot$ (i.e.\ $\mathbb{P}[M^{*}(x) = \bot] \le 1/2$).
    \item Let $m^{*}(x)$ be the random variable such that
          \begin{equation}
            \mathbb{P}[m^{*}(x) = \alpha] = \mathbb{P}[M^{*}(x) = \alpha \mid M^{*}(x) \ne \bot]
          \end{equation}
          for all $\alpha$. That is, let $m^{*}(x)$ be the distribution of non-$\bot$
          values of $M^{*}$. Then $\ang{P, V^{*}}(x)$ and $m^{*}(x)$ are
          identically distributed for all $x \in L$.
  \end{enumerate}
  In this case, we say the machine $M^{*}$ is a \emph{perfect simulator} for the
  interaction of $V^{*}$ with $P$.
\end{defn}

Looking at this definition, one might wonder why we would want the ability for
$M^{*}$ to output $\bot$. This is a reasonable thing to wonder, because we do not
actually want this. However, we do not know of any non-trivial proof systems
that do not actually output $\bot$ at least some of the time, although~\cite{GT20}
has made significant inroads on this problem.\footnote{More specifically, they
  have shown that all of $\NP$ has a zero-knowledge proof without use of $\bot$,
  which includes nontrivial problems in the (generally expected) case that
  $\NP \ne \BPP$.} Thankfully, we can make $\mathbb{P}[\bot]$ aribitrarily small (bounded
above by $2^{-\poly(\abs{n})}$), but we cannot make it truly perfect. We can do
this by simply re-running the simulator every time we get a $\bot$ until we get a
valid answer.

Also note that while the definition of interactive proof systems focus on
cheating \emph{provers}, the definition of zero-knowledge focuses on cheating
\emph{verifiers}. This is because we can think of the two as being resilient to
different threat models. For interactive proofs, we care about verifier
efficiency: our goal is to show it is possible for some computer with unbounded
resources to easily convince a verifier of something, and to show that no other
equally-strong computer could lie.

For zero-knowledge, our main goal is to ensure tha it is impossible for anybody
except for the honest verifier to extract any information from the honest
prover. The main way we do this is by ensuring that it is impossible for anybody
snooping on the transaction to gain any information (hence the simulator), but
we also want to ensure that the prover cannot be tricked into revealing
information by a dishonest verifier. We do not care about what happens with a
dishonest prover in this case because alternate provers could in theory reveal
anything---there is always a prover that just dumps any relevant information to
the interaction tape, for example.

As with other interactive proof systems, zero-knowledge proofs are
probabilistic; in particular this means they do \emph{not} function as proofs in
the mathematical sense.

\begin{defn}[{\cite[Def.\ 4.3.5]{Go01}}]\label{def:pzk}\index{PZK@$\PZK$}
  The class $\PZK$ is the class of all languages with a perfect zero-knowledge
  proof system.
\end{defn}

To demonstrate perfect zero-knowledge, we now show an example. In
\cref{sec:single-prover}, we demonstrated a non-zero-knowledge interactive proof
for the language $\GNI$ of non-isomorphic graphs. Here, we modify that algorithm
to not reveal anything beyond simply whether $G_{0}$ and $G_{1}$ are isomorphic.

\begin{thm}\label{thm:gi-pzk-ip}
  The language $\GI$ has a perfect zero-knowledge interactive proof.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{Two $n$-vertex graphs $G_{0}$ and $G_{1}$}
  \KwOut{Whether $G_{0} \cong G_{1}$}
  \For{$i \in [s]$}{
    $P$: pick a random $\sigma \in S^{n}$\;
    $P$: pick a random $b \in \{0, 1\}$\;
    $P$: send $\sigma \cdot G_{b}$ to $V$\;\nllabel{line:send-perm}
    $V$: pick a random $b' \in \{0, 1\}$\;
    $V$: send $b'$ to $P$\;\nllabel{line:send-b}
    $P$: compute $\sigma' \in S^{n}$ such that
    $\sigma' \cdot G_{b'} = \sigma \cdot G_{b}$\;\nllabel{line:sigma-prime}
    $P$: send $\sigma'$ to $V$\;
    \If{$\sigma' \cdot G_{b'} \ne \sigma \cdot G_{b}$}{\nllabel{line:confirm-sigma}
      reject\;
    }
  }
  accept\;
  \caption{A perfect zero-knowledge IP for $\GI$}\label{alg:gi-pzk-ip}
\end{algorithm}

\begin{algorithm}[htbp]
  $c \leftarrow 0$\;
  $L \leftarrow [\,]$\;
  \For{$i \in [2s]$}{
    \If{$c \ge s$}{
      \KwRet{$L$}\;
    }
    Choose random $\sigma \in S_{n}$\;
    Choose random $b \in \{0, 1\}$\;
    $H \leftarrow \sigma \cdot G_{b}$\;
    Add $H$ to $L$\;
    Simulate $V^{*}$ on input $(G_{0}, G_{1})$ and recieved message $H$ until it
    sends a message $\sigma'$\;\nllabel{line:sim-vstar-gi}
    \If{$b = b'$}{
      $c \leftarrow c + 1$\;
      Add $\sigma$ to $L$\;
    }
  }
  \KwRet{$\bot$}\;
  \caption{A simulator for \cref{alg:gi-pzk-ip}}\label{alg:gi-simulator}
\end{algorithm}

\begin{proof}
  We present the algorithm as \cref{alg:gi-pzk-ip}. Our proof will consist of
  two stages: first, we will prove that the algorithm is a functional
  interactive proof for $\GI$, and then we will show that it does not leak any
  knowledge.

  First, if $G_{0} \cong G_{1}$ and $P$ is honest, then regardless of what
  parameters we pick, there will always exist a $\sigma'$ we can compute in
  line~\ref{line:sigma-prime}. Hence, the condition in
  line~\ref{line:confirm-sigma} will never be true and hence we will always
  accept.

  If $G_{0} \ncong G_{1}$, then whenever $b' \ne b$ there exists no $\sigma'$ such that
  $\sigma' \cdot G_{b'} = \sigma \cdot G_{b}$. Hence, if $b' \ne b$ (which happens with probability
  $1/2$) then regardless of what $\sigma'$ is sent to $V$ the check in
  line~\ref{line:confirm-sigma} will fail and hence $V$ will reject. Since $b'$
  and $b$ are re-rolled in each round, the probability of them being equal in
  all $s$ rounds is $2^{-s}$. Hence, $V$ will accept with probability no more
  than $2^{-s}$.

  Now, we show zero-knowledge. To do this we show a simulator in
  \cref{alg:gi-simulator}. To summarize, it attempts to simulate a single round
  of the interaction repeatedly until it has $s$ successes; if it cannot do this
  in $2s$ tries, it outputs $\bot$.

  First, we show \cref{alg:gi-simulator} runs in polynomial time. We know
  $V^{*}$ runs in polynomial time by our hypothesis; hence
  line~\ref{line:sim-vstar-gi} runs in polynomial time. All the rest of the
  lines are either random choice of items, computing permutations, or simple
  arithmetic; all of these are also doable in polynomial time. Hence the
  interior of the loop runs in polynomial time, and since we iterate no more
  than $2s$ times, it follows that the whole algorithm is polynomial time.

  Note that an individual round of this simulator can only succeed when
  $b = b'$: if it could output a correct response $\sigma'$ when $b \ne b'$, then it
  would have $\sigma' \circ \sigma^{-1}$ as an isomorphism from $G_{0}$ to $G_{1}$, and thus
  \cref{alg:gi-simulator} would be a probabilistic polynomial-time determinier
  for $\GI$. Since we do not know whether or not $\GI \in \BPP$, we do not yet
  know how to construct any verifier that would do this.

  Since $V^{*}$ never recieves $b$ and $b$ is randomly generated, if $G_{0}$ is
  isomorphic to $G_{1}$, for any value of $G$ it is just as likely that $b = 0$
  as it is that $b = 1$. More formally, for all $G$,
  \[
    \mathbb{P}[G \mid b = 0] = \mathbb{P}[G \mid b = 1].
  \]
  As such, for any $(G_{0}, G_{1}) \in \GI$, it is impossible for any $V^{*}$ to
  send $b' \ne b$ with probability more than $1/2$.

  Since, each individual round succeeds with probability at least $1/2$, the
  binomial theorem tells us that the probability of exactly $k$ successes in
  $2s$ tries is $\binom{2s}{k}/2^{2s}$. Hence, the probability of at least $s$
  successes in $2s$ tries is
  \begin{equation}\label{eqn:binom-sum-2s}
    \frac{1}{2^{2s}}\sum_{i=s}^{2s}\binom{2s}{i}.
  \end{equation}
  Since $\binom{2s}{k} = \binom{2s}{2s - k}$ and the sum of $\binom{2s}{k}$ over
  all $k$ is $2s$, it follows that \cref{eqn:binom-sum-2s} is equal to
  \begin{equation}
    \frac{\displaystyle \sum_{i=s}^{2s}\binom{2s}{i}}{\displaystyle \sum_{i=1}^{2s}\binom{2s}{i}}
    = \frac{\displaystyle s + \sum_{i=s+1}^{2s}\binom{2s}{i}}{2\displaystyle \sum_{i=s+1}^{2s}\binom{2s}{i}}
    \ge \frac{1}{2}.
  \end{equation}
  Hence, the algorithm will output $\bot$ with probability no more than $1/2$.

  Next, we show \cref{alg:gi-simulator} outputs an identically-distributed view
  to what $V^{*}$ sees for all $x \in \GI$, regardless of what $V^{*}$ is. We show
  that it outputs an identical view for a single round; since the simulator will
  simulate $s$ rounds it follows that the total result will be identical exactly
  when an individual round is.

  An individual round of \cref{alg:gi-pzk-ip} has a total of three messages
  sent: two from $P$ and one from $V$. The view of a verifier only consists of
  the messages from $P$, so after each round we should be adding two items to
  the log. Our honest prover sends two messages: first, a random graph
  $H \cong G_{b}$ and second, an isomorphism $\sigma$ that maps $G_{b'}$ to $H$.

  The simulator picks a random $b$ and $H$ in exactly the same way as $P$; thus
  $H$ will be identically distributed in the simulator as it is in the original
  algorithm. Next, remember that we only care about the views being identical in
  the case where $(G_{1}, G_{2}) \in \GI$, that is, where $G_{1} \cong G_{2}$. In this
  case, picking a random isomorphic copy of $G_{1}$ will give you an
  identically-distributed random variable to picking a random isomorphic copy of
  $G_{2}$. Similarly, $H \cong G_{1} \cong G_{2}$ by construction, so picking a random
  isomorphic copy of $H$ will also give you an identical distribution. As such,
  $\sigma$ is a random isomorphism and hence the transaction $(H, \sigma)$ is distributed
  identically to the originally-generated transcript.
\end{proof}

\subsection{Commitment schemes}\label{sec:commitment-scheme}

It should not come as too much of a surprise to learn that most intuitive proofs
for a given problem are not in fact zero-knowledge.\footnote{As an example, I
  would certainly hope that the proofs in this text have, in fact, imparted at
  least some knowledge on the reader.} As such, we will want the assistance of a
few techniques that, once understood, will allow us to build zero-knowledge
proofs more easily. The first of these is a \emph{bit-commitment scheme}.

Abstractly, a bit-commitment scheme allows a machine to ``commit to'' a given
single bit, with the intent of revealing it later on to a verifier. To do this,
we need two important things: first, that the bit is not revealed to the
verifier at commitment time, and second, that if the revealed bit is not equal
to the committed bit, the verifier will be able to know (that is, the committer
will not be able to change its choice once it has committed).

Before we can define a bit-commitment scheme formally, we do need a few
preliminaries. First, we define what it means for an interaction to look like a
commitment from the reciever's perspective; since the reciever needs to be
convinced of the commitment, we need a definition that only considers its
perspective.

\begin{defn}\label{def:possible-commit}\index{possible commitment}
  Let $\sigma \in \{0, 1\}$. A reciever's view of an interaction
  $(x, r, m_{1}, \ldots, m_{n})$ is a \emph{possible $\sigma$-commmitment} if there exists
  a string $s$ such that $m_{i}$ describes the messages received by $R$ when $R$
  uses local coins $r$ and interacts with machine $S$ that uses local coins $s$
  and has input $(\sigma, 1^{n})$.
\end{defn}

The above definition does not preclude a series of messages looking like it
could be both a $0$-commitment and a $1$-commitment; consider the case of a
machine $S$ that completely ignores its input and sends the same series of
strings. In this case, the record of that interaction would be a possible
commitment for any input, since the input is ignored completely.

\begin{defn}\label{def:ambiguous-view}\index{ambiguous view}
  A reciever's view is \emph{ambiguous} if it is both a possible $0$-commitment
  and a possible $1$-commitment.
\end{defn}

Now, we can define a bit-commitment scheme. In brief, a bit-commitment scheme is
a scheme such that there are no ambiguous views and yet the total
publicly-released information is ambiguous.

\begin{defn}[{\cite[Def.\ 4.4.1]{Go01}}]\label{def:commitment-scheme}\index{commitment scheme}
  A \emph{bit-commitment scheme} is a pair of interactive probabilistic
  polynomial-time machines $(S, R)$, such that
  \begin{enumerate}
    \item Both machines recieve an integer $n$ in unary,
    \item $S$ recieves a single bit $v$,
    \item For any PPT machine $R'$, the output of $\ang{S(0), R'}(1^{n})$ and
          $\ang{S(1), R'}(1^{n})$ are computationally indistinguishable over all
          inputs $n$, and
    \item For almost all coin tosses of $R$, there exists no sequence of
          messages from $S$ such that the view of $R$ is ambiguous.
  \end{enumerate}
\end{defn}

\TODO{More explanation}

\FIXME{The existence of bit-commitment schemes implies the existence of one-way
  functions (Go01 Exercise 13), but CFGS22 talks about \emph{algebraic}
  commitment schemes without reference to one-way functions (nor do they mention
  $\P \ne \NP$)}

The simplest examples of bit-commitment schemes involve one-way functions. A
one-way function is a function that is computable in polynomial time, but whose
inverse is \emph{not} computable in polynomial time. These functions are not
known to exist: in particular their existence implies $\P \ne \NP$, but they are
still widely believed to exist.

\begin{algorithm}[htbp]
  \tcc{Commit}
  \TODO{}\;
  \tcc{Reveal}
  \TODO{}\;
  \caption{A bit-commitment scheme based on a one-way function
    $f$}\label{alg:bit-commit}
\end{algorithm}

\TODO{Define decommitment}

\section{Probabilistically-checkable proofs}\label{sec:pcp}

So far, all of our computational proofs have focused on the interaction between
two computers, but there exist non-interactive models as well.
Probabilistically-checkable proofs do not use interactive Turing machines, but
instead have access to a ``proof'' that their input is in the given language.
The nontriviality is that the number of bits of the proof we can access is
bounded---simply reading the entire proof will not suffice. For any string in the
language, we ensure there exists a correct proof, which our algorithm must
always recognize accurately. Further, for any string \emph{not} in the language,
the algorithm must reliably (but not necessarily always) reject.

In general, query-based machines (i.e.\ those where the \emph{number} of queries
is an important part of the complexity) come in two flavors: adaptive and
non-adaptive. Adaptive-query machines allow the machine to change what locations
it queries based on what it has already seen; non-adaptive machines do not allow
that. In general, adaptive queries are more powerful---it turns out that any
adaptive machine can be simulated by a non-adaptive machine using $2^{q}$
queries. Most interesting results about PCPs can be proven even with weaker
non-adaptive machines, so that is what we will focus on for the rest of this
paper.

\begin{defn}\label{def:adaptive}\index{adaptive}
  Let $M$ be a Turing machine with query access to some string
  $\pi \in \{0, 1\}^{*}$. The queries by $M$ are \emph{non-adaptive} if the
  locations queried depend only on the contents of the input tape and random
  generator. If the queries are dependent on previous query results, then $M$ is
  \emph{adaptive}.
\end{defn}

\TODO{}

\begin{defn}[{\cite[Def.\ 18.1]{AB09}}]\label{def:prob-check}\index{probabilistically-checkable proof}
  Let $L \subseteq \{0, 1\}^{n}$ be a language and $q, r\colon \mathbb{N} \rightarrow \mathbb{N}$. A
  \emph{$(r(n), q(n))$-verifier} for $L$ is a polynomial-time probabilistic
  algorithm $V$ such that
  \begin{enumerate}
    \item When given an input string $x \in \{0, 1\}^{n}$ and random access to a
          string $\pi \in \{0, 1\}^{*}$, $V$ uses at most $r(n)$ random coins and
          makes at most $q(n)$ non-adaptive queries to locations of $\pi$ before
          either accepting or rejecting.
    \item If $x \in L$ then there exists a $\pi_{x} \in \{0, 1\}^{*}$ such that $V$
          will always accept when given input $x$ and random string $\pi_{x}$.
    \item If $x \notin L$ then $V$ will reject with probability $\ge 1/2$ for
          \emph{all} random strings $\pi$.
  \end{enumerate}
  We call the random string $\pi$ the \emph{proof}. We denote the output of $V$ on
  input $x$ and proof $\pi$ with $V^{\pi}(x)$.
\end{defn}

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Stealth]
    \draw (0,0) rectangle ++(3,2);
    \foreach [count=\i] \c in {?,0,?,?,?,1,?,?,1,?,}{
      \pgfmathsetmacro{\xval}{\i*0.5};
      \draw (\xval,3) rectangle ++(0.5,0.5) node [midway] {\c};
    }
    % cdots doesn't appear to be centered in the box?
    \path (5.5,3) rectangle ++(0.5,0.5) node [midway] {$\cdots$};
    \foreach \x in {2, 6, 9}{
      \pgfmathsetmacro{\xval}{\x/2+0.25};
      \draw[->] (1.5,2) to (\xval,3);
    }
  \end{tikzpicture}
  \caption{A probabilistically-checkable proof verifier}\label{fig:pcp}
\end{figure}

The first piece of this definition to notice is that if $x \in L$ we require $L$
to accept unconditionally for $x$'s corresponding proof, despite the fact that
$L$ has access to randomness. We want this because we want to construct
verifiers that look for inconsistencies in the purported proof and reject if
they find them---a correct proof should not contain any inconsistency at all so we
should accept every time.

On the other hand, while an incorrect proof should still be noticeable
\emph{most} of the time (otherwise this would not be a particularly useful
definition), if we are only querying a limited number of bits from a proof it is
impossible to prevent scenarios where we only query bits that are identical to
the original proof. In this case, it would be impossible for any algorithm to
distinguish correct proofs from incorrect ones (outside of ignoring the proof
completely and just solving the problem itself) and as such we do not require
$V$ to always correctly reject.

So far, all of our proof-complexity classes have just had a single class for all
languages with the proof regardless of internal complexity, but for
probabilistically-checkable proofs we actually stratify the class further. This
is for multiple reasons: first, we can actually get astonishingly tight bounds
on the parameters for PCPs (as we will see in \cref{thm:pcp-theorem}), and
second, because these are ``access'' complexity (i.e.\ we measure the number of
preexisting bits actually read by the algorithm), they are actually independent
of computational model, so the need for polynomial equivalence is negated.

In addition, this means PCPs become susceptible to the alphabet the proof is
written in. Since individual bits carry more data when a larger alphabet is
used, a larger alphabet will necessarily require fewer queries to transmit the
same information. As such, while we will by default still work over the alphabet
$\{0, 1\}$, there are times where we will need to be a little more specific
about the alphabet we use.

\begin{defn}[{\cite[Def.\ 18.1]{AB09}}]\label{def:pcp}\index{PCP@$\PCP$}
  For any $q, r\colon \mathbb{N} \rightarrow \mathbb{N}$, the class $\PCP_{\Sigma}(q(n), r(n))$ is the class of all
  languages with a $(cq(n), dr(n))$-verifier for some $c, d \in \mathbb{N}$, where the
  proof is written over the alphabet $\Sigma$. When $\Sigma = \{0, 1\}$, we sometimes omit
  it.
\end{defn}

A small note on notation: The text ``PCP'' can be used as both a complexity
class and an abbreviation: in this paper when it is written $\PCP$, we are
referring to the complexity class (and therefore the set of \emph{languages}
with a probabilistically-checkable proof); when it is written PCP, we are
referring to the proofs themselves.

Next, we give an exapmle of a nontrivial probabilistically-checkable proof. As
with our previous examples, the language $\GNI$ provides a good example for us,
as it is a relatively simple language that is still not known to be in $\BPP$.

\begin{thm}\label{thm:gni-pcp}
  $\GNI \in \PCP(\poly(n), 1)$.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{Two $n$-vertex graphs $G_{1}$ and $G_{2}$}
  \KwOut{Whether $G_{1} \ncong G_{2}$}
  \tcc{Proof:}
  \For{$H$ a graph with $n$ nodes}{
    \eIf{$H \cong G_{0}$}{
      $\pi[H] \leftarrow 0$\;
    }{
      $\pi[H] \leftarrow 1$\;
    }
  }
  \KwRet{$\pi$}\;
  \tcc{Verifier:}
  Pick random $b \in \{0, 1\}$\;
  Pick random $\sigma \in S_{n}$\;
  Apply $\sigma$ to the vertices of $G_{b}$\;
  Accept if and only if $\pi[\sigma \cdot G_{b}] = b$\;\nllabel{line:gni-accept}
  \caption{A PCP for $\GNI$}\label{alg:gni-pcp}
\end{algorithm}

\begin{proof} % FIXME: Should I describe any background about isomorphic graphs?
  We describe such a PCP in \cref{alg:gni-pcp}. Next, we show that this
  algorithm has the properties we seek.

  The verifier runs in polynomial time: both picking and computing an $n$-bit
  permutation are known to be in polynomial time with respect to $n$. Further,
  picking a random $n$-bit permutation is doable in $\poly(n)$ bits, and picking
  a random bit is doable in $1$ bit; hence $V$ uses $\poly(n)$ bits of
  randomness. Lastly, we only make a single query to $\pi$, in
  line~\ref{line:gni-accept}.

  If $G_{0} \ncong G_{1}$, we show $V$ always accepts when given $\pi$ as input. Since
  isomorphisms are transitive, we know there is no $H$ with both $H \cong G_{0}$ and
  $H \cong G_{1}$. Hence, for all $H$, if $H \cong G_{0}$ then $\pi[H] = 0$ and if
  $H \cong G_{1}$ then $\pi[H] = 1$. Hence, since $\sigma \cdot G_{b} \cong G_{b}$ regardless of
  our choice of $\sigma$ and $b$, we have that $\pi[\sigma \cdot G_{b}] = b$.

  Next, if $G_{0} \cong G_{1}$ then $V$ rejects with probability at least $1/2$,
  regardless of choice of $\pi$. Since $G_{0} \cong G_{1}$, it follows that if
  $H \cong G_{0}$ then $H \cong G_{1}$, and vice versa. Hence, for any graph
  $H = \sigma \cdot G_{0}$, there exists a $\sigma' \in S_{n}$ with $H = \sigma' \cdot G_{1}$. For any
  $n$-vertex graph $H$, this means that we are equally likely to query $\pi[H]$
  with $b = 0$ and $\sigma$ as we are with $b = 1$ and $\sigma'$. Since we know $\pi[H]$ can
  only be $0$ or $1$, it must be the case that $\pi[H]$ is incorrect at least half
  the time. Hence, $V$ will reject with probability at least $1/2$ for any proof
  $\pi$.
\end{proof}

It is reasonable to be surprised about the fact that we only need one query to
determine this problem to the constraints imposed by a PCP\@. This is our first
clue that PCPs are surprisingly powerful: in \cref{chap:pcp-theorem} and again
in \cref{chap:zk-pcp-theorem} we will explore the extremes of the power of PCPs.

\subsection{PCPs of proximity}\label{sec:pcpp}

Since a PCP will always only check a limited proportion of any given proof, for
any strings in the language, the verifier will still accept any proof that is
close to the official proof with very high probability. While this is
interesting in and of itself, it can also lead to the question of how PCPs
perform on \emph{values} that are close to strings in our language. This is the
notion behind PCPs of proximity: what if we weaken a PCP to only require it to
reject strings that are not sufficiently close to strings in the given language?

To define PCPs of proximity, we first need a working definition of proximity.
Computer scientists have many definitions of string closeness, but for this
particular problem we will be working with a mathematically simple one
originally used for error-correcting codes.

\begin{defn}[{\cite{Ham50}}]\label{def:hamming-dist}\index{Hamming distance}
  Let $x, y \in \Sigma^{n}$ be strings of the same length. We say the \emph{Hamming
    distance} between $x$ and $y$ is the value
  \[
    \Delta(x, y) = \frac{\abs{\{i \in [n] \mid x_{i} \ne y_{i}\}}}{n}.
  \]
\end{defn}

Note that $\Delta(x, y) \in [0, 1]$ for any strings $x$ and $y$. This normalization is
not strictly necessary in general, since $\Delta$ is only defined between strings of
equal length (and there do exist cases where it is much nicer to keep the
distance as a natural number). In our case, however, we would like to keep the
distances all in the same bounded range; hence we normalize.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \foreach [count=\i] \c in
        {1,\textcolor{pumpkin}{0},0,0,\textcolor{pumpkin}{1},\textcolor{pumpkin}{0},0,1} {
      \pgfmathsetmacro{\xval}{\i*0.5};
      \draw (\xval,0.5) rectangle ++(0.5,0.5) node [midway] {\c};
    }
    \foreach [count=\i] \c in
        {1,\textcolor{pumpkin}{1},0,0,\textcolor{pumpkin}{0},\textcolor{pumpkin}{1},0,1} {
      \pgfmathsetmacro{\xval}{\i*0.5};
      \draw (\xval,0) rectangle ++(0.5,0.5) node [midway] {\c};
    }
  \end{tikzpicture}
  \caption{Two strings with Hamming distance $3/8$}\label{fig:hamming-dist}
\end{figure}

The definition of Hamming distance between two strings also generalizes to the
notion of distance from a set. Informally, we say the distance from a string to
a set is simply the distance to the closest element of the set.

\begin{defn}\label{def:far}\index{far}
  Let $\varepsilon > 0$. A vector $x \in \Sigma^{n}$ is \emph{$\varepsilon$-far} from a set $S \subseteq \Sigma^{n}$ if
  \[
    \min_{y \in S}(\Delta(x, y)) \ge \varepsilon.
  \]
\end{defn}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[>=Stealth]
    \coordinate (Z) at (0,0) {};
    \coordinate (X) at (1,-0.25);
    \draw[use Hobby shortcut,closed=true]
    (0,0) .. (0,1.5) .. (1,1) .. (1.5,2) .. (-.5,3) .. (-1,1.5) .. (-1,.5);
    \draw[->] (X) -- (Z) node[midway,below] {$\varepsilon$};
    \fill (X) circle (1pt) node[right] {$x$};
    \node (S) at (0.25, 2.25) {$S$};
    \node [draw,dashed] at (1,-0.25) [circle through=(Z)] {};
  \end{tikzpicture}
  \caption{A point $x$ that is $\varepsilon$-far from a set $S$}\label{fig:epsilon-far}
\end{figure}

Now that we know what proximity means, we can define a PCP of proximity. Unlike
a PCP, which is defined for any language $L$, a PCPP is only defined for
\emph{pair languages}\index{pair language}, languages that consist entirely of
ordered pairs of two objects. This is because we need our language to consist of
pairs so that we can have a notion of distance between the elements. The good
news is this will not affect us too much---lots of the languages we care about are
pair languages already.

The main difference between a PCP and a PCPP is that we relax the rejection
condition to only require consistent rejection for pairs $(x, y)$ where there is
no close $y'$ where $(x, y')$ is in the accepted language. There are many pair
languages that we can think of as being made of function-value pairs $(f, y)$
with the property that there is some $x$ such that $f(x) = y$. When we have a
language like this, we have a more intuitive explanation of PCPPs: here, a PCPP
will accept any $(f, y)$ pair where $f(x)$ is $\delta$-close to $y$.

\begin{defn}[{\cite[Def.\ 2.2]{GOS25}}]\label{def:pcpp}\index{PCPP@$\PCPP$}
  For $\delta\colon \mathbb{N} \rightarrow [0, 1]$, a \emph{probabilistically-checkable proof of
    proximity} for a language $L$ consisting of ordered pairs $(x, y)$ with
  proximity parameter $\delta$ consists of a prover $P$ and verifier $V$ such that
  the following holds for all $(x, y)$:
  \begin{enumerate}
    \item When given an input string $x \in \{0, 1\}^{n}$ and random access to a
          string $\pi \in \{0, 1\}^{*}$, $V$ uses at most $r(n)$ random coins and
          makes at most $q(n)$ non-adaptive queries to locations of $\pi$ before
          either accepting or rejecting.
    \item If $(x, y) \in L$, $V$ will always accept when given input $(x, y)$ and
          proof $\pi_{x}$.
    \item If $y$ is $\delta$-far from the set $L(x) = \{y \mid (x, y) \in L\}$, then for
          every oracle $\pi^{*}$, $V$ will reject on input $(x, y)$ with
          probability $\ge 1/2$.
  \end{enumerate}
\end{defn}

\TODO{Introduce example}

% TODO: Is this the best example? These are *extremely* weak bounds and don't
% actually have any probability involved (will always accept when valid and will
% always reject when invalid)
\begin{thm}
  $\GI \in \PCP[\poly(n), \poly(n)]$.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{Two graphs, $G_{1}$ and $G_{2}$}
  \KwOut{Whether $G_{1} \cong G_{2}$}
  \tcc{Proof}
  Let $\sigma$ be an isomorphism from $G_{1}$ to $G_{2}$\;
  \KwRet{$(\sigma, \sigma, \sigma)$}\;
  \tcc{Verifier}
  $\sigma' \leftarrow [\,]$\;
  \For{$i \in [\abs{\pi}/3]$}{
    Pick random $r \in \{0, 1, 2\}$\;
    Add $\pi[3r + i]$ to $\sigma'$\;
  }
  \eIf{$\sigma'$ is a representation of an isomorphism $G_{1} \rightarrow G_{2}$}{
    accept\;
  }{
    reject\;
  }
  \caption{A PCP for $\GI$}\label{alg:gi-pcp}
\end{algorithm}

\begin{proof}
  We demonstrate a PCP for $\GI$ with the specified parameters in
  \cref{alg:gi-pcp}. We need to show a few things: that the proof and verifier
  follow the specified parameters, that the verifier always accepts when given a
  correct proof for some $(G_{1}, G_{2}) \in \GI$, and that the verifier reliably
  rejects when given a $(G_{1}, G_{2}) \notin \GI$.

  First, an $n$-bit permutation can be described in $\poly(n)$ bits; for each
  bit in the permutation we write 3 bits to the proof, hence
  $\abs{P} \in \poly(n)$. Since $\abs{P} \in \poly(n)$, it follows that we can query
  at most $\poly(n)$ distinct bits. The verifier generates one random bit for
  every 3 bits in $P$, so it reads $\poly(n)$ bits of randomness. Finally, all
  the verifier does is reads $\poly(n)$ bits and then checks that it represents
  a permutation; this is all doable in polynomial time and thus the verifier is
  polynomial.

  Next, we show $V$ always accepts if $(G_{1}, G_{2}) \in \GI$ and
  $\pi = \pi_{(G_{1}, G_{2})}$. The way we have designed our proof is a simple form
  of error-correcting code: we have just repeated the isomorphism three times
  and when reconstructing the isomorphism from the proof, we pick each bit from
  one of the three copies at random. In the event of an honest proof, all of
  these copies are equal to the original, so no matter what our choices of $r$,
  our reconstructed $\sigma'$ will always be equal to $\sigma$ (an isomorphism
  $G_{1} \rightarrow G_{2}$ by definition) and thus $V$ will accept.

  Finally, we show $V$ reliably rejects if $G_{1} \ncong G_{2}$ regardless of what
  proof $\pi$ is given to $V$. In this case, $\sigma'$ will never be an isomorphism
  from $G_{1}$ to $G_{2}$ (since no such isomorphism exists); hence $V$ will
  always reject.
\end{proof}

\section{Zero-knowledge probabilistically-checkable proofs}\label{sec:pzkpcp}

A zero-knowledge probabilistically-checkable proof is a combination of the ideas
of zero-knowledge proofs (as seen in \cref{sec:zero-knowledge}) and
probabilistically-checkable proofs (as seen in \cref{sec:pcp}). Since we can
model a PCP as an interaction between between a verifier and a proof (instead of
a prover), we can model this interaction as being zero-knowledge as well.

Unlike interactive proofs, we cannot achieve \emph{arbitrary} zero-knowledge
guarantees: since the proof is non-interactive, we have no recourse against an
attacker who simply reads the entire proof end-to-end. As such, we introduce a
\emph{query bound}\index{query bound}: we limit our verifier to a certain number
of queries, under which we retain the standard zero-knowledge restrictions.

\begin{defn}[{\cite[Def.\ 8.6]{GOS24}}]\label{def:pzkpcp}\index{perfect-zero knowledge PCP}
  A probabilistically-checkable proof system is \emph{zero-knowledge} with query
  bound $q$ if for any verifier $V'$ such that $V'$ makes no more than $O(q(n))$
  adaptive queries on an input of length $n$, there exists a probabilistic
  polynomial-time simulator $S$ such that on input $x$, $S$ can simulate every
  interaction of $V'$ with the associated proof of $x$.\footnote{To clarify, $S$
    does \emph{not} have access to the proof of $x$, just $x$ itself.}
\end{defn}

The first thing to notice here is that for \emph{validity} of PCPs, we
parameterize over the proof (i.e.\ the one verifier should remain valid for all
proofs $\pi$), for \emph{zero-knowledge} we parameterize over the verifier $V$.
This is because for zero-knowledge proofs we care about not revealing any
information from the proof, no matter how clever we are about asking questions
with the verifier. In that way, outside of the ``happy path'' (where $x \in L$ and
the given string is the proof of $x$), the two notions are somewhat orthogonal:
a $\PCP$ cares about how we react when $x \notin L$ but $V$ is trusted, while
zero-knowledge cares about what happens when $V$ is not trusted, but the proof
is.

\begin{defn}\label{def:pzkpcp-class}\index{PZK-PCP@$\PZKPCP$}
  The class $\PZKPCP$ is the class of all languages that have a perfect
  zero-knowledge probabilistically-checkable proof.
\end{defn}

\TODO{Example (preferably $\GNI$)}

\begin{thm}\label{thm:gni-pzkpcp}
  There is a perfect zero-knowledge PCP for $\GNI$.
\end{thm}

\begin{algorithm}[htbp]
  \TODO{}
  \caption{A PZK-PCP for $\GNI$}\label{alg:gni-pzkpcp}
\end{algorithm}

\begin{proof}
  \TODO{}
\end{proof}

\section{Interactive probabilistically-checkable proofs}\label{sec:ipcp}

Interactive probabilistically-checkable proofs are a combination of the concepts
of an interactive protocol and a probabilistically-checkable proof. The broad
idea is our proof proceeds in two phases: first, the prover sends a purported
proof to the verifier, after which they engage in an interactive protocol,
during which the verifier can access the proof as an oracle.

\begin{defn}[{\cite[\defaultS 1.1]{KR08}}]\label{def:ipcp}\index{interactive PCP}
  Let $L$ be a language, let $p, q, l\colon \mathbb{N} \rightarrow \mathbb{N}$, and let $c, s \in [0, 1]$. An
  \emph{interactive probabilistically-checkable proof} for $L$ is an interactive
  protocol as follows:

  \begin{algorithm}[H]
    \KwIn{To both $P$ and $V$: a string $x$ of length $n$}
    \KwIn{To $P$ alone: A string $w$}
    \KwOut{Whether $x \in L$}
    $P$: Send an oracle $R$ to $V$\;
    $V^{R}$: Engage in an interactive protocol with $P$\;
    \caption{The IPCP protocol}\label{alg:ipcp-protocol}
  \end{algorithm}
\end{defn}

Next, we need to define a few properties of IPCPs. In general we will care about
IPCPs with specific bounds on these properties; later on we will spend some time
(in particular \cref{sec:reduce-queries}) working on optimizing bounds on some
of these properties at the expense of others.

The first two complexities we will look at come from the interactive-proof
portion of the IPCP\@. As a reminder, two important parameters we care about
with regard to interactive proofs are the number of communication rounds (i.e.,
the number of times the switch tape flips) and the total amount of information
sent between the two machines. Since these are supposed to relate to just the IP
portion of the protocol, we need to modify the definitions slightly to exclude
the information exchanged during the PCP phase.

\begin{defn}\label{def:ipcp-round-complexity}\index{round complexity!IPCP}
  The \emph{round complexity} of an IPCP is the number of rounds in the second
  portion of the protocol.
\end{defn}

\begin{defn}\label{def:ipcp-comm-complexity}\index{communication complexity!IPCP}
  The \emph{communication complexity} of an IPCP is the total number of bits
  exchanged between $P$ and $V$ \emph{except} for the message that contains $R$.
\end{defn}

The third complexity we care about comes from the PCP portion of the IPCP\@.
\TODO{}

\begin{defn}
  The \emph{query complexity} of an IPCP is the total number of queries that $V$
  makes to the PCP oracle $R$.
\end{defn}

\begin{defn}\label{def:ipcp-class}\index{IPCP@$\IPCP$}
  The class $\IPCP$ is the class of all languages with an interactive $\PCP$.
\end{defn}

Next, we give a few reasonable class inclusions regarding $\IPCP$.

\begin{thm}
  $\PCP \subseteq \IPCP$ and $\IP \subseteq \IPCP$.
\end{thm}

\begin{proof}
  Both of these come from the definition of an interactive PCP: a regular PCP is
  simply the first half of the IPCP protocol where the interactive portion is
  useless, and an interactive proof is simply the second half of the protocol
  where the oracle is useless.
\end{proof}

The tuple-notation we used when talking about the class $\PCP$ (see
\cref{def:pcp}) is rather hard to read when we have this many parameters, and as
such we will us the following clearer notation when talking about the various
bounds on $\IPCP$ algorithms:
\begin{equation*}
  L \in \IPCP\ipcp{r}{\ell}{c}{q}{\varepsilon}
\end{equation*}
to mean the language $L$ is a member of $\IPCP$ with the listed restrictions.

\TODO{Does it make sense to talk about BFL90's IPCP for all of NEXP?}

\begin{defn}\label{def:low-deg-ipcp}\index{low-degree IPCP@low-degree $\IPCP$}
  Let $\mathbb{F}$ be a field, and $d, m \in \mathbb{N}$. A \emph{low-degree IPCP} is an
  IPCP instance with the following two properties:
  \begin{enumerate}
    \item The oracle sent by the honest prover $P$ is an $m$-variable
          $\mathbb{F}$-polynomial $Q$ with multidegree no more than $d$ (i.e.\
          $Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$)
    \item Soundness is only required to hold against provers that send oracles
          that are polynomials in $\mathbb{F}[X_{1, \ldots, m}^{\le d}]$.
  \end{enumerate}
\end{defn}

Similarly to what we did with normal $\IPCP$ oracles, we will use the following
notation to talk about low-degree $\IPCP$ instances:
\begin{equation*}
  L \in \IPCP\ldipcp{r}{\ell}{c}{q}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{\varepsilon}.
\end{equation*}
We combine all the information about the degree of the oracle into one line
because we have an efficient notation for multidegree-bounded polynomials, and
so that we do not wind up with an exorbitant number of lines in our
notation.\footnote{Having said that, there are still a lot of lines in this
  notation, but this is the best we can do.}

\TODO{Examples}

\begin{defn}\label{def:ipcp-view}\index{view!IPCP}
  The \emph{view} of an IPCP $(P, V)$ on input $x$ is the random variable
  \[
    (x, r, s_{1}, \ldots, s_{n}, t_{1}, \ldots, t_{m})
  \]
  where $r$ is the random bits used by $V$, $s_{i}$ are the messages from $P$,
  and $t_{i}$ are the answers to $V$'s queries to the oracle sent by $P$.
\end{defn}

We denote the view of $P$ and $V$ on input $x$ by $\View_{V}^{P}(x)$.

\section{Zero-knowledge IPCPs}

At this point, the astute reader may have noticed a trend: after each new
interactive-proof variant we introduce, we then describe how to make it
zero-knowledge. We will continue this trend by showing how an interactive PCP
can be made zero-knowledge.

\begin{defn}[{\cite[\defaultS 5.2]{CFGS22}}]\label{def:zk-ipcp}\index{interactive PCP!zero-knowledge}
  An interactive PCP is \emph{perfect zero-knowledge} with query bound $b$ when
  there exists a polynomial-time simulator algorithm $S$ such that for every
  interactive Turing machine $\tilde{V}$ that makes no more than $b$ queries,
  $S^{\tilde{V}}(x)$ and $\View\ang{P(x), \tilde{V}(x)}$ are identically
  distributed.
\end{defn}

\begin{defn}\label{def:pzkipcp}\index{PZK-IPCP@$\PZKIPCP$}
  The class $\PZKIPCP$ is the class of all languages with a perfect
  zero-knowledge IPCP\@.
\end{defn}

\TODO{Example probably}

As with our other notations, we will write
\[
  L \in \PZKIPCP\pzkipcp{r}{\ell}{c}{q}{b}{\varepsilon}
\]
to show $L$ has a perfect zero-knowledge IPCP with the listed restrictions.

\chapter{The $\PCP$ theorem}\label{chap:pcp-theorem}

\begin{thm}[{$\PCP$ theorem,~\cite{AS98}}]\label{thm:pcp-theorem}\index{PCP theorem@$\PCP$ theorem}
  Any problem in $\NP$ has a probabilistically-checkable proof of constant query
  complexity and using a maximum of $O(\log n)$ random bits, and vice versa.
  Equivalently, $\NP = \PCP(\log n, 1)$.
\end{thm}

\TODO{}
% NOTE: Somewhere I need a section on low-degree tests

\section{Algebraic circuits}\label{sec:alg-circuit}

\TODO{Mention what the size of an algebraic circuit is}

\begin{defn}[{\cite[\defaultS 14.1]{AB09}}]\label{def:alg-circuit}\index{algebraic circuit}
  An \emph{algebraic circuit} is a directed acyclic graph such that
  \begin{enumerate}
    \item each leaf (called an \emph{input node}) takes values in some field
          $F$,
    \item each internal node (called a \emph{gate}) is labeled with either $+$
          or $\cdot$ (the two field operations),
    \item there is one output node, and
    \item each gate has in-degree no more than $2$.
  \end{enumerate}
  Optionally, there may be input nodes labeled $1$ and $-1$ as well.
\end{defn}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}[>=stealth,every node/.style={minimum size=0.75cm},scale=2]
      \node [diamond,draw] (R) at  (0,0) {$+$};
      \node [rectangle,draw] (A1) at  (-0.5,1) {$\times$};
      \node [diamond,draw] (A2) at  (0.5,1) {$+$};
      \node [diamond,draw] (B1) at  (-1,2) {$+$};
      \node [rectangle,draw] (B2) at  (0,2) {$\times$};
      \node [diamond,draw] (B3) at  (1,2) {$+$};
      \node [rectangle,draw] (C1) at  (-0.5,3) {$\times$};
      \node [diamond,draw] (C2) at  (0.5,3) {$+$};
      \node [circle,draw] (X1) at  (-1.5,4) {$x_{1}$};
      \node [circle,draw] (X2) at  (-0.5,4) {$x_{2}$};
      \node [circle,draw] (X3) at  (0.5,4) {$x_{3}$};
      \node [circle,draw] (X4) at  (1.5,4) {$x_{4}$};

      \draw[->] (X1) to (B1);
      \draw[->] (X2) to (C1);
      \draw[->] (X2) to (C2);
      \draw[->] (X3) to (C1);
      \draw[->] (X3) to [bend left=15] (B3);
      \draw[->] (X3) to (C2);
      \draw[->] (X4) to (B3);
      \draw[->] (C1) to (B2);
      \draw[->] (C2) to [bend left] (A1);
      \draw[->] (C2) to (B1);
      \draw[->] (C2) to (B2);
      \draw[->] (B3) to (A2);
      \draw[->] (B1) to (A1);
      \draw[->] (B2) to (A2);
      \draw[->] (A1) to (R);
      \draw[->] (A2) to (R);
    \end{tikzpicture}
  \end{center}
  \caption{An algebraic circuit}\label{fig:algebraic-circuit}
\end{figure}

We give an example of an algebraic circuit in \cref{fig:algebraic-circuit}.
Notice in particular that unlike the in-degree, the out-degree is unbounded (the
rightmost node in the second row has out-degree three, for example).

\TODO{More}

\begin{defn}\label{def:alg-assignment}\index{assignment!algebraic circuit}
  An \emph{assignment} to an algebraic circuit is a labeling of its input nodes.
  The \emph{result} of the assignment is the value in the output node, where the
  value of any $+$ gate is $a+b$ and any $\cdot$ gate is $a \times b$, where $a$ and $b$
  are the values of the two in-nodes.
\end{defn}

\TODO{}

\begin{defn}\label{def:alg-circuit-sat}\index{satisfying assignment!algebraic circuit}
  An algebraic circuit has a \emph{satisfying assignment} when there exists a
  labeling of its input nodes such that the value computed by the output node is
  $1$.
\end{defn}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}[>=stealth,every node/.style={minimum size=0.75cm},scale=2]
      \node [diamond,draw] (R) at (0,0) {$1$};
      \node [rectangle,draw] (A1) at (-0.5,1) {$5$};
      \node [diamond,draw] (A2) at (0.5,1) {$4$};
      \node [diamond,draw] (B1) at (-1,2) {$3$};
      \node [rectangle,draw] (B2) at (0,2) {$6$};
      \node [diamond,draw] (B3) at (1,2) {$4$};
      \node [rectangle,draw] (C1) at (-0.5,3) {$2$};
      \node [diamond,draw] (C2) at (0.5,3) {$3$};
      \node [circle,draw]  (X1) at (-1.5,4) {$3$};
      \node [circle,draw]  (X2) at (-0.5,4) {$2$};
      \node [circle,draw]  (X3) at (0.5,4) {$1$};
      \node [circle,draw]  (X4) at (1.5,4) {$3$};

      \draw[->] (X1) to (B1);
      \draw[->] (X2) to (C1);
      \draw[->] (X2) to (C2);
      \draw[->] (X3) to (C1);
      \draw[->] (X3) to [bend left=15] (B3);
      \draw[->] (X3) to (C2);
      \draw[->] (X4) to (B3);
      \draw[->] (C1) to (B2);
      \draw[->] (C2) to [bend left] (A1);
      \draw[->] (C2) to (B1);
      \draw[->] (C2) to (B2);
      \draw[->] (B3) to (A2);
      \draw[->] (B1) to (A1);
      \draw[->] (B2) to (A2);
      \draw[->] (A1) to (R);
      \draw[->] (A2) to (R);
    \end{tikzpicture}
  \end{center}
  \caption{A satisfying assignment to \cref{fig:algebraic-circuit} over $\mathbb{Z}/7\mathbb{Z}$}\label{fig:sat-assignment}
\end{figure}

We give a satisfying assignment to \cref{fig:algebraic-circuit} as an example in
\cref{fig:sat-assignment}. Notice that we have kept the shapes of the nodes from
the original in order to communicate the underlying operation.

Satisfying assignments will form the root of our circuit classes: the problem we
care about computing is whether or not a circuit has a satisfying assignment.
We find this a useful formulation because it works well with arbitrary algebraic
formulas: since we will be working in a primarily algebraic setting for these
proofs, having an example of a $\NP$-complete problem that is itself algebraic
will make for relatively easy transformations.

\TODO{More}

\begin{defn}\label{def:cktsat}\index{CktSAT@$\CktSAT$}
  The language $\CktSAT$ is the language of all algebraic circuits with a
  satisfying assignment.
\end{defn}

\begin{thm}\label{thm:cktsat-np-complete}
  $\CktSAT$ is $\NP$-complete.
\end{thm}

\begin{proof}
  First, we need that $\CktSAT \in \NP$. Evaluating a circuit can be done in
  polynomial time relative to its size, so a certificate that is simply the
  values of each input node in the satisfying assignment will suffice.

  Next, we need that there is a reduction from every problem in $\NP$ to
  $\CktSAT$. We will use the fact that we already know normal $\SAT$ is
  $\NP$-complete for this. First, consider the \emph{boolean circuit problem}:
  similar to algebraic circuits but where the operators are boolean formulae
  instead of the field operators. We can construct a boolean circuit from a
  boolean formula by replacing every variable with an input node and every
  boolean operator with a gate corresponding to its formula.

  From there, we can transform a boolean circuit into an algebraic circuit over
  the field $\mathbb{Z}/2\mathbb{Z}$. Multiplication in $\mathbb{Z}/2\mathbb{Z}$ is exactly an AND gate, so that is
  simply a one-to-one replacement. OR gates are slightly trickier, however. We
  would like the replacement to be as simple as multiplication: mapping $x \vee y$
  to $x + y$. However, $T \vee T = T$ but $1 + 1 = 0$ in $\mathbb{Z}/2\mathbb{Z}$. Instead, we map
  $x \vee y$ to $(x + y) + (x \times y)$, which gives the answer we seek. Lastly, we map
  NOT gates to $x + 1$.

  We have shown $\CktSAT \in \NP$ and we have shown a polynomial-time reduction
  from $\SAT$ to $\CktSAT$; hence $\CktSAT$ is $\NP$-complete.
\end{proof}

\begin{defn}\label{def:cktval}\index{CktVal@$\CktVal$}
  The language $\CktVal$ is the language of all pairs $(C, w)$, where $C$ is an
  algebraic circuit and $w$ is a satisfying assignment (i.e., $C(w) = 1$).
\end{defn}

\begin{thm}\label{thm:cktval-in-p}
  $\CktVal$ is in $\P$.
\end{thm}

\begin{proof}
  We are given a circuit and an assignment; computing the resultant value of a
  circuit is possible in polynomial time relative to its length. Hence, we can
  simply compute $C(w)$ directly and check if the answer is $1$.
\end{proof}

\begin{thm}[{\cite[Prop.\ 2.4]{BGHSV06}}]\label{thm:cktval-cktsat}
  If $\CktVal$ has a PCPP, then $\CktSAT$ has a PCP with identical parameters.
\end{thm}

\begin{proof}
  % TODO
\end{proof}

% FIXME: Is this the right theorem?
\begin{thm}[{\cite[Theorem 3.2]{BGHSV06}}]\label{thm:ckt-val-pcpp}
  For any $\varepsilon > 0$, $\CktVal$ has a PCPP $(P, V)$ where $P$ is deterministic and
  polynomial-time, such that
  \[
    \CktVal \in \PCPP\pcpp{\log(n) + O(\log^{\varepsilon}(n))}{O(1/\varepsilon)}{\Theta(\varepsilon)}{1/2}.
  \]
\end{thm}

\begin{proof}
  % TODO: Do I even \emph{want} a proof or is this too much?
\end{proof}

\section{Robust verifiers}\label{sec:robust-verifier}

\begin{defn}[{\cite[Def.\ 2.6]{BGHSV06}}]\label{def:robust-verifer}\index{robust verifier}
  Let $s, \rho\colon \mathbb{N} \rightarrow [0, 1]$. A PCP verifier $V$ has
  \emph{robust-soundness error} $s$ with \emph{robustness parameter} $\rho$ if for
  all $x \notin L$, the bits read by $V$ are $\rho$-close to being accepted with
  probability strictly less than $s$.
\end{defn}

We will denote robust PCP classes with
\[
  \PCP\pcpr{q(n)}{r(n)}{s(n)}{\rho(n)}.
\]

\section{The composition theorem}\label{sec:comp-theorem}

We now introduce a theorem that will allow us to compose robust PCPs with robust
PCPPs to make a new (non-robust) PCP with improved bounds.

% NOTE: I believe this replaces 2.1 (outer/inner verifiers) from ALMSS98
\begin{thm}[{\cite[Theorem 2.7]{BGHSV06}}]\label{thm:composition}
  Let
  \begin{align*}
    r_{\out}, r_{\oin}, d_{\out}, d_{\oin}, q_{\oin}&\colon \mathbb{N} \rightarrow \mathbb{N} \\
    \varepsilon_{\out}, \varepsilon_{\oin}, \rho_{\out}, \delta_{\oin}&\colon \mathbb{N} \rightarrow [0, 1]
  \end{align*}
  be functions such that the following holds:
  % TODO: Put in IPCP-style bracket notation
  \begin{enumerate}
    % TODO: Do we care about decision complexity? If so, define it
    \item The language $L$ has a robust PCP verifier $V_{\out}$ with randomness
          complexity $r_{\out}$, decision complexity $d_{\out}$,
          robust-soundness error $1 - \varepsilon_{\out}$, and robustness parameter
          $\rho_{\out}$.
          % TODO: Define Circuit-Value problem
    \item $\CktVal$ has a PCPP verifier $V_{\oin}$ with randomness complexity
          $r_{\oin}$, query complexity $q_{\oin}$, decision complexity
          $d_{\oin}$, proximity parameter $\delta_{\oin}$, and soundness error
          $1 - \varepsilon_{\oin}$.
    \item For every $n \in \mathbb{N}$, $\delta_{\oin}(d_{\out}(n)) \le \rho_{\out}(n)$.
  \end{enumerate}
  % TODO: More bracket notation
  Then $L$ has a standard PCP $V_{\comp}$ with
  \begin{enumerate}[label=\alph*.]
    \item randomness complexity $r_{\out}(n) + r_{\oin}(d_{\out}(n))$,
    \item query complexity $q_{\oin}(d_{\out}(n))$,
    \item decision complexity $d_{\oin}(d_{\out}(n))$, and
    \item soundness error $1 - \varepsilon_{\out}(n)\varepsilon_{\oin}(d_{\out}(n))$.
  \end{enumerate}
  % TODO: Still another paragraph here
\end{thm}

\begin{algorithm}[htbp]
  % TODO
  \caption{A composed PCP~\cite[Theorem 2.7]{BGHSV06}}\label{alg:composed-pcp}
\end{algorithm}

\begin{proof}
  We show the validity of the proof and verifier described in
  \cref{alg:composed-pcp}.
  % TODO
\end{proof}

% TODO: Rest of proof

\section{Alphabet reduction}\label{sec:alph-reduction}

\begin{thm}[{\cite[Lemma 2.13]{BGHSV06}}]%
  \label{thm:alph-reduction}\index{alphabet reduction}
  Let $L$ be a language with a PCP over the language $\{0, 1\}^{a}$ such
  that
  \[
    L \in \PCP\pcpr{q}{r}{s}{\rho}.
  \]
  Then $L$ has a PCP over the language $\{0, 1\}$ such that
  \[
    L \in \PCP\pcpr{r}{O(aq)}{s}{\Omega(\rho)}.
  \]
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{A PCP $(P, V)$ over the language $\{0, 1\}^{a}$ for $L$}
  \tcc{Proof}
  Run $P(x)$ to obtain a proof $\pi$\;
  Define the proof oracle $\tau$ by $\tau(\gamma) = \ECC(\pi(\gamma))$\;
  \KwRet{$(\pi, \tau)$}\;
  \tcc{Verifier}
  % TODO
  \caption{A boolean reduction of a PCP~\cite[Construction 3.6]{GOS25}}\label{alg:boolean-reduction}
\end{algorithm}

\begin{proof}
  We show that \cref{alg:boolean-reduction} is a boolean reduction algorithm.
  % TODO
\end{proof}

\section{Robust PCP verifiers for $\CktSAT$}\label{sec:robust-pcp-cktsat}

% TODO: Theorem 2.1.12?

\section{Parallelizable PCPPs}\label{sec:parallel-pcp}

% TODO: Goal: Prove Theorem 2.1.9 from ALMSS98 and then Proposition 2.14 from
% BGHSV06 (removing as much complexity from ALMSS98 as possible)

\begin{thm}[{\cite[Theorem 2.1.9]{ALMSS98}}]\label{thm:always-robust-pcpp}
  % TODO: Rephrase to use PCPP verifiers instead of inner verifiers
\end{thm}

\begin{proof}
  % TODO
\end{proof}

\begin{thm}[{\cite[Prop.\ 2.14]{BGHSV06}}]\label{thm:parallel-vs-robust}
  % TODO
\end{thm}

\begin{proof}
  % TODO
\end{proof}

\section{An upper bound on minimal PCP queries}\label{sec:pcp-query-complexity}

\begin{thm}[{\cite{Has97}}]\label{thm:pcp-max-queries}
  Any language in $\NP$ has a $\PCP$ that queries a maximum of $3$ bits of the
  proof and uses $O(\log n)$ random bits.
\end{thm}

\chapter{Quantum computation}\label{chap:quantum}

\section{Quantum computers}\label{sec:quant-comp}

\begin{defn}\label{def:qubit}\index{qubit}
  A \emph{qubit} is a unit vector in $\mathbb{C}^{2}$.
\end{defn}

\begin{defn}\label{def:operator}\index{operator}
  A \emph{operator} is a linear function $A\colon V \rightarrow V$ such that
  $v \cdot A \cdot v^{T} \ge 0$ for all $v \in V$.
\end{defn}

\subsection{Measurement}

\begin{defn}\label{def:projective-measure}\index{projective measurement}
  A \emph{projective measurement} is
\end{defn}

\begin{defn}\label{def:povm}\index{positive operator-valued measure}
  A \emph{positive operator-valued measure} is
\end{defn}

\section{Quantum complexity classes}

\begin{defn}\label{def:bqp}\index{BQP@$\BQP$}
  The class $\BQP$ is
\end{defn}

\begin{defn}\label{def:qma}\index{QMA@$\QMA$}
  The class $\QMA$ is
\end{defn}

\section{Quantum interactive proofs}\label{sec:quant-interactive}

\begin{defn}\label{def:mip-star}\index{MIP*@$\MIP*$}
  The class $\MIP*$ is
\end{defn}

% TODO: This is actually worth exploring a *lot* more (it's non-relativizing!)
% See https://scottaaronson.blog/?p=4512
\begin{thm}[{\cite{JNVWY21}}]\label{thm:mip-star-is-re}
  $\MIP* = \RE$.
\end{thm}

\section{Quantum low-multidegree test}

\begin{thm}[{\cite{JNVWY20}}]\label{thm:quantum-low-degree}
  % TODO: What is a projective strategy and (F, d, m)-test?
  There is a universal constant $C > 0$ such that the following holds. Let
  $(\tilde{P}_{1}, \tilde{P}_{2}, \ket{\Psi})$ be a projective strategy that passes
  the $(\mathbb{F}, d, m)$-low-multidegree test with probability at least
  $1 - \varepsilon$. For $i \in \{1, 2\}$, $\alpha \in \mathbb{F}^{m}$, denote by
  $\{A_{i,\alpha}^{z}\}_{z \in \mathbb{F}}$ the measurement applied by $\tilde{P}_{i}$
  upon recieving question $\alpha$. Then there exist projective measurements
  $\{L_{1}^{Q}\}_{Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]}$ and
  $\{L_{2}^{Q}\}_{Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]}$ such that for
  $v \in \poly(m, d)(d/\abs{\mathbb{F}} + c)^{C}$, the following holds:
  \begin{enumerate}
    \item Consistency with
          $\{A_{1, \alpha}^{z}\}_{z \in \mathbb{F}, \alpha \in \mathbb{F}^{m}}$ and
          $\{A_{1, \alpha}^{z}\}_{z \in \mathbb{F}, \alpha \in \mathbb{F}^{m}}$:
          \begin{align}
            \mathbb{E}_{\alpha \in \mathbb{F}^{m}}\sum_{Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]}\sum_{z \in \mathbb{F} \setminus \{Q(\alpha)\}}
            \braopket{\Psi}{A_{1,\alpha}^{z} \otimes L_{2}^{Q}}{\Psi} &\le v, \\
            \mathbb{E}_{\alpha \in \mathbb{F}^{m}}\sum_{Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]}\sum_{z \in \mathbb{F} \setminus \{Q(\alpha)\}}
            \braopket{\Psi}{L_{1}^{Q} \otimes A_{2,\alpha}^{z}}{\Psi} &\le v.
          \end{align}
    \item Consistency of $\{L_{1}^{Q}\}_{Q}$ and $\{L_{2}^{Q}\}_{Q}$:
          \begin{equation}
            \sum_{Q \ne Q' \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]}\braopket{\Psi}{L_{1}^{Q} \otimes L_{2}^{Q}}{\Psi} \le v.
          \end{equation}
  \end{enumerate}
\end{thm}

\begin{proof}
  % TODO
\end{proof}

% TODO: Rename chapter
\chapter{Lifting $\IPCP$ to $\MIP*$}\label{chap:ipcp-to-mip}

\section{Reducing query complexity}\label{sec:reduce-queries}

\begin{thm}[{\cite[Prop.\ 9.2]{CFGS22}}]\label{thm:ipcp-one-query}
  There exists a transformation $T$ such that, for every $m, d \in \mathbb{N}$ and finite
  field $\mathbb{F}$, if
  \begin{equation*}
    (P, V) \in \IPCP\ldipcp{r}{\ell}{c}{q}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{\varepsilon},
  \end{equation*}
  then $T(P, V)$ recognizes the same language as $(P, V)$ and
  \begin{equation*}
    T(P, V) \in \IPCP\ldipcp{r+1}{\ell}{c+\poly(m,d,q)}{1}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{\varepsilon+\frac{mdq}{\abs{\mathbb{F}}-q}}.
  \end{equation*}
\end{thm}

\begin{algorithm}[htbp]
  % TODO: Any bounds on s?
  \KwIn{An honest oracle $R \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ and string $s$}
  \KwOut{Whether or not $s \in L_{(P, V)}$}
  Let $S \subseteq \mathbb{F}$ with $\abs{S} < q$\;
  $V'$: choose random $r \in \mathbb{F}^{m}$\;
  $V'$: choose random $t \in \mathbb{F} \setminus S$\;
  $V'$: Simulate $V$ on $s$ answering every query with
  $0$\;\nllabel{line:zero-sim}
  Let $A$ be the set of queries of $V$\;
  $V'$: compute polynomial $\gamma\colon \mathbb{F} \rightarrow \mathbb{F}^{m}$ of degree $q$
  such that $\{\gamma(s)\}_{s \in S} = A$ and $\gamma(t) = r$\;
  $V'$: Send $\gamma$ to $P'$\;
  $P'$: Reply with the coefficients of the polynomial $\rho = R \circ \gamma$\;
  $V'$: query $R$ at $r$ and recieve answer $a$\;
  \If{$a \ne \rho(t)$}{\nllabel{line:check-rho}
    reject\;
  }
  $V'$: Simulate $V(x)$, answering all queries $a \in A$ with
  $\rho(A)$\;\nllabel{line:proper-sim}
  \KwRet{the result of $V(x)$}\;
  \caption{A single-query, zero-knowledge transformation of an
    IPCP~\cite[Construction 4]{CFGS22}}\label{alg:single-query}
\end{algorithm}

\begin{proof}
  We describe the result of $T$ in \cref{alg:single-query}. First, note that
  this algorithm is not $T$ directly, but the result of $T(P, V)$. So, what we
  require is to show that $L_{T(P, V)} = L_{(P, V)}$, and to briefly demonstrate
  why $V'$ runs in polynomial time.

  To show $V'$ runs in polynomial time, note that the random choices are in
  polynomial time (as a reminder, random choice is polynomial relative to the
  length), simulating $V$ is polynomial time through the definition of $V$, and
  computing polynomials can be done in polynomial time as well.

  When $P'$ is honest, then this will simulate $V$ answering all its queries via
  $\rho$ (which gives the correct answer to every value in $A$); hence $V$ will
  gave the correct answer by definition.

  When $P'$ is dishonest, then we know $\rho$ is a univariate polynomial with
  degree at most $dq$ (since $\rho = R\gamma$). Further note that our random choice of
  both $t$ and $\gamma$ means that $\gamma(t)$ is uniformly distributed over
  $\mathbb{F}^{m}$. Since $\rho$ is a univariate polynomial with degree at most
  $mdq$, then it can agree with $R \circ \gamma$ at most $mdq$ distinct points without
  being equal. Hence, since we query it at a random point in $\mathbb{F} \setminus S$,
  if the check in line~\ref{line:check-rho} passes with at probability bigger
  than $\frac{mdq}{\abs{\mathbb{F}} - q}$, it must mean that $\rho = R\gamma$ on more
  than $mdq$ points. Thus, $\rho = R \circ \gamma$ and so we will get the correct answer for
  all of our queries on $A$. If it passes with a lower probability, then we are
  within our soundness error overall and so the result does not matter.
\end{proof}

Next, we show that \cref{alg:single-query} also preserves zero knowledge. This
is important for us because zero knowledge is \emph{actually} what we care
about.
% TODO: More transition (brief proof overview)

\begin{thm}[{\cite[Prop.\ 9.2]{CFGS22}}]\label{thm:one-query-pzk}
  If $(P, V)$ is perfect zero-knowledge with query bound $b$, then $T(P, V)$
  (where $T$ is the transformation from \cref{thm:ipcp-one-query}) is perfect
  zero-knowledge with query bound $b - (mdq + 1)$.
\end{thm}

\begin{proof}
  We again look at \cref{alg:single-query}. First, notice that if we make a
  query $r \in \gamma(\mathbb{F})$, since $\rho = R\gamma$ we could replace our call to $R$
  with a call to $\rho(\gamma^{-1}(r))$, and this would no longer count towards our
  query bound. Further, from what we know of the degree of $\rho$, we know that in
  the worst case, we would only need $mdq + 1$ queries to $R$ to effectively
  replicate $\rho$. Hence, if we reduce our query bound by $mdq + 1$, it follows
  that we can still effectively simulate the interaction of $P'$ and $V'$, since
  we know $P$ and $V$ have a perfect simulator.
\end{proof}

\section{A lifting algorithm}

\begin{thm}[{\cite[Lemma 9.1]{CFGS22}}]\label{thm:lift-ipcp-mip}
  Let $L$ be a language, let $m, d, q \in \mathbb{N}$, and let $\mathbb{F}$ be a finite
  field of size $\poly(m, d, q)$ sufficiently large. Then, there exists a
  transformation
  \begin{equation*}
    T: \IPCP\ldipcp{r}{\ell}{c}{q}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{\varepsilon}
    \rightarrow \MIP*\mipstar{2}{r+1}{c}{1 - \frac{1}{\poly(m, d)}}.
  \end{equation*}
  such that $(P', V')$ and $T(P', V')$ recognize the same language.

  Further, if the IPCP $(P', V')$ is zero-knowledge with query bound
  $b \ge 2(q+1)md + 3$, then the $\MIP*$ $(P_{1}, P_{2}, V)$ is zero-knowledge.
\end{thm}

\begin{algorithm}[htbp]
  $V$: Choose a random $r \in \{0, 1\}$\;
  \eIf{$r = 0$}{
    $V$: Perform the low multidegree test from \cref{thm:quantum-low-degree}\;
  }{
    \tcp{IPCP emulation}
    $P_{1}$ and $V$ emulate the interaction of the IPCP $(P'', V'') = T(P', V')$
    (see \cref{thm:ipcp-one-query})\;
    The above emulation generates a uniform $\beta \in \mathbb{F}^{m}$, and a
    $c \in \mathbb{F}$ such that with probability $1 - \varepsilon$, $x \in \mathcal{L}$ if and only if
    $R(\beta) \in c$\;
    $V$: ask $P_{2}$ for an evaluation of $R$ at $\beta$\;
    $P_{2}$: reply with an element $z \in \mathbb{F}$\;
    $V$: accept if and only if $c = z$\;
  }
  \caption{Construction of a $\MIP*$ from an IPCP~\cite[Construction 2]{CFGS22}}\label{alg:mip-from-ipcp}
\end{algorithm}

\subsection{Soundness of \cref{alg:mip-from-ipcp}}

\subsection{\Cref{alg:mip-from-ipcp} preserves zero-knowledge}

\begin{algorithm}[htbp]
  % TODO
  \caption{A simulator for \cref{alg:mip-from-ipcp}~\cite[\defaultS 9.4]{CFGS22}}\label{alg:sim-mip}
\end{algorithm}

% TODO: Rename chapter
\chapter{Low-degree $\IPCP$ with zero-knowledge}\label{chap:ipcp-zero-knowledge}

Now that we can construct a zero-knowledge $\MIP*$ instance from a
zero-knowledge IPCP, all that remains is to show that $\NEXP \subseteq \PZKIPCP$. From
there, we will be able to leverage \cref{thm:lift-ipcp-mip} to demonstrate
$\NEXP \subseteq \PZKMIP*$.

% TODO: Should this be moved to the AQC section?
% Should AQC become its own chapter?
\section{AQC of polynomial summation}\label{sec:aqc-poly-sum}

We first need to define the \emph{polynomial summation problem}. We will want a
lower bound on the algebraic query complexity of this problem, similar to the
examples we saw in \cref{sec:alg-query-complexity}.
% TODO: More examples

% FIXME: Relate this to the Sum language we talk about later
\begin{defn}\label{def:poly-sum}\index{polynomial summation problem}
  The \emph{polynomial summation problem} is the following:
  \begin{quote}
    Let $\mathbb{F}$ be a field with $G \subseteq \mathbb{F}$. Let $m, k, d, d' \in \mathbb{N}$ and
    let $Z \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d'}]$.
    % TODO: A few more bounds here as per Lemma 7.1.2?
    What is the value of the polynomial
    \[
      R(X) = \sum_{\beta \in G^{k}}Z(X, \beta) \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]?
    \]
  \end{quote}
\end{defn}

We will need a lower bound on the algebraic query complexity of this problem
(where $Z$ functions as the oracle) later on in order to help demonstrate
zero-knowledge. We will do this with one additional restriction---we will also
need $d'$ to be sufficiently large relative to $G$, but this will not hamper us
in practice. In brief, the lower bound will tell us that so long as we limit
our total queries, we will not recieve \emph{any} information about $R(X)$.
% TODO: What exactly will this allow us to do?

% TODO: Define the polynomial summation problem

\begin{lemma}[{\cite[Lemma 12.1]{CFGS22}}]
  % TODO: Better notation for matrices (more standard M_{r,c}(\mathbb{F}))
  Let $\mathbb{F}$ be a field, $m, k, d, d' \in \mathbb{N}$, and $G, K, L$ be finite
  subsets of $\mathbb{F}$ such that $K \subseteq L$, $d' \ge \abs{G} - 2$, and
  $\abs{K} = d + 1$. If $S \subseteq \mathbb{F}^{m+k}$ is such that there exist matrices
  $C \in \mathbb{F}^{L^{m} \times \ell}$ and $D \in \mathbb{F}^{S \times \ell}$ such that for
  all $Z \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d'}]$ and all
  $i \in \{1, \ldots, \ell\}$
  \begin{equation}\label{eqn:polynomial-sum}
    \sum_{\alpha \in L^{m}}C_{\alpha,i}\sum_{y \in G^{k}}Z(\alpha, y) = \sum_{q \in S}D_{q,i}Z(q),
  \end{equation}
  then $\abs{S} \ge \rk(BC)(\min(d' - \abs{G} + 2, \abs{G}))^{k}$, where
  $B \in \mathbb{F}^{K^{m} \times L^{m}}$ is such that the column of $B$ indexed by $\alpha$
  represents $Z(\alpha)$ in the basis $\{Z(\beta) \mid \beta \in K^{m}\}$.
\end{lemma}

\begin{proof}
  First, if $d' = \abs{G} - 2$, then $d' - \abs{G} + 2 = 0$; hence our bound
  simplifies to
  \begin{align*}
    \abs{S} &\ge \rk(BC)\min(0, \abs{G})^{k} \\
            &\ge 0\rk(BC) \\
            &\ge 0,
  \end{align*}
  which is true regardless of $S$.

  % FIXME: Explain exactly why this works (see notes)
  Otherwise, we can rewrite the left-hand side of \cref{eqn:polynomial-sum} as
  follows:
  \begin{equation}\label{eqn:poly-sum-constant}
    \sum_{\alpha \in L^{m}}C_{\alpha,i}\sum_{y \in G^{k}}Z(\alpha,y) = \sum_{\alpha \in L^{m}}C_{m,i}\sum_{\beta \in K^{m}}b_{\beta,\alpha}\sum_{y \in G^{k}}Z(\beta,y).
  \end{equation}
  Then, define $B \in M_{K^{m},L^{m}}(\mathbb{F})$ to be the matrix whose
  $(i,j)$-entry is $\beta_{i,j}$, and define $C' = BC \in M_{K^{m},\ell}(\mathbb{F})$.
  From that, \cref{eqn:poly-sum-constant} simplifies to
  \begin{equation}\label{eqn:poly-sum-prime}
    \sum_{\alpha \in L^{m}}C_{m,i}\sum_{\beta \in K^{m}}b_{\beta,\alpha}\sum_{y \in G^{k}}Z(\beta,y) = \sum_{\beta \in K^{m}}C'_{\beta,i}\sum_{y \in G^{k}}Z(\beta,y).
  \end{equation}
  % TODO: Explain what H is and why we want it
  Next, define $H \subseteq G$ such that $\abs{H} = \min(d' - \abs{G} + 2, \abs{G})$.
  Further, let
  \[
    P_{0} = \{p \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le \abs{H} - 1}]
      \mid p(q) = 0 \text{ for all } q \in S\}.
  \]
  We can write $P_{0}$ as the kernel of the linear function
  $F_{S}\colon \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le \abs{H} - 1}] \rightarrow \mathbb{F}^{S}$
  defined by $(F_{S}(p))_{s} = p(s)$. By the rank-nullity theorem, this means
  \[
    \dim(P_{0}) \ge (d+1)^{m}\abs{H}^{m} - \abs{S}.
  \]

  Let $A_{0} \in M_{n,(K^{m} \times H^{k})}(\mathbb{F})$ (for some arbitrary $n$) be a
  matrix whose rows form a basis for
  % TODO: Give this a name so we can use it in the later proof
  \[
    \hat{P}_{0} = \{(p(\alpha,y))_{\alpha \in K^{m},y \in H^{k}} \mid p \in P_{0}\}.
  \]
  % TODO: Rephrase
  The dimension of $\hat{P}_{0}$ is exactly the dimension of $P_{0}$; since two
  distinct polynomials of degree $n$ can agree at no more than $n$ points,
  polynomials in $P$ have maximum degree $d^{m}(\abs{H}-1)^{k}$, and each vector
  in $\hat{P}_{0}$ has $\abs{K^{m} \times H^{k}} = (d+1)^{m}\abs{H}^{k}$ points, it
  follows that the map from $p$ to its corresponding vector in $\hat{P}_{0}$ is
  a bijection and thus dimension is preserved.

  For any $y_{0} \in H^{k}$, let $A_{y_{0}} \in M_{n,K^{m}}(\mathbb{F})$ be the
  submatrix of $A_{0}$ consisting only of rows where $y = y_{0}$.
  Since $A_{0}$'s columns form a basis, it has full rank. Further, the set of
  all $A_{y_{0}}$s span the row space of $A_{0}$; hence
  \[
    n = \rk(A_{0}) \le \sum_{y_{0} \in H^{k}}\rk(A_{y_{0}}).
  \]
  Hence, there exists a $y_{0}$ such that
  \[
    \dim(A_{y_{0}}) \ge \frac{\rk(A_{m})}{\abs{H^{k}}}
    \ge \frac{(d+1)^{m}\abs{H^{k}} - \abs{S}}{\abs{H^{k}}} = (d+1)^{m} - \frac{\abs{S}}{\abs{H^{k}}}.
  \]

  Let $q \in \mathbb{F}[Y_{1, \ldots, k}^{\ge \abs{G} - 1}]$ be the polynomial such that
  $q(y_{0}) = 1$ and $q(y) = 0$ for all $y \in G^{k} \setminus \{y_{0}\}$ (as per
  \cref{thm:low-deg-ext-exists}). Then, for all $i \in \{1, \ldots, n\}$ and
  $j \in \{1, \ldots, k\}$ it holds that
  \begin{equation}
    (A_{y_{0}}C')_{ij} = \sum_{\beta \in K^{m}}C'_{\beta,j}p_{i}(\beta, y_{0})
  \end{equation}
  where $p_{i}(\alpha, \beta)$ is the element of $A_{0}$ in row $i$ and column $(\alpha,\beta)$.
  This comes from the definition of matrix multiplication and $A_{y_{0}}$. Next,
  the definition of $q$ gives us
  \begin{equation}
    \sum_{\beta \in K^{m}}C'_{\beta,j}p_{i}(\beta, y_{0}) = \sum_{\beta \in K^{m}}C'_{\beta,j}\sum_{y \in G^{k}}q(y)p_{i}(\beta,y).
  \end{equation}
  Now, since $p_{i}$ is a row of $A_{0}$ and the rows of $A_{0}$ form a basis
  for the space of all outputs of polynomials in $P_{0}$, we can simply treat
  $p_{i}$ as a polynomial in $P_{0}$.

  Note that $qp_{i} \in \mathbb{F}[X_{1, \ldots, k}^{\ge d}, Y_{1, \ldots, k}^{\ge d'}]$ (from
  the definitions of $q$ and $p_{i}$). Hence, \cref{eqn:poly-sum-prime} tells us
  that
  \begin{equation}\label{eqn:sum-qpi}
    \sum_{\beta \in K^{m}}C'_{\beta,j}\sum_{y \in G^{k}}q(y)p_{i}(\beta,y) = \sum_{s \in S}D_{s,i}(qp_{i})(s).
  \end{equation}
  Since $s \in Q$, the definition of $P_{0}$ tells us that $p_{i}(s) = 0$; hence
  the entire sum in \cref{eqn:sum-qpi} is equal to zero and thus
  $A_{y_{0}}C' = 0$.

  Lastly, we can apply Sylvester's rank inequality:
  \begin{align*}
    \rk(A_{y_{0}}) + \rk(C') - (d+1)^{m} &\le \rk(0) \\
    (d+1)^{m} - \rk(C') &\ge \rk(A_{y_{0}}) \\
    (d+1)^{m} - \rk(C') &\ge (d+1)^{m} - \abs{S}/\abs{H}^{k} \\
    \abs{S} &\ge \rk(C')\abs{H}^{k}.
  \end{align*}
  From the definition of $H$, this means
  \[
    \abs{S} \ge \rk(C')(\min(d' - \abs{G} + 2, \abs{G}))^{k},
  \]
  as desired.
\end{proof}

\begin{cor}[{\cite[Corollary 12.2]{CFGS22}}]
  Let $\mathbb{F}$ be a finite field, $G \subseteq \mathbb{F}$, and $d, d' \in \mathbb{N}$ with
  $d' \ge 2(\abs{G} - 1)$. If $S \subseteq \mathbb{F}^{m+k}$ is such that there exist
  $(c_{\alpha})_{\alpha \in \mathbb{F}^{m}}$ and $(d_{\beta})_{\beta \in \mathbb{F}^{m+k}}$ such that
  \begin{enumerate}
    \item for all $Z \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d}]$ it
          holds that
          \begin{equation}
            \sum_{\alpha \in \mathbb{F}^{m}}c_{\alpha}\sum_{y \in G^{k}}Z(\alpha, y) = \sum_{q \in S}d_{q}Z(q),
          \end{equation}
          and
    \item there exists $Z' \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d}]$
          such that
          \begin{equation}
            \sum_{\alpha \in \mathbb{F}^{m}}c_{\alpha}\sum_{y \in G^{k}}Z'(\alpha, y) = 0,
          \end{equation}
  \end{enumerate}
  then $\abs{S} \ge \abs{G}^{k}$.
\end{cor}

\begin{proof}
  % TODO
\end{proof}

From here, we get that the algebraic query complexity of polynomial summation is
at least $\abs{G}^{k}$. That is, if we query $Z$ no more than $\abs{G}^{k}$
times, then we we will recieve \emph{no information} about the polynomial
$R(X) = \sum_{\beta \in G^{k}}Z(X, \beta)$.

\begin{cor}[{\cite[Corollary 12.3]{CFGS22}}]
  Let $\mathbb{F}$ be a finite field, $G \subseteq \mathbb{F}$, and $d, d' \in \mathbb{N}$ with
  $d' \ge 2(\abs{G} - 1)$. Let $Q$ be a subset of $\mathbb{F}^{m+k}$ with
  $\abs{Q} \le \abs{G}^{k}$, and let $Z$ be uniformly random in
  $\mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d'}]$. Then, the random
  % FIXME: Better notation for these
  variables $(\sum_{y \in G^{k}}Z(\alpha, y))_{\alpha \in \mathbb{F}^{m}}$ and $(Z(q))_{q \in Q}$
  are independent.
\end{cor}

\begin{proof}
  We will leverage \cref{thm:lin-indep-stat-indep} that we proved earlier. Now,
  consider the vector space
  \begin{equation} % FIXME: Holy cow this is *ugly* (and exceptionally unclear)
    \mleft\{
      \mleft(
        (Z(\gamma))_{\gamma \in \mathbb{F}^{m+k}}, \mleft(\sum_{y \in G^{k}}Z(\alpha, y)\mright)_{\alpha \in \mathbb{F}^{m}}
      \mright)
      \middlemid
      Z \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d'}]
    \mright\}
  \end{equation}
  This is a vector space over $\mathbb{F}$ with domain
  $\mathbb{F}^{m+k} \cup \mathbb{F}^{m}$. % FIXME: What on earth do they mean "domain"?
  % TODO
\end{proof}

% TODO: Additional theorem that links this to algebraic query complexity somehow

\section{Algebraic commitment schemes}\label{sec:alg-commit-scheme}

We gave an explanation of bit-commitment schemes in
\cref{sec:commitment-scheme}, but there are many circumstances in which
commitment to a single bit is insufficient. In some cases, we will want to
commit to an entire polynomial. This is the setting in which \emph{algebraic
  commitment schemes} exist.

\begin{defn}\label{def:alg-comm-scheme}\index{commitment scheme!algebraic}
  An \emph{algebraic commitment scheme} is a commitment scheme such that instead
  of $S$ recieving a single bit it recieves a polynomial
  $Q\colon \mathbb{F}^{m} \rightarrow \mathbb{F}$.
\end{defn}

\TODO{Mention the role of algebraic query complexity
  (\cref{sec:alg-query-complexity}) in commitment schemes}

\begin{algorithm}[htbp]
  \TODO{Should bring in single-element commitment scheme} \\
  \KwIn{A polynomial $Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d_{Q}}]$}
  Let $K \subseteq \mathbb{F}$ with $\abs{K} = d + 1$\;
  \For{$\alpha \in K^{m}$}{
    sample a random $B^{\alpha} \in \mathbb{F}^{n}$ such that
    $\sum_{i=1}^{m}B^{\alpha}_{i} = Q(\alpha)$\;
  }
  \FIXME{What on earth is $G$?}
  Define $B\colon K^{m} \times G^{k} \rightarrow \mathbb{F}$ by $B(\alpha, x) = B^{\alpha}(x)$\;
  \TODO{$d_{Q}$ vs $d$}
  Define $\hat{B}\colon \mathbb{F}^{m} \times \mathbb{F}^{k} \rightarrow \mathbb{F}$ to be a
  low-degree extension of
  $B$\tcc*{Note: $P \in \mathbb{F}[X_{1, \ldots, m}^{\le d_{Q}}, Y_{1, \ldots, m}^{\le d}]$}
  \KwRet{$\hat{B}$}\;
  \caption{An algebraic commitment scheme~\cite[\defaultS
    12]{CFGS22}}\label{alg:alg-commit-scheme}
\end{algorithm}

We would next like to give an example of an algebraic commitment scheme. \TODO{}

\begin{thm}\label{thm:alg-commit-scheme}
  \Cref{alg:alg-commit-scheme} is a valid algebraic commitment scheme.
\end{thm}

\begin{proof}
  \TODO{}
\end{proof}

\section{The sumcheck problem}

Central to our upcoming work will be a nice answer to the \emph{sumcheck
  problem}, a computational problem about verifying whether or not the sum of a
polynomial over some subset of a field is equal to a provided value. This will
prove useful to us in arithmetizing our problems: if we can turn our boolean
formulae into a question in the sumcheck format, then we can delegate to the
protocol to complete our proof.

\begin{defn}[{\cite{LFKN92}}]\index{sumcheck problem}\label{def:sumcheck}
  % TODO: Should this be a theorem instead?
  The \emph{sumcheck problem} is the following problem:
  \begin{quote}
    Let $H$ be a subset of a finite field $\mathbb{F}$, let
    $F \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ a polynomial over $\mathbb{F}$, and let
    $a \in \mathbb{F}$. Does $\sum_{x \in H^{m}}F(x) = a$?
  \end{quote}
  For this question, we give $H$ and $a$ to both the prover and verifier, but
  only the prover gets access to $F$ as an algebraic oracle.
\end{defn}

\subsection{A non-zero-knowledge sumcheck protocol}

We begin with an interactive protocol for sumcheck that does not have any
zero-knowledge characteristics. While this protocol itself does not preserve
anything, it will form an essential building block for the zero-knowledge
protocols we will construct later.

\begin{algorithm}[htbp]
  % TODO: This looks very similar to algorithm 3.1; can I make a connection there?
  \KwIn{A polynomial $F \in \mathbb{F}[X_{1, \ldots, n}^{\le d}]$, subset $H \subseteq \mathbb{F}$, and number $a$}
  \KwOut{Whether $\sum_{x \in H^{m}}F(x) = a$}
  $P$: send the polynomial $F_{1}(x_{1}) = \sum_{X \in \{0, 1\}^{n-1}}F(x_{1}, X)$\;
  $V$: check if $a = \sum_{x \in H}F_{1}(x)$\;\nllabel{line:init-sumcheck}
  $V$: choose random $r_{1} \in \mathbb{F}$ and send to $P$\;
  \For{$i$ from $2$ to $n$}{
    $P$: send the polynomial
    \[
      F_{i}(x_{i}) = \sum_{X \in H^{m-i}}F(r_{1}, \ldots, r_{i-1}, x_{i}, X)
    \]
    to $V$\;\nllabel{line:equation-sent}
    $V$: check $F_{i-1}(r_{i-1}) = \sum_{x \in H}F_{i}(x)$\;\nllabel{line:seq-sumcheck}
    \If{$i \ne n$}{
      $V$: choose random $r_{i} \in \mathbb{F}$ and send to $P$\;
    }
  }
  $V$: choose random $r_{n} \in \mathbb{F}$\;
  $V$: check $F_{n}(r_{n}) = F(r_{1}, \ldots, r_{n})$\;\nllabel{line:final-check}
  \caption{The standard sumcheck protocol~\cite[Thm.\ 1]{LFKN92}}\label{alg:sumcheck-std}
\end{algorithm}

\begin{thm}\label{thm:sumcheck-ip}
  The sumcheck problem is in $\IP$.
\end{thm}

\begin{proof}
  We show this by implementing an interactive protocol for sumcheck in
  \cref{alg:sumcheck-std}. We need to start by showing that this algorithm will
  correctly answer the sumcheck question. First, if $\sum_{x \in H^{m}}F(x) = a$ and
  $P$ is honest, then $V$s check in line~\ref{line:init-sumcheck} will succeed
  if and only if the question is correct. Again assuming an honest $P$, from the
  definition of each $F_{i}$,
  \[
    F_{i-1}(r_{i-1}) = \sum_{X \in H^{m-i+1}}F(r_{1}, \ldots, r_{i-2}, r_{i-1}, X)
  \]
  and
  \[
    \sum_{x \in H}\sum_{x \in H^{m-i}}F(r_{1}, \ldots, r_{i-1}, x, X) = \sum_{X \in H^{m-i+1}}F(r_{1}, \ldots, r_{i-1}, X).
  \]
  Hence, line~\ref{line:seq-sumcheck} will always succeed with an honest $P$.
  Finally, if $P$ has been honest in the last iteration,
  line~\ref{line:final-check} will always succeed since the sum in the equation
  in line~\ref{line:equation-sent} is now over a single object.

  If $P$ is dishonest, then we show soundness by induction on $n$. If $n = 1$
  then there is only one message sent. Two distinct $n$-variable polynomials of
  degree $d$ can be equal at most $d^{n}$ points, as such in the $n = 1$ case
  the probability of incorrectly passing the check in
  line~\ref{line:final-check} is at most $d/\abs{\mathbb{F}}$.

  Next, assume the $(n - 1)$-variable case has soundness error at most
  $(n - 1)d/\abs{\mathbb{F}}$. Let
  \[
    G_{1}(x_{1}) = \sum_{X \in H^{n-1}}F(x_{1}, X),
  \]
  i.e.\ the ``correct'' value of $F_{1}$ (were $P$ not to lie). If
  $F_{1} \ne G_{1}$, then as we saw before, $F_{1}(r_{1}) \ne G_{1}(r_{1})$ with
  probability $1 - d/\abs{\mathbb{F}}$. If this is the case, then in the rest of
  the loop, the system is attempting to prove the claim
  \[
    F_{1}(r_{1}) = \sum_{X \in H^{n-1}}F(r_{1}, X),
  \]
  which we know to be false. $F(r_{1}, \cdot)$ is an $(n - 1)$-variable polynomial
  of multidegree $d$; by induction this has soundness error
  $(r - 1)d/\abs{\mathbb{F}}$. Thus, $V$ will reject with probability
  \[
    1 - \mathbb{P}[F_{1}(r_{1}) \ne G_{1}(r_{1})] - \mathbb{P}[V \text{ rejects in round } j > 1 \mid F_{1}(r_{1}) \ne G_{1}(r_{1})]
  \]
  which, by substituting in our definitions, gives us probability at least
  \[
    1 - \frac{d}{\abs{\mathbb{F}}} - \frac{d(n - 1)}{\mathbb{F}} = 1 - \frac{dn}{\abs{\mathbb{F}}}.
  \]
  % FIXME: Why is this good enough? (only works for low degree?)

  The only other step is to show that $V$ runs in polynomial time. Since each
  loop iteration only requires checking a sum over $H$, we only need to compute
  the various $F_{i}$s (which are themselves only polynomially larger than the
  original $F$) a total of $nH$ times; computing $F_{i}$ is also in polynomial
  time. Thus, $V$ overall runs in polynomial time.
\end{proof}

\begin{figure}[htbp]
  % TODO
  \caption{An example computation of \cref{alg:sumcheck-std}}\label{fig:sumcheck-ex}
\end{figure}

To illustrate this, in \cref{fig:sumcheck-ex} we provide an example of an
iteration of the protocol when the answer is affirmative and $P$ is honest.

\subsection{A weakly zero-knowledge sumcheck protocol}

% TODO: I've "inlined" this into the fully zero-knowledge algorithm, so I think
% I can remove this section now? The question remains whether I need to do this
% for the simulator as well, I think
Now that we have a non-zero-knowledge interactive protocol for the sumcheck
problem, we need to turn it into a zero-knowledge protocol. While we would very
much like to jump straight from \cref{alg:sumcheck-std} to a perfect
zero-knowledge algorithm, we need to go through one intermediate step first. We
first construct \cref{alg:sumcheck-wzk}, which is only \emph{weakly}
zero-knowledge: while an outside observer to the protocol can learn
\emph{something}, it will only learn a limited amount about the polynomial in
question. Only once we have this will we be able to proceed to the fully
zero-knowledge algorithm.

\begin{defn}\label{def:weak-zk}\index{weak zero-knowledge}\index{zero-knowledge proof!weak}
  A sumcheck protocol is \emph{weak zero-knowledge} if any verifier that makes
  $q$ queries to $R$ learns at most $q$ evaluations of $F$.
\end{defn}

\begin{thm}
  There exists a weakly zero-knowledge algorithm for the sumcheck protocol.
\end{thm}

\begin{algorithm}[htbp]
  % FIXME: This is a model where both also get access to a polynomial R as an
  % oracle
  \KwIn{A subset $H \subseteq \mathbb{F}$ and integer $a$}
  \KwIn{A polynomial $F$, as an algebraic oracle to $P$}
  \KwOut{Whether $\sum_{x \in H^{m}}F(x) = a$}
  $z \leftarrow \sum_{\alpha \in H^{m}}R(a)$\;
  $P$: send $z$ to $V$\;
  $V$: draw random $\rho \in \mathbb{F}$\;
  $V$: send $\rho$ to $P$\;
  $Q \leftarrow \rho F + R$\;
  Both: run \cref{alg:sumcheck-std} on the statement $\sum_{\alpha \in H^{m}}Q(\alpha) = \rho v + z$\;
  \caption{A weakly zero-knowledge sumcheck protocol~\cite[Construction
    5.4]{BCFGRS17}}\label{alg:sumcheck-wzk}
\end{algorithm}

\begin{proof}
  We describe the weakly zero-knowledge algorithm in \cref{alg:sumcheck-wzk}. We
  begin by showing this algorithm implements the sumcheck protocol. Notice that
  this protocol is a very small wrapper around \cref{alg:sumcheck-std}; hence we
  only need to demonstrate that the modified statement will always have the same
  result as the original.
  % TODO
\end{proof}

\subsection{Making the sumcheck protocol zero-knowledge}

% \begin{thm}[{\cite{BCFGRS17}}]
%   There exists a probabilistic algorithm $\mathcal{A}$ such that, for every finite field
%   $\mathbb{F}$, $m, d \in \mathbb{N}$, $H \subseteq \mathbb{F}$, subset
%   $S = \{(\alpha_{1}, \beta_{1}), \ldots, (\alpha_{\ell}, \beta_{\ell})\} \subseteq \mathbb{F}^{\le m} \times \mathbb{F}$,
%   and $(\alpha, \beta) \in \mathbb{F}^{\le m} \times \mathbb{F}$,
%   \begin{equation}
%     \mathbb{P}[\mathcal{A}(\mathbb{F}, m, d, H, S, \alpha) = \beta] =
%     \underset{R \leftarrow \mathbb{F}[X_{1, \ldots, m}^{\le d}]}{\mathbb{P}}\mleft[R(\alpha) = \beta \middlemid
%       \begin{matrix}
%         R(\alpha_{1}) = \beta_{1} \\
%         \vdots \\
%         R(\alpha_{\ell}) = \beta_{\ell}
%       \end{matrix}
%     \mright]
%   \end{equation}
%   Further, $\mathcal{A}$ runs in time
%   $m(d\ell\abs{H} + d^{3}\ell^{3})\poly(\log(\abs{\mathbb{F}})) = \ell^{3}\poly(m, d, \abs{H}, \log(\abs{\mathbb{F}}))$.
% \end{thm}

% \begin{proof}
%   % TODO
% \end{proof}

% TODO: Cite location (\S 13 somewhere)
% TODO: Mention that in 7.1, both P and V get F, but here only P does (for
% zero-knowledge)
% TODO: Note that this isn't /perfect/ zero-knowledge: a single evaluation of F
% is leaked as part of the protocol
\begin{thm}[{\cite{CFGS22}}]\index{zero-knowledge proof!for sumcheck}\label{thm:zk-sumcheck}
  % TODO
  There exists a zero-knowledge variant of \cref{alg:sumcheck-wzk}.
\end{thm}

\begin{algorithm}[htbp]
  % TODO: Where exactly does G come from and who knows about its contents?
  \KwIn{An instance $(H, a)$ to both $P$ and $V$}
  \KwIn{A polynomial $F \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ as an oracle to $P$}
  \KwOut{Whether $\sum_{x \in H^{m}}F(x) = a$}
  $P$: draw uniformly random polynomials
  $Z \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, m}^{\le 2\lambda}]$ and
  $A \in \mathbb{F}[Y_{1, \ldots, k}^{\le 2\lambda}]$\;
  $P$: send the polynomial
  \[
    O(W, X, Y) = W \cdot Z(X, Y) + (1 - W) \cdot A(Y)
  \]
  to $V$\;
  \tcp{Note that $Z = O(1, \cdot)$ and $A = O(0, 0, \cdot)$, so $V$ can use both $Z$ and
    $A$ later}
  $P$: send $z = \sum_{\alpha \in H^{m}}\sum_{\beta \in G^{k}}Z(\alpha, \beta)$ to $V$\;
  $V$: draw a random element $\rho_{1} \in \mathbb{F}^{\times}$ and send to $P$\;
  Run the standard sumcheck IP (\cref{alg:sumcheck-std}) on the statement
  $\sum_{\alpha \in H^{m}}Q(\alpha) = \rho_{1}a + z$, where
  \[
    Q(X_{1}, \ldots, X_{m}) = \rho_{1}F(X_{1}, \ldots, X_{m}) + \sum_{\beta \in G^{k}}Z(X_{1}, \ldots, X_{m}, \beta).
  \]
  % TODO: Format this better
  We have $P$ play the prover and $V$ the verifier, with the following
  modification: For $i = 1, \ldots, m$, in the $i$th round, $V$ samples its random
  element $r_{i}$ from the set $I$ instead of from all of $\mathbb{F}$; if $P$
  ever recieves $r_{i} \in \mathbb{F} \setminus I$, it immediately aborts. In particular,
  in the $m$th round, $P$ sends a polynomial
  \[
    g_{m}(X_{m}) = \rho_{1}F(c_{1}, \ldots, c_{m-1}, X_{m}) + \sum_{\beta \in G^{k}}Z(c_{1}, \ldots, c_{m-1}, X_{m}, \beta)
  \]
  for some $c_{1}, \ldots, c_{m-1} \in I$\;\nllabel{line:sumcheck-1}
  $V$: send $c_{m} \in I$ to $P$\;
  $P$: send $w \in \sum_{\beta \in G^{k}}Z(c, \beta)$ to $V$, where $c = (c_{1}, \ldots, c_{m})$\;
  % Both: engage in the weak-ZK sumcheck protocol (\cref{alg:sumcheck-wzk}) with
  % respect to the claim $\sum_{\beta \in G^{k}}Z(c, \beta) = w$, using $A$ as the masking
  % polynomial. If the verifier in that protocol rejects, so does $V$\;
  $z' \leftarrow \sum_{\alpha \in H^{m}}A(\alpha)$\;
  $P$: send $z$ to $V$\;
  $V$: draw random $\rho \in \mathbb{F}^{\times}$\;
  $V$: send $\rho$ to $P$\;
  $Q'(x) \leftarrow \rho Z(c, x) + A$\;
  Both: run \cref{alg:sumcheck-std} on the statement $\sum_{\alpha \in H^{m}}Q'(\alpha) = \rho w + z'$\;
  $V$: output the claim $F(c) = \frac{g_{m}(c_{m}) - w}{\rho_{1}}$\;\nllabel{line:sumcheck-2}
  \caption{Strong zero-knowledge sumcheck~\cite[Construction 3]{CFGS22}}\label{alg:zk-sumcheck}
\end{algorithm}

\begin{proof}
  % TODO
  We describe the algorithm in \cref{alg:zk-sumcheck}, and then show it is
  zero-knowledge by constructing a simulator in \cref{alg:zk-sumcheck-fast}.

  If $P$ is honest in \cref{alg:zk-sumcheck}, then both sumcheck protocols will
  pass if and only if $(F, H, a)$ is valid. From the various definitions in the
  program, we have
  \begin{align*}
    \sum_{\alpha \in H^{m}}Q(a) &= \rho_{1}a + z \\
    \sum_{\alpha \in H^{m}}\mleft(\rho_{1}F(\alpha) + \sum_{\beta \in G^{k}}Z(\alpha, \beta)\mright) &= \rho_{1}a + \sum_{\alpha \in H^{m}}\sum_{\beta \in G^{k}}Z(\alpha, \beta) \\
    \sum_{\alpha \in H^{m}}\rho_{1}F(a) + \sum_{a \in H^{m}}\sum_{\beta \in G^{k}}Z(\alpha, \beta) &= \rho_{1}a + \sum_{\alpha \in H^{m}}\sum_{\beta \in G^{k}}Z(\alpha, \beta) \\
    \rho_{1}\sum_{a \in H^{m}}F(a) &= \rho_{1}a \\
    \sum_{a \in H^{m}}F(a) &= a.
  \end{align*}
  Since $\rho_{1} \ne 0$, all of these transformations are biconditionally true;
  hence the sumcheck protocol in line~\ref{line:sumcheck-1} will pass if and
  only if $(F, H, a)$ is valid. The modification does not affect this
  correctness since all it does is limit the set of elements we can randomly
  sample from---since we know this works for all $r_{i} \in \mathbb{F}$, it will
  also be true for all $r_{i} \in I$.

  For the second sumcheck (in line~\ref{line:sumcheck-2}), we have
  \begin{align*}
    \sum_{\alpha \in H^{m}}Q'(\alpha) &= \rho w + z \\
    \sum_{\alpha \in H^{m}}\mleft(\rho Z(c, \alpha) + A(\alpha)\mright) &= \rho w + \sum_{\alpha \in H^{m}}A(\alpha) \\
    \sum_{\alpha \in H^{m}}\rho F(\alpha) + \sum_{\alpha \in H^{m}}A(\alpha) &= \rho w + \sum_{\alpha \in H^{m}}A(\alpha) \\
    \rho\sum_{\alpha \in H^{m}}F(\alpha) &= \rho w \\
    \sum_{\alpha \in H^{m}}F(\alpha) &= w.
  \end{align*}
  As before, these transformations are all biconditionally true; hence an honest
  $P$ will always cause the sumcheck in line~\ref{line:sumcheck-2} to succeed if
  and only if $(F, H, a)$ is valid.

  If $P$ is dishonest, things get trickier.
  % TODO
\end{proof}

\begin{algorithm}[htbp]
  Draw $Z_{s} \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le 2\lambda}]$\;
  Run the weak-ZK sumcheck simulator\; % FIXME: Which algorithm is this?
  Begin simulating $\tilde{V}$, answering its oracle queries with $Z_{s}$ and
  the simulated $A$\;
  Send $z_{s} = \sum_{\alpha \in H^{m}}\sum_{\beta \in G^{k}}Z_{s}(\alpha, \beta)$\;
  Recieve $\tilde{\rho}$\; % FIXME: What is this?
  Draw $Q_{s} \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ such that
  $\sum_{\alpha \in H^{m}}Q_{s}(\alpha) = \tilde{\rho}\alpha + z_{s}$\;
  Engage in the sumcheck protocol (\cref{alg:sumcheck-std}) on the claim
  $\sum_{\alpha \in H^{m}}Q_{s}(\alpha) = \tilde{\rho}\alpha + z_{s}$\;
  \If{$\tilde{V}$ sends $c_{i} \notin I$ as a challenge in the above protocol}{
    abort\;
  }
  Let $c \in I^{m}$ be the point chosen by $\tilde{V}$\;
  Query $F(c)$\;
  $w_{s} \leftarrow Q_{s}(c) - \tilde{\rho}F(c)$\;
  Send $w_{s}$\;
  Draw $Z_{s}' \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le 2\lambda}]$ such that
  % FIXME: Why are the line numbers disappearing?
  \begin{itemize}
    \item $\sum_{\beta \in G^{k}}Z_{s}'(c, \beta) = w_{s}$
    \item $Z_{s}'(\gamma) = Z_{s}(\gamma)$ for all previous queries $\gamma$\;
  \end{itemize}
  Use $S'$ to simulate the sumcheck protocol for the claim
  $\sum_{\beta \in G^{k}}Z_{s}'(c, \beta) = w_{s}$\;
  Output the view of the simulated $\tilde{V}'$\;
  % TODO
  \caption{An inefficient simulator for
    \cref{alg:zk-sumcheck}~\cite[p.\ 15:33]{CFGS22}}\label{alg:zk-sumcheck-sim}
\end{algorithm}

\begin{algorithm}[htbp]
  % TODO
  \caption{An efficient variant of
    \cref{alg:zk-sumcheck-sim}~\cite[p.\ 15:34]{CFGS22}}\label{alg:zk-sumcheck-fast}
\end{algorithm}

% TODO: Zero-knowledge sumcheck

\section{Extending the sumcheck algorithm to $\NEXP$}

% FIXME: CFGS22 claims BFL90 has a IPCP for NEXP, but IPCPs weren't invented
% until 18 years after BFL90 was published?

% TODO: Define PZK-IPCP

Armed with our sumcheck algorithm, we are ready to take on the fight of the rest
of $\NEXP$. Like so many other theorems, we will do this with a $\NEXP$-complete
problem, and as before our problem of choice is $\OSAT$.

\begin{thm}[{\cite[Thm.\ 14.2]{CFGS22}}]\label{thm:pzkipcp-for-nexp}
  There exists a $c \in \mathbb{N}$ such that for any query-bound function $b(n)$,
  $d(n) \in \Omega(n^{c})$, $m(n) \in O(n^{c}\log(b))$, and any sequence of fields
  $\mathbb{F}(n)$ that are field extensions of $\mathbb{F}_{2}$ with
  $\abs{\mathbb{F}(n)} \in \Omega((n^{c}\log(b))^{4})$,
  \begin{equation*}
    \OSAT \in \IPCP\ldipcp{O(n, b)}{\poly(2^{n}, b)}{\poly(n, \log(b))}{\poly(n, \log(b))}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{1/2},
  \end{equation*}
  which is zero-knowledge with query bound $b$.
\end{thm}

\begin{algorithm}[htbp]
  \Repeat{$\sum_{\beta \in G^{k}}Z(\alpha, \beta) = A(\gamma_{2}(\alpha))$ for all $\alpha \in H^{m_{2}}$}{
    $P$: Draw a random
    $Z \in \mathbb{F}[X_{1, \ldots, m}^{\le \abs{H} + 2}, Y_{1, \ldots, m}^{\le 2\abs{H}}]$\;
  }
  % TODO
  \For{$i \in \{1, 2, 3\}$}{
    $P$ and $V$ implement \cref{alg:sumcheck-wzk} on the claim
    $\sum_{\beta \in H^{k}}Z(c_{i}', \beta) = h_{i}$\;
  }
  \caption{A low-degree IPCP for $\OSAT$~\cite[p.\ 15:36]{CFGS22}}\label{alg:ipcp-o3sat}
\end{algorithm}

\begin{algorithm}[htbp]
  Draw a random polynomial
  $Z_{s} \in \mathbb{F}[X_{1, \ldots, m_{2}}^{\le \abs{H}+2}, Y_{1, \ldots, k}^{\le 2\abs{H}}]$\;
  TODO\; % Step 2 is weird
  Recieve $x, y \in \mathbb{F}^{3s + r}$ from $\tilde{V}$\;
  TODO\; % Step 4 is also weird
  \For{$i \in \{1, 2, 3\}$}{
    % TODO: Simulation is a little strange and I want to check what the shared
    % secret polynomial is
  }
  \caption{A simulator for \cref{alg:ipcp-o3sat}~\cite[p.\ 15.37]{CFGS22}}\label{alg:o3sat-simulator}
\end{algorithm}

\begin{proof}
  We demonstrate this through an algorithm for $\OSAT$, which we have laid out
  in \cref{alg:ipcp-o3sat}. We will show this is zero-knowledge by implementing
  a simulator in \cref{alg:o3sat-simulator}.
  % TODO
\end{proof}

\begin{cor}\label{nexp-pzkipcp}
  $\NEXP \subseteq \PZKIPCP$.
\end{cor}

\begin{proof}
  Since $\OSAT$ is $\NEXP$-complete as per \cref{thm:o3sat-nexp-complete}, we
  can perform a polynomial reduction from any other language to $\OSAT$ and then
  run \cref{alg:ipcp-o3sat}.
\end{proof}

\section{Zero-knowledge $\MIP*$ for $\NEXP$}\label{sec:zk-mipstar-nexp}

% TODO: Rename chapter
\chapter{Zero-knowledge PCPs for $\#\P$}\label{chap:zk-pcp-for-hp}

\section{Constraint location}\label{sec:constraint-loc}

\section{ZKPCPs for $\#\SAT$}\label{sec:zkpcp-hsat}

\begin{thm}[{\cite[Theorem 8.1]{GOS24}}]\label{thm:pzkpcp-for-sat}
  There exists a perfect zero-knowledge PCP for $\#\SAT$.
\end{thm}

\begin{cor}\label{cor:hashp-subset-pzkpcp}
  $\#\P \subseteq \PZKPCP$.
\end{cor}

\chapter{A zero-knowledge PCP theorem}\label{chap:zk-pcp-theorem}

\section{Locally-computable proofs}\label{sec:loc-comp-proof}

% TODO: This will probably need to be moved to the original PCP theorem chapter

\begin{defn}[{\cite[Def.\ 3.1]{GOS25}}]\label{def:loc-comp}%
  \index{locally-computable proof}
  Let $A$ and $A_{0}$ be randomized Turing machines, and let $\ell: \mathbb{N} \rightarrow \mathbb{N}$. Then
  $A$ is \emph{$\ell$-locally computable} from $A_{0}$ on a subset
  % FIXME: What is the oracle of f?
  $C \subseteq \{0, 1\}^{*}$ if there exists an oracle Turing machine $f$ that runs in
  polynomial time and makes no more than $\ell$ queries to its oracle such that for
  every $x \in C$, the distribution of $A(x)$ is identical to the distribution of
  $f$ with oracle $\pi_{0} = A_{0}(x)$.
  % TODO: Be more clear about what this oracle actually is
  % $(f^{\pi_{0}} \mid \pi_{0} \leftarrow A_{0}(x))$
\end{defn}

% FIXME: This paper refers to proofs as algorithms computable from their input:
% this conflicts with how we have been referring to PCPs so far

\begin{thm}[{\cite[Lemma 3.2]{GOS25}}]\label{thm:local-comp-pzk}
  % TODO
  % FIXME: Is (P, V) supposed to have the same language as (P_0, V_0)?
  Let $(P_{0}, V_{0})$ be a PZK-PCP for some language $L$ with query bound
  $q^{*}$, and let $(P, V)$ be a PCP for a langauge $M$ such that $P$ is
  $\ell$-locally computable from $V$ on $M$. Then $(P, V)$ is perfect
  zero-knowledge with query bound $q^{*}/\ell$.
\end{thm}

% TODO: Define hybrid simulator somewhere
\begin{algorithm}[htbp]
  \KwIn{A string $x$ and random coins $r$, as well as oracle access to a
    function $\pi_{0}$}
  \KwOut{The interaction transcript of $(P, V^{*})$ on input $x$}
  % TODO: KwIn/KwOut
  % NOTE: r is an input
  $T \leftarrow \lbrack\,\rbrack$\;
  Run $V^{*}$ on random coins $r$\;
  \For{each query $\alpha$ that $V^{*}$ makes}{
    $\beta \leftarrow f^{\pi_{0}}(\alpha)$\;
    Push $(\alpha, \beta)$ onto $T$\;
  }
  \KwRet{$(r, T)$}\;
  \caption{A hybrid simulator for a locally-computable PCP~\cite[Construction
    3.3]{GOS25}}\label{alg:hybrid-sim-lc}
\end{algorithm}

\begin{algorithm}[htbp]
  \KwIn{A string $x$}
  \KwOut{The interaction transcript of $(P, V^{*})$ on input $x$}
  % TODO: Define \overline{\Sim}
  Run $\overline{\Sim}_{A_{V^{*}}}(x)$ to obtain $T_{0}$ with random coins $r$\;
  Run $A_{V^{*}}(x, r)$ (\cref{alg:hybrid-sim-lc}) using $T_{0}$ to answer its
  questions\;
  \KwRet{$(r, T)$}\;
  \caption{A PZK simulator for a locally-computable PCP~\cite[Construction
    3.4]{GOS25}}\label{alg:pzk-sim-lc}
\end{algorithm}

\begin{proof}
  We construct a simulator for $(P, V)$ in \cref{alg:pzk-sim-lc}.

  Let $V^{*}$ be a malicious verifier for $(P, V)$, and let $\pi \leftarrow P$ and
  $\pi_{0} \leftarrow P_{0}$ be random variables. Since $P$ is $\ell$-locally computable with
  the function $f$, by definition we have that $\pi$ is identically-distributed to
  $(f^{\pi_{0}}(\alpha))_{\alpha \in \dom(\pi_{0})}$.
  % FIXME: Not quite by definition; the definition is identically-distributed
  % for all x but this is a pretty simple jump
  Since all \cref{alg:hybrid-sim-lc} does is compute $f^{\pi_{0}}(\alpha)$ for each
  query $\alpha$, it will reproduce the same transcript as $(P, V^{*})$ would.

  % FIXME: Why is this a verifier?
  Next, since $(P_{0}, V_{0})$ is a PZK-PCP, and \cref{alg:hybrid-sim-lc} is a
  verifier for $\pi_{0}$, by definition there exists a simulator
  $\overline{\Sim}_{A_{V^{*}}}$ whose output is identically-distributed to
  $\View_{A_{V^{*}}, P_{0}}$, so long as $A_{V^{*}}$ makes no more than $q^{*}$
  queries to $\pi_{0}$. Since $P$ is $\ell$-locally computable from $P_{0}$, it
  follows that \cref{alg:hybrid-sim-lc} makes no more than $\ell$ queries to
  $\pi_{0}$. Hence, \cref{alg:pzk-sim-lc} makes no more than $q^{*}$ queries to
  $\pi_{0}$ so long as $V^{*}$ makes no more than $q^{*}/\ell$ queries to $\pi$.
  % TODO
\end{proof}

\begin{cor}\label{cor:local-comp-pzk}
  % TODO: Previous theorem but for PCPPs
  Let $(P_{0}, V_{0})$ be a PZK-PCPP for some language $L$ with query bound
  $q^{*}$ and proximity parameter $\delta$, and let $(P, V)$ be a PCPP for a langauge
  $M$ with proximity parameter $\delta$ such that $P$ is $\ell$-locally computable
  from $V$ on $M$. Then $(P, V)$ is perfect zero-knowledge with query bound
  $q^{*}/\ell$.
\end{cor}

\begin{proof}
  % TODO
\end{proof}

\section{Zero-knowledge proof composition}\label{sec:zk-proof-comp}

\begin{thm}[{\cite[Theorem 3.7]{GOS25}}]\label{thm:comp-pzk}
  The construction in \cref{thm:composition} is perfect zero-knowledge with
  query bound $q^{*}/q_{\out}$ if $V_{\out}$ is perfect zero-knowledge with
  query bound $q^{*}$.
\end{thm}

\begin{algorithm}[htbp]
  % TODO: Steps 2-4 from the composed PCP
  % Define D_out and \pi_out
  \KwIn{A string $r \in \{0, 1\}^{r_{\out}}$}
  \KwOut{The function $\pi_{r}$}
  $I_{\out} \leftarrow Q_{\out}(x, r)$\;
  Compile $D_{\out}$ on input $r$ into a circuit
  $C_{\out}: \{0, 1\}^{n} \times \{0, 1\}^{\ell_{\out}} \rightarrow \{0, 1\}$\;
  Run $P_{\oin}(C_{\out}, \pi_{\out}|_{I_{\out}})$ to get $\pi_{r}$\;
  \KwRet{$\pi_{r}$}\;
  \caption{An algorithm for $\pi_{r}$ from $\pi_{0}$}\label{alg:pi-r-from-pi-0}
\end{algorithm}

\begin{proof}
  All that is needed to prove this theorem is to show that
  \cref{alg:composed-pcp} preserves zero-knowledge, since we have already showed
  that it satisfies the conditions in \cref{thm:composition}. We do this by
  showing $P_{\comp}$ is $q_{\out}$-locally computable from $P_{\out}$.

  We need to show that for any input $x \in L$, the distributions of $P(x)$ and
  $f^{\pi_{0}}(x)$ are identically distributed. Consider the function
  % TODO: Lemma about these "split instances": we seem to do this a lot
  \begin{equation}
    f(O, r) = \begin{cases}
      \pi_{0}(r) & O = b_{0} \\
      \text{\cref{alg:pi-r-from-pi-0}} & O = b_{r}.
    \end{cases}
  \end{equation}
  % TODO: Correctness (why does above algorithm compute?)
  If $O = b_{0}$, then we make no more than one query to $\pi_{0}$. If
  $O = b_{r}$, then \cref{alg:pi-r-from-pi-0} makes no more than
  $\abs{I_{\out}}$ queries to $\pi_{\out}$ (since that is the size of the domain
  of the restricted function); regardless of $r$ we have that
  $\abs{I_{\out}} \le q_{\out}$ since $I_{\out}$ is a set of queries and
  $q_{\out}$ is the maximum number of queries that $V_{\out}$ makes. Hence $f$
  makes no more than $q_{\out}$ queries.

  Lastly, so long as $V_{\out}$ runs in polynomial time, so must $D_{\out}$ and
  $Q_{\out}$; compiling into a circuit is also known to take polynomial time.
  % TODO: Reference
  Lastly, the problem statement tells us $P_{\oin}$ is guaranteed to run in
  polynomial time; it follows that $f$ runs in polynomial time. Lastly, since
  \cref{alg:pi-r-from-pi-0} uses its input $r$ as randomness to all the
  algorithms it calls, it follows that $f$ itself is deterministic (since all
  randomness comes from the choice of $r$). Hence, $P_{\comp}$ is
  $q_{\out}$-locally computable from $P_{\out}$.

  Since $P_{\comp}$ is $q_{\out}$-locally computable from $P_{\out}$, by
  \cref{thm:local-comp-pzk} we have that $P_{\comp}$ is perfect zero-knowledge
  with query bound $q^{*}/q_{\out}$.
\end{proof}

\section{Zero-knowledge alphabet reduction}\label{sec:zk-alph-red}

% TODO: Talk a little about what alphabet reduction means (also earlier)

\begin{thm}[{\cite[Lemma 2.13]{BGHSV06}}]%
  \label{thm:zk-alph-red}\label{alphabet reduction!zero-knowledge}
  Let $L$ be a language with a PZK-PCP over the language $\{0, 1\}^{a}$ such
  that
  \[
    L \in \PZKPCP\pzkpcpr{r}{q}{q^{*}}{\varepsilon}{s}{\rho}.
  \]
  Then $L$ has a PZK-PCP over the language $\{0, 1\}$ such that
  \[
    L \in \PZKPCP\pzkpcpr{r}{O(aq)}{q^{*}}{\varepsilon}{s}{\Omega(\rho)}.
  \]
\end{thm}

\begin{proof}
  We proved the non-zero-knowledge version of this as \cref{thm:alph-reduction},
  so all that remains is to prove that \cref{alg:boolean-reduction} preserves
  zero-knowledge.

  First, we show $P$ is $1$-locally computable from $P_{a}$. Consider the
  function $f$ (with oracle $\pi_{a}$) defined by
  % TODO: Refresher on the terms
  % TODO: Calling O \pi_a and \tau is a little confusing since we are not passing the
  % functions themselves, just a marker
  \begin{equation}\label{eqn:local-alph-red}
    \begin{aligned}
      % FIXME: Do I have the codomain right?
      f^{\pi_{a}}\colon \{b_{\pi}, b_{\tau}\} \times \dom(\pi_{a}) \times [n] &\rightarrow \{0, 1\} \\
      f^{\pi_{a}}(O, \alpha, i) &\mapsto \begin{cases}
        \pi_{a}(\alpha) & O = b_{\pi} \\
        \ECC(\pi_{a}(\alpha))_{i} & O = b_{\tau}.
      \end{cases}
    \end{aligned}
  \end{equation}
  We show the distribution of $P$ is the same as the distribution of $f$ with
  oracle from $P_{a}$ for all $x$.
  % TODO

  % TODO: Don't like this phrasing particularly much (introduce domain
  % shenanigans before defining f?)
  The domain for our function is three values: $O$, a marker for either $\pi_{a}$
  or $\tau$; $\alpha$, the value we query; and $i$, the bit of the error-correcting code
  we wish to query. This is equivalent to a way to access the output of $P$ from
  \cref{alg:boolean-reduction}; since it returns an ordered pair of two
  functions $(\pi_{a}, \tau)$, whenever we query it we need first to choose which of
  the two functions to query, the value we are querying them on, and then since
  we can only look at one bit, if we are querying $\tau$ we need to also determine
  which specific bit we are asking for. Hence, for any $(O, \alpha, i)$, the
  definition of $f$ means that $\pi(O, \alpha, i) = f^{\pi_{0}}(O, \alpha, i)$. Hence the two
  are identically distributed and thus $P$ is $1$-locally computable from
  $P_{a}$.

  Since $P$ is $1$-locally computable from $P_{a}$, it follows from
  \cref{thm:local-comp-pzk} that $P$ is perfect zero-knowledge with the same
  query bound as $P_{a}$.
\end{proof}

\begin{cor}\label{cor:constant-query}
  % TODO: Define all these
  % FIXME: Get this equation number to be in a reasonable place
  \begin{equation}\label{eqn:constant-query}
    \PZKPCP\pzkpcpr{r}{q}{q^{*}}{\varepsilon}{s}{\Omega(1)} \subseteq
    \PZKPCP\pzkpcpr{r + \log(n)}{q}{q^{*}/q}{\varepsilon}{s}{\Omega(1)}.
  \end{equation}
\end{cor}

\begin{proof}
  % TODO
\end{proof}

\section{Robust total-degree test}\label{sec:robust-degree}

\begin{thm}[{\cite[Prop.\ 5.7]{Par21}}]\label{thm:robust-low-deg}
  % TODO: Rephrase in terms of PCPPs if possible
\end{thm}

\begin{algorithm}[htbp]
  % TODO
  \caption{A robust low-degree test~\cite[Prop.\ 5.7]{Par21}}\label{alg:robust-low-deg}
\end{algorithm}

\section{PCPPs for polynomial summation}\label{sec:pcpp-poly-sum}

% TODO: This looks like the same language we've seen earlier in this paper: do
% we want this as the formal definition somewhere or are there better ones?
\begin{defn}[{\cite[Def.\ 4.1]{GOS25}}]\label{def:sum-lang}\index{Sum@$\Sum$}
  The language $\Sum$ is the set of all ordered pairs
  $((\mathbb{F}, 1^{m}, 1^{d}, H, \gamma), F)$, where
  \begin{itemize}
    \item $\mathbb{F}$ is a finite field,
    \item $m, d \in \mathbb{N}$,
    \item $H \subseteq \mathbb{F}$,
    \item $\gamma \in F$, and
    \item $F \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ with
          \[
            \sum_{b \in H^{m}}F(b) = \gamma.
          \]
  \end{itemize}
\end{defn}

\begin{defn}\label{def:sum-params}
  For a fixed finite field $\mathbb{F}$, $m, d \in \mathbb{N}$, $H \subseteq \mathbb{F}$, and
  $\gamma \in \mathbb{F}$, the language $\Sum[\mathbb{F}, m, d, H, \gamma]$ is the subset of
  $\Sum$ where the co-named parameters in \cref{def:sum-lang} are equal to their
  fixed values.
\end{defn}

% FIXME: Separate proof & verifier more cleanly
\begin{algorithm}[htbp]
  \tcc{Proof}
  \For{$i \in \{1, \ldots, m - 1\}$}{
    $g_{i}(X) \leftarrow \sum_{b \in H^{m-i}}F(X, b)$\;
  }
  Define $\pi\colon \mathbb{F}^{m-1} \rightarrow \mathbb{F}^{m-1}$ by
  \[
    \pi(c_{1}, \ldots, c_{m-2}, \alpha) = (g_{1}(\alpha), g_{2}(c_{1}, \alpha), \ldots, g_{m-1}(c_{1}, \ldots, c_{m-2}, \alpha))
  \]
  for each $(c_{1}, \ldots, c_{m-2}, \alpha) \in \mathbb{F}^{m-1}$\;
  \KwRet{$\pi$}\;
  \tcc{Verifier}
  Sample $c \in \mathbb{F}^{m-1}$ at random\;
  \For{$\alpha \in \mathbb{F}$}{
    Query $\pi(c_{1}, \ldots, c_{m-2}, \alpha)$\;
    Query $F(c_{1}, \ldots, c_{m-1}, \alpha)$\;
  }
  \For{$i \in \{1, \ldots, m-1\}$}{
    \If{$g_{i} \notin \mathbb{F}[X_{1, \ldots, i}^{\le d}]$}{
      reject\;
    }
  }
  Check $\sum_{b \in H}g_{1}(b) = \gamma$\;
  \For{$i \in \{1, \ldots, m - 2\}$}{
    Check
    \begin{algomathdisplay}
      \sum_{b \in H}g_{i+1}(c_{1}, \ldots, c_{i}, b) = g_{i}(c_{1}, \ldots, c_{i})
    \end{algomathdisplay}
  }
  Check
  \begin{algomathdisplay}
    \sum_{b \in H}F(c, b) = g_{m-1}(c)
  \end{algomathdisplay}
  Run \cref{alg:robust-low-deg} on $F$, with proximity parameter
  $\delta_{R} = \min(\delta, 1/5)$\;
  Accept if and only if the prior test passes\;
  \caption{A robust PCPP for $\Sum$~\cite[Construction 4.3]{GOS25}}\label{alg:sum-pcpp}
\end{algorithm}

% TODO: I think this can/should be moved to the normal PCP theorem section?
\begin{thm}
  Let $\delta > 0$, $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$,
  $\gamma \in \mathbb{F}$, and $m, d \in \mathbb{N}$ such that $\frac{md}{\abs{\mathbb{F}}} < \delta$
  and $d > \abs{H} + 1$. Then there exists a PCP of
  proximity for $\Sum[\mathbb{F}, m, d, H, \gamma]$ over the alphabet
  $\mathbb{F}^{m+1}$ with proximity parameter $\delta$ and robustness parameter
  $\rho = \Omega(\delta)$. Further, the verifier makes $O(\abs{m})$ queries to $F$ and $\pi$
  and the proof length is $O(\abs{\mathbb{F}}^{m})$.
\end{thm}

\begin{proof}
  We construct such an algorithm as \cref{alg:sum-pcpp}. % TODO
\end{proof}

\begin{algorithm}[htbp]
  \tcc{Proof}
  Sample $Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ uniformly\;
  \For{$i \in \{1, \ldots, m\}$}{
    Sample
    $T_{i} \in \mathbb{F}[X_{1, \ldots, i-1}^{\le d}, X_{i}^{\le d-\abs{H}}, X_{i+1, \ldots, m}^{\le d}]$
    uniformly\;
  }
  Define $\pi_{P}(x) = (Q(x), T_{1}(x), \ldots, T_{m}(x))$\;
  Define $Z_{H} = \prod_{a \in H}(X - a)$\;
  Define $Q_{\text{rev}}(X) = Q(X_{\text{rev}})$\; % FIXME: I think rev means reversed?
  Define $R(X) = Q(X) - Q_{\text{rev}}(X) + \sum_{i=1}^{m}Z_{H}(X_{i})T_{i}(X)$\;
  Run \cref{alg:sum-pcpp} with explicit input $(\mathbb{F}, m, d, H, \gamma, \delta)$ and
  implicit input $F + R$\;
  \KwRet{$(\pi_{\Sigma}, \pi_{P})$}\;
  \tcc{Verifier}
  Emulate \cref{alg:sum-pcpp} on input $F + R$ and proof $\pi_{\Sigma}$. To query
  $F + R$ at some $\alpha \in \mathbb{F}^{m}$, query $F(\alpha)$, $\pi_{P}(\alpha)$, and
  $\pi_{P}(\alpha_{\text{rev}})$, then compute
  \begin{algomathdisplay}
    (F+R)(\alpha) = F(\alpha) + (\pi_{P}(\alpha))_{1} - (\pi_{P}(\alpha_{\text{rev}}))_{1} + \sum_{i=1}^{m}Z_{H}(\alpha_{i})(\pi_{P}(\alpha))_{i+1}
  \end{algomathdisplay}
  Perform \cref{alg:robust-low-deg} on $\pi_{P}$ with proximity parameter
  $\varepsilon_{P} = \delta_{R}/8$ and degree parameter $d_{P} = md$\;
  \If{\cref{alg:robust-low-deg} fails}{
    reject\;
  }
  Accept\;
  % TODO
  \caption{A zero-knowledge robust PCPP for $\Sum$~\cite[Construction
    5.2]{GOS25}}\label{alg:sum-pzk-pcpp}
\end{algorithm}

\begin{thm}[{\cite[Lemma 5.1]{GOS25}}]\label{thm:pcpp-sum-pzk}
  % TODO: Rewrite in bracket notation
  Let $\delta > 0$, $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$,
  $\gamma \in \mathbb{F}$, and $m, d \in \mathbb{N}$ such that $\frac{md}{\abs{\mathbb{F}}} < \delta$
  and $d > \abs{H} + 1$. Then there exists a perfect zero-knowledge PCP of
  proximity for $\Sum[\mathbb{F}, m, d, H, \gamma]$ over the alphabet
  $\mathbb{F}^{m+1}$ with proximity parameter $\delta$ and robustness parameter
  $\rho = \Omega(\delta)$. Further, the verifier makes $O(\abs{m})$ queries to $F$ and $\pi$
  and the proof length is $O(\abs{\mathbb{F}}^{m})$.
\end{thm}

\begin{proof}
  We construct such an algorithm as \cref{alg:sum-pzk-pcpp}. % TODO
\end{proof}

\section{A zero-knowledge $\PCP$ for $\NP$ and $\NEXP$}\label{sec:pzkpcp-np-nexp}

\begin{thm}[{\cite[Theorem 6.3]{GOS25}}]\label{thm:np-zk-pcp}
  For any query bound $q^{*}(n) \le 2^{\poly(n)}$,
  \[
    \NEXP \subseteq
    \PZKPCP_{\Sigma(n)}\pzkpcpr{\log(n)+\log(q^{*}(n))}{\poly(\log(n) + \log(q^{*}(n)))}{q^{*}(n)}{\varepsilon}{s}{\Omega(1)}.
  \]
  where $\abs{\Sigma(n)} = \poly(n, q)$.
\end{thm}

\begin{algorithm}[htbp]
  % TODO
  \caption{A $\PZKPCP$ for $\NP$~\cite[Theorem 6.3]{GOS25}}\label{alg:pzkpcp-np}
\end{algorithm}

\begin{algorithm}[htbp]
  % TODO
  \caption{A simulator for \cref{alg:pzkpcp-np}}\label{alg:pzkpcp-np-sim}
\end{algorithm}

\begin{proof}
  % TODO
\end{proof}

\begin{thm}[{\cite[Theorem 6.3]{GOS25}}]\label{thm:nexp-zk-pcp}
  For any query bound $q^{*}(n) \le 2^{\poly(n)}$,
  \[
    \NEXP \subseteq
    \PZKPCP_{\Sigma(n)}\pzkpcpr{\poly(n)+\log(q^{*}(n))}{\poly(n)}{q^{*}(n)}{\varepsilon}{s}{\Omega(1)}.
  \]
  where $\Sigma(n)$ is any alphabet with $\abs{\Sigma(n)} \in \poly(n, q)$.
\end{thm}

\begin{algorithm}[htbp]
  % TODO
  \tcc{Proof}
  Let $A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ be a satisfying assignment for $B$\;
  Choose $\hat{C} \in \mathbb{F}[X_{m_{2} + k}^{\le 2(\abs{H} - 1)}]$ randomly such
  that
  \[
    \sum_{c \in H^{k}}\hat{C}(b, c) = A(\gamma_{2}, b)
  \]
  for all $b \in H^{m_{2}}$\;
  \For{$\tau \in \mathbb{F}^{m_{1}+3m_{2}+3}$}{
    Let $\pi_{\tau}$ be a PZK-PCPP for the claim
    % FIXME: There *has* to be a better way to do this
    \begin{algomathdisplay}
      \sum_{\substack{z \in H^{m_{1}} \\ b_{1}, b_{2}, b_{3} \in H^{m_{2}}}}
      \sum_{a_{1}, a_{2}, a_{3} \in \{0, 1\}}\sum_{c_{1}, c_{2}, c_{3} \in H^{k}}
      L_{H_{m_{1}+3m_{2}} \times \{0, 1\}^{3},(z,b,a)}(\tau)h_{\hat{C}}(\tau, c_{1}, c_{2}, c_{3}) = 0
    \end{algomathdisplay}
  }
  \KwRet{$(\pi_{C}, (\pi_{\tau})_{\tau \in \mathbb{F}^{m_{1}+3m_{2}+3}})$}\;
  \tcc{Verifier}
  \caption{A $\PZKPCP$ for $\OSAT$~\cite[Construction 6.4]{GOS25}}\label{alg:pcp-osat}
\end{algorithm}

\begin{algorithm}[htbp]
  % TODO
  \KwIn{A query $\alpha$ to an oracle $O$, either $\pi_{C}$ or $\pi_{\tau}$}
  \KwOut{The result of the query}
  \eIf{The query request is to $\pi_{C}$}{
    % TODO
  }{
    \If{This is the first query to $\pi_{\tau}$}{
      % TODO
    }
    % TODO
  }
  \caption{A simulator for \cref{alg:pcp-osat}~\cite[Construction 6.7]{GOS25}}\label{alg:pcp-osat-sim}
\end{algorithm}

\begin{proof}
  We construct a PZK-PCP for $\OSAT$, a $\NEXP$-complete language, in
  \cref{alg:pcp-osat}.
  % TODO
\end{proof}

\subsection{Reducing query complexity to constant}\label{sec:constant-pcp-np}

We are now finally able to present our analogues to the classical $\PCP$
theorem.

\begin{thm}[{\cite[Theorem 2]{GOS25}}]\label{thm:zk-pcp}\index{PCP theorem@$\PCP$ theorem!zero-knowledge}
  $\NP \subseteq \PZKPCP[\log(n), 1]$.
\end{thm}

\begin{proof}
  To show this, we will take the PCP for $\NP$ that we showed in
  \cref{thm:np-zk-pcp}, which has relatively weak bounds and an arbitrary
  alphabet, and transform it into one with the exact parameters we seek. We do
  this first through an alphabet reduction (as per \cref{thm:alph-reduction})
  and then through proof-composing it with the algorithm for $\CktVal$ with
  \cref{cor:constant-query}, we can get a constant query-complexity PCP.

  Let $q^{*}(n) \le 2^{\poly(n)}$ be arbitrary. This will be the query complexity
  of our final PCP after we do all the class inclusions. As per
  \cref{thm:np-zk-pcp}, we know that
  \begin{equation}\label{eqn:np-subseteq-pzkpcp}
    \NP \subseteq
    \PZKPCP_{\Sigma(n)}\pzkpcpr{\log(n)+\log(\tilde{q}(n))}{\poly(\log(n) + \log(\tilde{q}(n)))}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)}.
  \end{equation}
  for any $\tilde{q} \le 2^{\poly(n)}$ and where
  $\abs{\Sigma(n)} \in \poly(n, \tilde{q})$. \Cref{thm:np-zk-pcp} only guarantees this
  inclusion for alphabets of $\{0, 1\}^{a}$, however a PCP over $\Sigma(n)$ is
  equivalent to a PCP over $\{0, 1\}^{\log_{2}(\abs{\Sigma(n)})}$ by a simple
  relabeling of alphabet items, so this inclusion still holds.

  Next, we perform an alphabet reduction: by \cref{thm:zk-alph-red},
  % TODO: Highlight changes?
  \begin{equation}\label{eqn:alph-red-np}
    \begin{aligned}
      \PZKPCP_{\Sigma(n)}&\pzkpcpr{\log(n)+\log(\tilde{q}(n))}{\poly(\log(n) + \log(\tilde{q}(n)))}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)} \\
      \subseteq \PZKPCP_{\{0,1\}}&\pzkpcpr{\log(n)+\log(\tilde{q}(n))}{\poly(\log(n) + \log(\tilde{q}(n)))}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)}.
    \end{aligned}
  \end{equation}

  Next, we perform proof composition. For any language $L \in \NP$, let
  $Q(n) \in \poly(\log(n) + \log(\tilde{q}))$ be the query complexity of the
  PZK-PCP within the parameters of the right-hand side of \cref{eqn:alph-red-np}
  that recognizes $L$. Since $\tilde{q}$ was arbitrary, we can define it to be
  whatever we like; hence let $\tilde{q}(n)$ be a polynomial in $q^{*}$ and $n$
  large enough that for all $n \in \mathbb{N}$, $\tilde{q}(n)/Q(n) \ge q^{*}(n)$. By
  \cref{cor:constant-query}, we have that
  \begin{equation}\label{eqn:proof-comb-np}
    \begin{aligned}
      \PZKPCP_{\{0,1\}}&\pzkpcpr{\log(n)+\log(\tilde{q}(n))}{\poly(\log(n) + \log(\tilde{q}(n)))}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)} \\
      \subseteq \PZKPCP_{\{0,1\}}&\pzkpcp{\log(n)+\log(q^{*}(n))}{1}{q^{*}(n)}{\varepsilon}.
    \end{aligned}
  \end{equation}

  At the beginning of the proof, we said $q^{*}$ was arbitrary; hence we can set
  $q^{*} \in O(1)$. Thus, we get
  \begin{equation}\label{eqn:qstar-arbitrary-np}
      \PZKPCP_{\{0,1\}}\pzkpcp{\log(n)+\log(q^{*}(n))}{1}{q^{*}(n)}{\varepsilon} \subseteq \PZKPCP[\log(n), 1].
  \end{equation}
  By combining the inclusions in
  \crefrange{eqn:np-subseteq-pzkpcp}{eqn:qstar-arbitrary-np}, we get that
  $\NP \subseteq \PZKPCP[\log(n), 1]$, as desired.
\end{proof}

\begin{thm}[{\cite[Theorem 7.1]{GOS25}}]\label{thm:zk-pcp-nexp}
  $\NEXP \subseteq \PZKPCP[\poly(n), 1]$.
\end{thm}

\begin{proof}
  Broadly speaking, this proof will proceed in the same style as the proof for
  \cref{thm:zk-pcp}. % TODO: More intro

  Let $q^{*} \le 2^{\poly(n)}$ be arbitrary. This will be the query complexity of
  our final PCP after we do all the class inclusions. As per
  \cref{thm:nexp-zk-pcp}, we know that
  \begin{equation}\label{eqn:nexp-subseteq-pzkpcp}
    \NEXP \subseteq \PZKPCP_{\Sigma(n)}\pzkpcpr{\poly(n)+\log(\tilde{q}(n))}{\poly(n)}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)}.
  \end{equation}
  for any $\tilde{q} \le 2^{\poly(n)}$ and where
  $\abs{\Sigma(n)} \in \poly(n, \tilde{q})$. As before, \cref{thm:np-zk-pcp} only
  guarantees this inclusion for alphabets of $\{0, 1\}^{a}$, however a PCP over
  $\Sigma(n)$ is equivalent to a PCP over $\{0, 1\}^{\log_{2}(\abs{\Sigma(n)})}$ by a
  simple relabeling of alphabet items, so this inclusion still holds.

  Next, we perform an alphabet reduction: by \cref{thm:zk-alph-red},
  % TODO: Highlight changes?
  \begin{equation}\label{eqn:alph-red-nexp}
    \begin{aligned}
      \PZKPCP_{\Sigma(n)}&\pzkpcpr{\poly(n)+\log(\tilde{q}(n))}{\poly(n)}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)} \\
      \subseteq \PZKPCP_{\{0,1\}}&\pzkpcpr{\poly(n)+\log(\tilde{q}(n))}{\poly(n,\log(\tilde{q}(n)))}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)}.
    \end{aligned}
  \end{equation}

  Next, we perform proof composition. For any language $L \in \NP$, let
  $Q(n) \in \poly(n)$ be the query complexity of the PZK-PCP within the parameters
  of the right-hand side of \cref{eqn:alph-red-np} that recognizes $L$. Since
  $\tilde{q}$ was arbitrary, we can define it to be whatever we like; hence let
  $\tilde{q}(n) = q^{*}(n) \cdot Q(n)$ for all $n \in \mathbb{N}$. By
  \cref{cor:constant-query}, we have that
  \begin{equation}\label{eqn:proof-comb-nexp}
    \begin{aligned}
      \PZKPCP_{\{0,1\}}&\pzkpcpr{\poly(n)}{\poly(n, \log(\tilde{q}(n)))}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)} \\
      \subseteq \PZKPCP_{\{0,1\}}&\pzkpcp{\poly(n) + \log(q^{*}(n))}{1}{q^{*}(n)}{\varepsilon}.
    \end{aligned}
  \end{equation}

  At the beginning of the proof, we said $q^{*}$ was arbitrary; hence we can set
  $q^{*} \in O(1)$. Thus, we get
  \begin{equation}\label{eqn:qstar-arbitrary-nexp}
      \PZKPCP_{\{0,1\}}\pzkpcp{\poly(n)+\log(q^{*}(n))}{1}{q^{*}(n)}{\varepsilon} \subseteq \PZKPCP[\poly(n), 1].
  \end{equation}
  By combining the inclusions in
  \crefrange{eqn:nexp-subseteq-pzkpcp}{eqn:qstar-arbitrary-nexp}, we get that
  $\NEXP \subseteq \PZKPCP[\poly(n), 1]$, as desired.
\end{proof}

\begin{appendices}

\chapter{More on extension polynomials}\label[appendix]{app:ext-poly}

In this appendix, we will work through some of the algebra we mentioned but did
not go into detail about in \cref{sec:polynomial}.

\section{A proof of \cref{eqn:delta-is-delta}}\label{sec:delta-is-delta}

% TODO: Restate preliminaries
Our goal is to demonstrate the following:
\begin{equation}\label{eqn:delta-is-iverson}
  [x = y] = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
    \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
  \mright)\mright)
\end{equation}
for all $x, y \in H^{n}$. We will do this in two cases: one where $x = y$ and one
where $x \ne y$.

First, assume $x \ne y$ (so we want to show $\delta_{y}(x) = 0$). In this case, there
exists at least one $i$ where $x_{i} \ne y_{i}$. For this $i$, for each $\omega$ there
exists some $\gamma \in H \setminus \{\omega\}$ such that either $x_{i} = \gamma$ or
$y_{i} = \gamma$.\footnote{This piece fails in the case where $x_{i} = y_{i}$, since
  if $\omega = x_{i} = y_{i}$ neither of the terms will ever be zero.} As
such, it follows that either $(x_{i} - \gamma) = 0$ or $(\gamma_{i} - \gamma) = 0$. Hence, for
this $i$ the sum will be entirely over zero terms (since there will be at least
one zero term in the product for each $\omega$). As such, this means that the $i$th
term of our outermost product is $0$, and hence the entire product is $0$, as
desired.

When $x = y$ (and so we want to show $\delta_{y}(x) = 1$), the above equation
simplifies to
\begin{equation}
  [x = y] = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
    \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)^{2}}{(\omega - \gamma)^{2}}
  \mright)\mright)
\end{equation}
Whenever $\omega \ne x_{i}$, the innermost product becomes $0$ since there will be a
term where $\gamma = x_{i}$. Hence, we can simplify this further to
\begin{equation}
  [x = y] = \prod_{i=1}^{m}\mleft(\prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)^{2}}{(x_{i} - \gamma)^{2}}\mright).
\end{equation}
Since $\gamma \ne x_{i}$, we can simplify the fraction to $1$; since we have two nested
products it follows that the equation as a whole simplifies to $1$.

\section{Algebra behind \cref{eqn:delta-poly-small}}\label{sec:delta-poly-small}

Our goal is to show that the equation in \cref{eqn:delta-poly} simplifies to
that of \cref{eqn:delta-poly-small} when $H = \{0, 1\}^{n}$.
% TODO

As a refresher, our starting equation has the form
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
  \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
  \mright)\mright).
\end{equation}
We start by manually substituting the outer sum:
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft(\mleft(
    \prod_{\gamma \in \{0, 1\} \setminus \{0\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
    \mright) + \mleft(
    \prod_{\gamma \in \{0, 1\} \setminus \{1\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
    \mright)
  \mright).
\end{equation}
Next, notice that the inner products are actually each over one term, so we can
manually substitute there:
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft(
    \frac{(x_{i} - 1)(y_{i} - 1)}{(0 - 1)^{2}} +
    \frac{(x_{i} - 0)(y_{i} - 0)}{(1 - 0)^{2}}
  \mright).
\end{equation}
Next, we simplify, taking note that the denominator of both fractions is $1$:
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft((x_{i} - 1)(y_{i} - 1) + x_{i}y_{i}\mright).
\end{equation}
From here, we take advantage of the fact that $y \in \{0, 1\}^{n}$; here we split
our product into two smaller products: one where $y_{i} = 0$ and one where
$y_{i} = 1$.
\begin{equation}
  \delta_{y}(x) = \mleft(\prod_{i:y_{i}=0}(x_{i} - 1)(0 - 1) + 0x_{i}\mright)
  \mleft(\prod_{i:y_{i}=1}(x_{i} - 1)(1 - 1) + x_{i}1\mright).
\end{equation}
Finally, we simplify, bringing us to \cref{eqn:delta-poly-small}.
\begin{equation}
  \delta_{y}(x) = \mleft(\prod_{i:y_{i}=0}(1 - x_{i})\mright)\mleft(\prod_{i:y_{i}=1}x_{i}\mright).
\end{equation}

\chapter{More on \cref{lem:multilinear-is-pspace}}\label[appendix]{app:bug-in-pspace}

\begin{defn}\label{def:alternating-tm}\index{Turing machine!alternating}
  An \emph{alternating Turing machine} is % TODO
\end{defn}

\begin{proof}[{Proof of \cref{lem:multilinear-is-pspace} as written in~\cite{BFL90}}]
  Let $L$ be a $\PSPACE$-robust language. Let $g_{n}(x_{1}, \ldots, x_{n})$ be the
  multilinear extension of the characteristic function of
  $L_{n} = L \cap \{0, 1\}^{n}$. Clearly, $L \in \P^{g}$, where
  $g = \{g_{n} \mid n \ge 0\}$. We will describe an alternating polynomial-time
  Turing machine with access to $L$ computing $g$. First guess the value
  $z = g_{n}(x_{1}, \ldots, x_{n})$. Then existentially guess the linear function
  $h_{1}(y) = g(y, x_{2}, \ldots, x_{n})$ and verify that $h_{1}(x_{1}) = z$. Then
  universally choose $t_{1} \in \{0, 1\}$ and existentially guess the linear
  function $h_{2}(y) = g(t_{1}, y, x_{3}, \ldots, x_{n})$. Keep repeating this
  process until we have specified $t_{1}, \ldots, t_{n}$ and then verify
  $t_{1}, \ldots, t_{n} \in L$. Since a $\PSPACE$ machine can simulate an alternating
  polynomial-time Turing machine, if $L$ is $\PSPACE$-robust then $g$ is
  Turing-reducible to $L$.
\end{proof}

% \begin{thm}[{\cite[Lemma 6.2]{BFL90}}]\label{thm:multilinear-robust}
%   Every $\PSPACE$-robust language has a Turing-equivalent family of multilinear
%   functions over the integers.
% \end{thm}

% \begin{proof}
%   % TODO
%   We use the following algorithm:

%   \begin{algorithm}[htbp]
%     \KwIn{$x_{1}, \ldots, x_{n} \in \mathbb{F}$} \KwOut{$g_{n}(x_{1}, \ldots, x_{n})$}
%     Existentially guess some
%     $z \in \mathbb{F}$\tcc*{$z = g_{n}(x_{1}, \ldots, x_{n})$}\nllabel{line:def-z}
%     Existentially guess some linear
%     $h_{1}(y)$\tcc*{$h_{1}(y) = g_{n}(y, x_{2}, \ldots, x_{n})$}\nllabel{line:def-h1}
%     Verify $h_{1}(x_{1}) = z$\;
%     \For{$i$ from $2$ to $n$}{
%       Existentially guess some linear
%       $h_{i}(y)$\tcc*{$h_{i}(y) = g_{n}(t_{1}, \ldots, t_{i-1}, y, x_{i+1}, \ldots, y_{n})$}\nllabel{line:def-hn}
%       Verify $h_{i-1}(t_{i-1}) = h_{i}(x_{i})$\;\nllabel{line:verify-hi}
%       Universally choose $t_{i} \in \{0, 1\}$\;
%     }
%     Verify $h_{n}(t_{n}) = [(t_{1}, \ldots, t_{n}) \in L]$\;
%     \Return $z$\;
%     \caption{An algorithm to compute $g_{n}$}\label{alg:compute-gn}
%   \end{algorithm}

%   % TODO: Put info about alternating TMs in thesis?
%   First, we show that this is a polynomial-time alternating Turing machine. Any
%   guess can be made in polynomial time, as can calculating the value of a linear
%   function. We do this a polynomial number of times (specifically linear), so
%   that is in polynomial time. Lastly, calculating whether
%   $(t_{1}, \ldots, t_{n}) \in L$ is in $\PSPACE$, so by a theorem of Chandra, Kozen,
%   and Stockmeyer~\cite[Corollary 3.6]{CKS81}, we can simulate it in polynomial
%   time. Again by that theorem, the fact that \cref{alg:compute-gn} is a
%   polynomial time alternating Turing machine means that the problem it computes
%   is in $\PSPACE$.

%   Next, we need to show that \cref{alg:compute-gn} actually computes the
%   function it purports to. We have put in the comments some equations next to
%   each line in which we existentially guess a value: we shall start by
%   demonstrating that these equations hold for exactly the choices that lead to
%   an accepting configuration.

%   First, we show that the equations in line~\ref{line:def-h1}
%   and~\ref{line:def-hn} hold. First, note that line~\ref{line:def-h1} is just
%   the $i=1$ case of line~\ref{line:def-hn}; hence proving the general case will
%   give us both equations. If we assume
%   \begin{equation}\label{eqn:hi-is-restriction}
%     h_{i}(y) = g_{n}(t_{1}, \ldots, t_{i-1}, y, x_{i+1}, \ldots, y_{n})
%   \end{equation}
%   for all $n$, then the expressions on each side of the equality in the
%   verification on line~\ref{line:verify-hi} will simplify to
%   \begin{equation}
%     g_{n}(t_{1}, \ldots, t_{i-1}, x_{i}, \ldots, x_{n}).
%   \end{equation}
%   Since both sides simplify to the same equation, it follows that they are
%   equal. For the converse, if we assume
%   \begin{equation}
%     h_{i}(y) \ne g_{n}(t_{1}, \ldots, t_{i-1}, y, x_{i+1}, \ldots, y_{n}),
%   \end{equation}
%   for at least one $i$, then either $h_{i-1}(t_{i-1})$ will not be equal to
%   $h_{i}(x_{i})$, in which case line~\ref{line:verify-hi} will fail, or that
%   check will succeed, in which case things get more complex. % TODO

%   Next, we show that the equation in line~\ref{line:def-z} holds. Formatted
%   mathematically, the statement we are making here is that of
%   \cref{lem:multilinear-forces-unique}. Since we have already proved that that
%   statement holds, it follows that line~\ref{line:def-z} does as well. Further,
%   since the equation in line~\ref{line:def-z} is the same as the goal of
%   \cref{alg:compute-gn}, it follows that the algorithm computes the value of
%   $g_{n}$ for arbitrary inputs.
% \end{proof}
\end{appendices}

\printbibliography[heading=bibintoc]{}

\printindex{}

\end{document}
