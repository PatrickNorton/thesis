\documentclass{reedthesis}
\usepackage{amsfonts, amscd, amssymb, amsthm, amsmath}
\usepackage{mathtools} %xmapsto etc
\usepackage{pdfsync} %leaves makers for tex searching
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{complexity}
\usepackage{mleftright}

\usepackage[linesnumbered]{algorithm2e}
\usepackage{biblatex}
\usepackage{imakeidx}
\usepackage{microtype}
\usepackage{tikz}

\usepackage[colorlinks]{hyperref}

%%% Theorems %%%---------------------------------------------------------
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\theoremstyle{definition}
\newtheorem*{def*}{Definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{topic}[thm]{Topic}
\theoremstyle{remark}
\newtheorem{example}{Example}[thm]
\newtheorem{remark}[thm]{Remark}
\newtheorem{subrem}[example]{Remark}


\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dimn}{dim}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\ev}{ev}
\def\f{\varphi}
\def\half{\hbox{$\frac12$}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\img}{img}
\DeclareMathOperator{\Inn}{Inn}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\mdeg}{mdeg}
\DeclareMathOperator{\rad}{rad}
\DeclareMathOperator{\Rep}{Rep}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\rk}{rank}
\def\normeq{\trianglelefteq}
\DeclareMathOperator{\nul}{nullity}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Syl}{Syl}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\tr}{tr}
\def\vep{\varepsilon}
\DeclareMathOperator{\lcm}{lcm}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\ang}{\langle}{\rangle}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\middlemid}{%
  \ensuremath{\;\middle\vert\;}
}

\newcommand{\dblang}[1]{%
  \ensuremath{\left\langle\!\left\langle#1\right\rangle\!\right\rangle}
}

\newcommand{\comment}[1]{%
  \text{\phantom{(#1)}} \tag{#1}%
}

\newcommand{\commath}[1]{%
  \phantom{(#1)} \tag{#1}%
}


\addbibresource{bibliography.bib}

\numberwithin{equation}{section}

\makeindex

\title{Thesis Draft: Algebrization}
\author{Patrick Norton}

\approvedforthe{Committee}
\thedivisionof{The Established Interdisciplinary Committee for \\}
\division{Mathematics and Computer Science}
\department{Mathematics and Computer Science}
\advisor{Zajj Daugherty}
\altadvisor{Adam Groce}

\begin{document}

\maketitle

\tableofcontents

% TODO: Abstract

\chapter*{Introduction}

The $\P$ vs $\NP$ problem is perhaps the most important open problem in
complexity theory.

\chapter{Preliminaries}

% TODO: Do we need Hamming distance? I can't find them actually using it in CFGS22

\section{Turing machines}

% TODO: Do we even need to formally define a TM?

Central to our definitions of complexity is that of a Turing machine. This is
the most common mathematical model of a computer, and is the jumping-off point
for mant variants. There are many ways to think of a Turing machine, but the
most common is that of a small machine that can read and write to an
arbitrarily-long ``tape'' according to some finite set of rules. We give a more
formal definition below, and then we will attempt to take this definition into a
more manageable form.
\begin{defn}[{\cite[Def.\ 3.1]{Sip97}}]\label{def:TM}\index{Turing machine}
  A \emph{Turing machine} is a 7-tuple $(Q, \Sigma, \Gamma, \delta, q_{0}, q_{a}, q_{r})$ where
  $Q$, $\Sigma$, and $\Gamma$ are all finite sets and
  % TODO: Rephrase to separate out terminology and definitions?
  \begin{enumerate}
    \item $Q$ is the set of \emph{states},
    \item $\Sigma$ is the \emph{input alphabet},
    \item $\Gamma$ is the \emph{tape alphabet},
    \item $\delta: Q \times \Gamma \rightarrow Q \times \Gamma \times \{L, R\}$ is the \emph{transition function},
    \item $q_{0} \in Q$ is the \emph{start state},
    \item $q_{a} \in Q$ is the \emph{accept state},
    \item $q_{r} \in Q$ is the \emph{reject state}, with $q_{a} \ne q_{r}$.
  \end{enumerate}
\end{defn}

While we have this formalism here as a useful reference, even here we will most
frequently refer to Turing machines in a more intuitionisitc form. There are
several ways we will think about Turing machines.

The first way to think about a Turing machine is as a little computing box with
a tape. We let the box read and write to the tape, and each step it can move the
tape one space in either direction. At some point, the machine can decide it is
done, in which case we say it ``halts''; however it does not necessarily need to
halt. For this paper, we will only think about machines that \emph{do} halt, and
in particular we will care about how many it takes us to get there. Further, we
will use this informalism as a base from which we can define our Turing machine
variants intuitively, without needing to deal with the (potentially extremely
convoluted) formalism.

% TODO: Cite Church-Turing
Another way we think about a Turing machine is as an algorithm. Perhaps the
foundational paper of modern computer science theory, the \emph{Church-Turing
  thesis}, states that any actually-computable algorithm has an equivalent
Turing machine, and vice versa. We will use this fact liberally; in many cases
we will simply describe an algorithm and not deal with putting it into the
context of a Turing machine. If we have explained the algorithm well enough that
a reader can execute it (as we endeavor to do), then we know a Turing machine
must exist.
% TODO: Is any of this really necessary?

% TODO: Nondeterministic TM

\section{Complexity classes}

% TODO: Do I need to define big-O for this?

Complexity classes are the main way we think about the hardness of problems in
computer science. A complexity class\index{complexity class} is a collection of
languages that all share a common level of difficulty.

\subsection{Time complexity}

The most intuitive (and most important) notion of complexity is that of time
complexity. Time complexity is the answer of the question of how long it takes
to solve a problem. We begin with an abstract base for our time classes, and
will then introduce some specific ones that we care about.

\begin{defn}[{\cite[Def.\ 1.19]{AB09}}]\label{def:dtime}\index{DTIME@$\DTIME$}
  % FIXME: Reword to make more clear what $n$ is
  Let $f: \mathbb{N} \rightarrow \mathbb{N}$ be a function. The class $\DTIME(f(n))$ is the class of all
  problems computable by a deterministic Turing machine in $O(f(n))$ steps for
  some constant $c > 0$.
\end{defn}

While $\DTIME$ is a useful base to start from, it is rare that we deal with
$\DTIME$ classes directly. % TODO

\begin{defn}[{\cite[Def.\ 1.20]{AB09}}]\label{def:p}\index{P@$\P$}
  The complexity class $\P$ is the class
  \[
    \P = \bigcup_{c > 0}\DTIME(n^{c}).
  \]
\end{defn}

The class $\P$ is perhaps the most important complexity class. Mathematically,
we care about $\P$ because it is closed under composition: a polynomial-time
algorithm iterated a polynomial number of times is still in $\P$. Further, $\P$
turns out to generally be invariant under change of (deterministic) computation
model, which allows us to reason about $\P$ problems easily without needing to
resort to the formal definition of a Turing machine. More philosophically, $\P$
generally represents the set of ``efficient'' algorithms in the real world.

As we have defined $\P$ in terms of $\DTIME$, the question naturally arises of
whether there is an equivalent in terms of $\NTIME$. Naturally, there is, and we
call it $\NP$.

\begin{defn}[{\cite[Cor.\ 7.22]{Sip97}}]\label{def:np}\index{NP@$\NP$}
  The complexity class $\NP$ is the class
  \[
    \NP = \bigcup_{c > 0}\NTIME(n^{c}).
  \]
\end{defn}

While this definition demonstrates how $\NP$ is similar to $\P$, there are other
equivalent ones that we can use. In particular, we very often like to think of
$\NP$ in terms of deterministic \emph{verifiers}. Since nondeterministic
machines do not exist in real life, this definition gives a practical meaning to
$\NP$.

\begin{thm}[{\cite[Def.\ 7.19]{Sip97}}]\label{thm:np-verifier}
  $\NP$ is exactly the class of all languages verifiable by a $\P$-time Turing
  machine.
\end{thm}

% TODO: Should I prove this?

\subsection{Space complexity}

In addition to time complexity, the an additional notion of complexity is that
of space complexity. Space complexity is the question of how much space on its
memory tape a machine needs in order to compute a problem. In many ways, our
definitions of space complexity are analagous to those for time complexity that
we have already defined. In particular, $\DSPACE$ will correspond nicely to
$\DTIME$, and $\NSPACE$ to $\NTIME$.

\begin{defn}[{\cite[Def.\ 4.1]{AB09}}]\label{def:dspace}\index{DSPACE@$\DSPACE$}
  Let $f: \mathbb{N} \rightarrow \mathbb{N}$ be a function. A language $L$ is in $\DSPACE(f(n))$ if there
  exists a deterministic Turing machine $M$ such that the number of locations on
  the tape that are non-blank at some point during the execution of $M$ is
  in $O(f(n))$.
\end{defn}

In the same way as we have defined $\DSPACE$ for deterministic machines, we now
need to define $\NSPACE$ for nondeterministic machines.

\begin{defn}[{\cite[Def. 4.1]{AB09}}]\label{def:nspace}\index{NSPACE@$\NSPACE$}
  Let $f: \mathbb{N} \rightarrow \mathbb{N}$ be a function. A language $L$ is in $\NSPACE(f(n))$ if there
  exists a nondeterministic Turing machine $M$ such that the number of locations
  on the tape that are non-blank at some point during the execution of $M$ is in
  $O(f(n))$.
\end{defn}

Analagously to $\P$ and $\NP$, our two main classes of space complexity are
$\PSPACE$ and $\NPSPACE$.

\begin{defn}[{\cite[Def.\ 4.5]{AB09}}]\label{def:pspace}\index{PSPACE@$\PSPACE$}
  The complexity class $\PSPACE$ is the class
  \[
    \PSPACE = \bigcup_{c > 0}\DSPACE(n^{c}).
  \]
\end{defn}

\begin{defn}[{\cite[Def.\ 4.5]{AB09}}]\label{def:npspace}\index{NPSPACE@$\NPSPACE$}
  The complexity class $\NPSPACE$ is the class
  \[
    \NPSPACE = \bigcup_{c > 0}\NSPACE(n^{c}).
  \]
\end{defn}

Unlike with $\P$ and $\NP$, the relationship between $\PSPACE$ and $\NPSPACE$ is
well known. Due to the complexity of the proof of the theorem, we will not prove
it here, as it is mostly not relevant to what we will be doing.

\begin{thm}[{Savitch's theorem;~\cite{Sav70}}]\label{thm:savitch}\index{Savitch's theorem}
  $\PSPACE = \NPSPACE$.
\end{thm}

Upon seeing this, one might ask why it is that we believe $\P \ne \NP$ if we know
that $\PSPACE = \NPSPACE$, given they are defined analogously. The answer to
this question boils down to the fact that we are able to reuse space, while we
are not able to reuse time. Space on the tape that is no longer needed can be
overwritten, while time that is no longer needed is gone forever.

Since $\PSPACE$ and $\NPSPACE$ are equal classes, it is relatively rare to see
$\NPSPACE$ referred to. Here, we will only refer to it when it makes a class
relationship clearer; most frequently when comparing $\NPSPACE$ to some other
nondeterministic class.

\subsection{Completeness}

% TODO: Not a huge fan of this paragraph
Even within a complexity class, not all problems are created equal. The notion
of \emph{completeness} gives us a mathematically-rigorous way to talk about
which problems in a class are the hardest. Since putting upper bounds on hard
problems naturally puts those same bounds on any easier problems, complete
problems can be useful in reasoning about the relationship between complexity
classes.

\begin{defn}[{\cite[Def.\ 7.29]{Sip97}}]\label{def:p-reduction}\index{Polynomial-time reduction}
  A language $A$ is \emph{polynomial-time reducible} to a language $B$ if a
  polynomial-time computable function $f: \Sigma^{*} \rightarrow \Sigma^{*}$ exists such that for
  all $w \in \Sigma^{*}$, $w \in A$ if and only if $f(w) \in B$.
\end{defn}

Polynomial-time reductions are important because they give us a way to say that
$A$ is \emph{no harder} than $B$. In particular, if we have an algorithm $M$
that determines $B$, we can construct the following algorithm that determines
$A$ with only a polynomial amount of additional work:

\begin{algorithm}[H]
  \KwIn{A string $w \in \Sigma^{*}$}
  \KwOut{Whether $w \in A$}
  Compute $f(w)$\;
  Use $M$ to check whether $f(w) \in B$\;
  \KwRet{the result of $M$}\;
  \caption{An algorithm to reduce $A$ to $B$}
\end{algorithm}

\begin{defn}[{\cite[Def.\ 7.34]{Sip97}}]\label{def:np-complete}\index{NP-complete@$\NP$-complete}
  A language $L$ is $\NP$-complete if $L \in \NP$ and every $A \in \NP$ is
  polynomial-time reducible to $L$.
\end{defn}

This is a practical use of our polynomial-time reductions: since an
$\NP$-complete language has a reduction from every other language in $\NP$, it
follows that it is \emph{at least as hard} as any other language in $\NP$. Of
particular interest to complexity theorists is the fact that $\P = \NP$ if and
only if \emph{any} $\NP$-complete language is in $\P$.

Just as we have $\NP$-completeness for time complexity, we also have notions of
completeness for space complexity. Since $\PSPACE = \NPSPACE$, instead of
calling the class $\NPSPACE$-complete, we call it $\PSPACE$-complete.

\begin{defn}[{\cite[Def.\ 8.8]{Sip97}}]\label{def:pspace-complete}\index{PSPACE-complete@$\PSPACE$-complete}
  A language $L$ is $\PSPACE$-complete if $L \in \PSPACE$ and every $A \in \PSPACE$
  is polynomial-time reducible to $\NP$.
\end{defn}

While this definition is mostly analagous to that of $\NP$-completeness, one
might wonder why we use a time complexity for our reduction when $\PSPACE$ is a
space-complexity class. This is because if we were to use space complexity, we
would want to use $\PSPACE$-reductions, but that would make every language in
$\PSPACE$ trivially $\PSPACE$-complete. Since that is not a useful definition,
we instead restrict ourselves to polynomial-time reductions.

% TODO: If/when we pull random/quantum stuff in here, probably worth defining
% BPP and BQP (QMA might need its own section?)

\section{Polynomials}

Much of our work will deal with multivariate polynomials. For a given field
$\mathbb{F}$, we will denote the set of $m$-variable polynomials over
$\mathbb{F}$ with $\mathbb{F}[x_{1, \ldots, m}]$.

\begin{defn}[{\cite[8]{AW09}}]\label{def:mdeg}\index{multidegree}
  The \emph{multidegree} of a multivariate polynomial $p$, written $\mdeg(d)$,
  is the maximum degree of any variable $x_{i}$ of $p$.
\end{defn}

It is worth noting that for monovariate polynomials, multidegree and degree
coincide. The difference between multidegree and degree is subtle, but
important. We shall illustrate the difference with a simple example.

\begin{example}
  Consider the polynomial $x_{1}^{2}x_{2} + x_{2}^{2}$. The multidegree of this
  polynomial is 2, while its degree is 3.
\end{example}

We denote by $\mathbb{F}[x_{1, \ldots, m}^{\le d}]$ the subset of
$\mathbb{F}[x_{1, \ldots, m}]$ of polynomials with multidegree at most $d$. We also
need two special cases of these polynomials, which we will want to quickly be
able to reference throughout the paper.

\begin{defn}[{\cite[8]{AW09}}]\label{def:mlin}\index{multilinear}\index{multiquadratic}
  A polynomial is \emph{multilinear} if it has multidegree at most 1. Similarly,
  a polynomial is \emph{multiquadratic} if it has multidegree at most 2.
\end{defn}

From here, we need to define the notion of an \emph{extension polynomial}. This
gives the ability to take an arbitrary multivariate function defined on a subset
of a field and extend it to be a multivariate polynomial over the \emph{whole}
field.

\begin{defn}[{\cite[8]{AW09}}]\label{def:ext-poly}\index{extension polynomial}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f: H^{m} \rightarrow \mathbb{F}$ be a function. An \emph{extension polynomial} of $f$
  is any polynomial $f' \in \mathbb{F}[x_{1, \ldots, m}]$ such that $f(h) = f'(h)$ for
  all $h \in H$.
\end{defn}

It turns out that this polynomial needs only to be of a surprisingly low
multidegree. Since polynomials of lower degree are generally easier to compute,
we would like to have some measure of what a ``small'' polynomial actually is in
this context.

% TODO: Rewrite \hat{f} as \tilde{f} to agree with AW09
\begin{defn}[{\cite[\defaultS 5.1]{CFGS22}}]\label{def:low-deg-ext}\index{low-degree extension}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f: H^{m} \rightarrow \mathbb{F}$ be a function. A \emph{low-degree extension} $\hat{f}$
  of $f$ is an extension of $f$ with multidegree at most $\abs{H} - 1$.
\end{defn}

% TODO: Cite these statements
It turns out that this is the minimum possible degree of any extension
polynomial. Further, it turns out that for any $f$, there is a \emph{unique}
low-degree extension. Neither of these statements are particularly important for
our further work, so we will not endeavor to prove them here. Something of
practical use to us is an explicit formula for the low-degree extension, which
we shall now calculate.

\begin{thm}[{\cite[\defaultS 5.1]{CFGS22}}]\label{thm:low-deg-ext-exists}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f: H^{m} \rightarrow \mathbb{F}$. Then a low-degree extension $\hat{f}$ of $f$ is the
  function
  \begin{equation}
    \hat{f}(x) = \sum_{\beta \in H^{m}}\delta_{\beta}(x)f(\beta),
  \end{equation}
  where $\delta$ is the polynomial
  % TODO? Flip x and y here
  \begin{equation}\label{eqn:delta-poly}
    \delta_{x}(y) = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
        \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
      \mright)\mright).
  \end{equation}
\end{thm}

\begin{proof}
  First, we must show $\hat{f}$ has multidegree $\abs{H} - 1$. First, note that
  $\hat{f}$ is a linear combination of some $\delta_{x}$es; hence asking about the
  multidegree of $\hat{f}$ is really just asking about the multidegree of
  $\delta_{x}$. Looking at $\delta_{x}$, the innermost product has $\abs{H} - 1$ terms,
  each with the same $y_{i}$; thus those terms have multidegree $\abs{H} - 1$.
  Summing terms preserves their multidegree, and the outer product iterates over
  the variables, thus it preserves multidegree as well. Thus, $\delta_{x}$ has
  multidegree $\abs{H} - 1$.

  % TODO: Turn this into a lemma?
  To understand why $\hat{f}(x)$ agrees with $f(x)$ on $H$, we first should look
  at $\delta_{\beta}(x)$. In particular, for all $x, y \in H^{m}$,
  \begin{equation*}
    \delta_{y}(x) = \begin{cases}
      1 & x = y \\
      0 & x \ne y.
    \end{cases}
  \end{equation*}
  This can be shown through some straightforward but tedious algebra which we
  have omitted here. The equivalence above is the reason we have chosen our
  notation here to be reminiscent of the Kronecker delta function.

  Taking the above statement, we get that for all $x \in H^{m}$, the only nonzero
  term of $\hat{f}(x)$ is the term where $\beta = x$; thus $\hat{f}(x) = f(x)$.
  Hence, $\hat{f}$ is a low-degree extension of $f$.
  % TODO: Does this even need a proof or can we leave it as a claim?
\end{proof}

Of particular interest to us will be the case of low-degree extensions where
$H = \{0, 1\}$. Since every field contains both $0$ and $1$, this will allow us
to construct a set consisting of an extension for \emph{every} field. Further,
since $\abs{H} = 2$ here, it means our low-degree extensions will be
multilinear. Not only do we thus constrain our polynomial to have a very low
multidegree, the $\delta$ function also dramatically simplifies in this case, which
makes it much easier to reason about.

\begin{cor}[{\cite[\defaultS 4.1]{AW09}}]\label{cor:low-degree-boolean}
  Let $\mathbb{F}$ be a finite field, $m \in \mathbb{N}$ a number, and
  $f: \{0, 1\}^{m} \rightarrow \mathbb{F}$. Then
  \begin{equation}
    \hat{f}(x) = \sum_{\beta \in \{0, 1\}^{m}}\delta_{\beta}(x)f(\beta)
  \end{equation}
  is a low-degree extension of $f$, where $\delta$ is the polynomial
  \begin{equation}\label{eqn:delta-poly-small}
    \delta_{x}(y) = \mleft(\prod_{i:x_{i}=1}y_{i}\mright)\mleft(\prod_{i:x_{i}=0}(1 - y_{i})\mright).
  \end{equation}
\end{cor}
Note that in the product bound $i:x_{i} = 1$, we mean the product over all
numbers $i$ such that $x_{i} = 1$.
% TODO: Prove?

As we can see, the form of $\delta$ in Equation~\eqref{eqn:delta-poly-small} is much
more manageable than the form in Equation~\eqref{eqn:delta-poly}, and it is
perhaps more immediately apparent here why $\delta$ has the property it does.

The form of $\delta_{x}$ defined in Equation~\eqref{eqn:delta-poly-small} has further
use to us than just being simpler.

% TODO: Find better statement of this theorem
\begin{thm}[{\cite[\defaultS 4.1]{AW09}}]
  For any field $\mathbb{F}$, the set $\{\delta_{x} \mid x \in \{0, 1\}^{n}\}$ forms a
  basis for the vector space of multilinear polynomials
  $\mathbb{F}^{n} \rightarrow \mathbb{F}$.
\end{thm}

% TODO: I should probably define the Boolean cube somewhere
This is a particularly useful basis because it allows us to reason about
multilinear polynomials in terms of their outcomes on the Boolean cube.

\begin{thm}[{\cite[Theorem 4.3]{AW09}}]
  Let $\mathbb{F}$ be a field and $Y \subseteq \mathbb{F}^{n}$ be a set of $t$ points
  $y_{1}, \ldots, y_{t}$. Then for at least $2^{n} - t$ Boolean points
  $w \in \{0, 1\}^{n}$, there exists a multiquadratic extension polynomial
  $p: \mathbb{F}^{n} \rightarrow \mathbb{F}$ such that
  \begin{enumerate}
    \item $p(y_{i}) = 0$ for all $i \in [t]$,
    \item $p(w) = 1$,
    \item $p(z) = 0$ for all Boolean $z \ne w$.
  \end{enumerate}
\end{thm}

\begin{proof}
  % TODO
  % NOTE: AW09 Lemma 4.2 is *almost* a corollary of our Theorem 1.3.5, except
  % for the fact that not all our points are in \{0, 1\}^n
\end{proof}

% TODO: Do we want this here?
% TODO: Add preceding lemmas
% TODO: Come up with descriptive names for these things (adversary polynomials?)
\begin{lemma}[{\cite[Lemma 4.5]{AW09}}]\label{lem:multiquad-adversary}
  Let $\mathcal{F}$ be a collection of fields. Let $f: \{0, 1\}^{n} \rightarrow \{0, 1\}$ be a
  Boolean function, and for every $\mathbb{F} \in \mathcal{F}$, let
  $p_{\mathbb{F}}: \mathbb{F}^{n} \rightarrow \mathbb{F}$ be a multiquadratic polynomial
  over $\mathbb{F}$ extending $f$. Also let $\mathcal{Y}_{\mathbb{F}} \in \mathbb{F}^{n}$
  for each $\mathbb{F} \in \mathcal{F}$, and define
  $t = \sum_{\mathbb{F} \in \mathcal{F}}\abs{\mathcal{Y}_{\mathbb{F}}}$.

  Then, there exists a subset $B \subseteq \{0, 1\}^{n}$, with $\abs{B} \le t$, such that
  for all Boolean functions $f': \{0, 1\}^{n} \rightarrow \{0, 1\}$ that agree with $f$ on
  $B$, there exist multiquadratic polynomials
  $p_{\mathbb{F}}':\mathbb{F}_{n} \rightarrow \mathbb{F}$ (one for each $\mathbb{F} \in \mathcal{F}$)
  such that
  \begin{enumerate}
    \item $p_{\mathbb{F}}'$ extends $f'$, and
    \item $p_{\mathbb{F}}'(y) = p_{\mathbb{F}}(y)$ for all $y \in \mathcal{Y}_{\mathbb{F}}$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  % TODO
\end{proof}

% NOTE: JKRS09 is actually even stronger than this (we need it only to be linear
% in at least one variable)
\begin{lemma}[{\cite[Lemma 7]{JKRS09}}]\label{lem:monomial-sum}
  Let $m(x_{1}, \ldots, x_{n})$ be a multilinear monomial. Over a field of
  characteristic other than 2, we have
  \begin{equation}
    \sum_{b \in \{-1, 1\}}m(b) = 0.
  \end{equation}
\end{lemma}

\begin{proof}
  For some $x_{i}$, we can write $m = x_{i} \cdot m'$, where the degree of $x_{i}$
  in $m'$ is 0. Then
  \begin{align*}
    \sum_{b \in \{1, -1\}^{n}}m(b)
    &= \sum_{a \in \{-1, 1\}}\sum_{b' \in \{1, -1\}^{n-1}}a \cdot m'(b') \\
    &= \sum_{a \in \{-1, 1\}} a \cdot \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) \\
    &= \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) - \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) \\
    &= 0.
  \end{align*}
\end{proof}

\chapter{Relativization}

% TODO: Examples

An important prerequisite to understanding algebrization is the similar, but
simpler, concept of \emph{relativization}, also called \emph{oracle separation}.
To do this, we first must define an \emph{oracle}.
\begin{defn}[{\cite[Def.\ 2.1]{AW09}}]\label{def:oracle}\index{oracle}
  An \emph{oracle} $A$ is a collection of Boolean functions
  $A_{m}: \{0, 1\}^{m} \rightarrow \{0, 1\}$, one for each natural number $m$.
\end{defn}
There are several ways to think of an oracle; this will extend the most
naturally when it comes time to define an extension oracle in
Definition~\ref{def:ext-oracle}. Another way to think of an oracle is as a
subset $A \subseteq \{0, 1\}^{*}$. This allows us to think of $A$ as a language. Since
we can do this, it gives us the ability to think of the complexity of the
oracle. If we want to think about the subset in terms of our functions, we can
write $A$ as
\begin{equation}
  A = \bigcup_{m \in \mathbb{N}}\mleft\{x \in \{0, 1\}^{m} \mid A_{m}(x) = 1\mright\}.
\end{equation}

An oracle is not particularly interesting mathematical object on its own; its
utility comes from when it interacts with a Turing machine.
\begin{defn}\label{def:tm-oracle}\index{Turing machine!with oracle}
  A \emph{Turing machine with an oracle} is % TODO
\end{defn}

Of course, the question now becomes how we can effectively use an oracle in an
algorithm. The previously-mentioned conception of an oracle as a set of strings
is useful here. If we consider the set of strings as being a \emph{language} in
its own right, then querying the oracle is the same as determining whether a
string is in the langauge, just in one step. If the language is computationally
hard, this means our machine can get a significant power boost from the right
oracle.

\begin{defn}[{\cite[Def.\ 2.1]{AW09}}]\label{def:oracle-class}
  For any complexity class $\mathcal{C}$, the complexity class $\mathcal{C}^{A}$ is the class of all
  languages determinable by a Turing machine with access to $A$ in the number of
  steps defined for $\mathcal{C}$.
\end{defn}

We will be using this definition in many places, so we should take a moment to
look at it in more depth. First, it is important to realize that $\mathcal{C}^{A}$ is a
set of \emph{languages}, not \emph{machines}: despite the notation, augmenting
$\mathcal{C}$ with an oracle does not modify any languages, it just adds new ones that are
computable. Second, since a machine can always ignore its oracle, it follows
that adding an oracle can only increase the number of languages in the class,
never decrease it.

\begin{lemma}\label{thm:relativizing-increases}
  For any complexity class $\mathcal{C}$ and oracle $A$, $\mathcal{C} \subseteq \mathcal{C}^{A}$.
\end{lemma}

\begin{proof}
  % TODO
\end{proof}

\section{Defining relativization}

We are now ready to define what relativization is. First, note that
relativization is a statement about a \emph{result}: we talk about inclusions
algebrizing, not sets themselves.

% TODO: Define what it means to relativize (and why this is a barrier)
% TODO: Cite
\begin{defn}\label{def:relativization}\index{relativization}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \subseteq \mathcal{D}$. We say the result
  $\mathcal{C} \subseteq \mathcal{D}$ \emph{relativizes} if $\mathcal{C}^{A} \subseteq \mathcal{D}^{A}$ for all oracles $A$. Conversely,
  if there exists $A$ such that $\mathcal{C} \nsubseteq \mathcal{D}$, we say that the result $\mathcal{C} \subseteq \mathcal{D}$
  \emph{does not algebrize}.
\end{defn}

\begin{defn}\label{def:relativization-ne}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \nsubseteq \mathcal{D}$. We say the result
  $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{relativizes} if $\mathcal{C}^{A} \nsubseteq \mathcal{D}^{A}$ for all oracles $A$. Conversely,
  if there exists $A$ such that $\mathcal{C} \subseteq \mathcal{D}$, we say that the result $\mathcal{C} \nsubseteq \mathcal{D}$
  \emph{does not algebrize}.
\end{defn}

We start with a very straightforward example of a relativizing result.

\begin{lemma}\label{lem:pa-subset-npa}
  For any oracle $A$, $\P^{A} \subseteq \NP^{A}$. Equivalently, the result $\P \subseteq \NP$
  relativizes.
\end{lemma}

\begin{proof}
  Since any deterministic Turing machine is also a nondeterministic machine, it
  follows that a machine that solves a $\P^{A}$ problem is also an $\NP^{A}$
  machine. Hence, $\P^{A} \subseteq \NP^{A}$.
\end{proof}

This result tells us that not \emph{everything} is weird in the world of
relativization: if we have a machine that can do more operations without an
oracle, it can still do so with an oracle. Further, for the question of $\P$
vs.\ $\NP$ that we will discuss in Section~\ref{sec:rel-p-np}, this means that
the question we care about is whether $\NP \subseteq^{?} \P$ relativizes. As such, the
question we are asking simplifies to determining where $\P^{A} = \NP^{A}$ and
where $\P^{A} \subsetneq \NP^{A}$.

\section{Query complexity}\label{sec:query-complexity}

The goal of query complexity is to ask questions about some Boolean function
$A: \{0, 1\}^{n} \rightarrow \{0, 1\}$ by querying $A$. For this, we will interchangeably
think of $A$ as a \emph{function} as well as a bit string of length $N = 2^{n}$,
where each string element is $A$ applied to the $i$th string of length $n$,
arranged in some lexicographical order. % TODO: Better way to phrase this
We can further think of the property itself as being a Boolean function; a
function that takes as input the bit-string representation of $A$ and outputs
whether or not $A$ has the given property. We will call the function
representing the property $f$. When viewed like this, $f$ is a function from
$\{0, 1\}^{N}$ to $\{0, 1\}$. We define three types of query complexity for
three of the most common types of computing paradigms: deterministic,
randomized, and quantum. Nondeterministic query complexity is interesting, but
it is outside the scope of this paper.
% TODO: Why on earth does this paper not define nondeterministic query complexity?

% TODO: Find better source for these definitions
% Perhaps rephrase in the style of AW09 Def. 4.1?
\begin{defn}[{\cite[17]{AW09}}]\label{def:det-qc}\index{query complexity!deterministic}
  Let $f: \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{deterministic query complexity} of $f$, which we write $D(f)$, is the
  minimum number of queries made by any deterministic algorithm with access to
  an oracle $A$ that determines the value of $f(A)$.
  % TODO: I don't quite understand the phrasing here; perhaps rephrase
\end{defn}

To make this more clear, let us give an example problem.

\begin{defn}\label{def:or-problem}\index{OR@$\mathsf{OR}$}
  The $\mathsf{OR}$ problem is the following oracle problem:
  \begin{quote}
    Let $A: \{0, 1\}^{n} \rightarrow \{0, 1\}$ be an oracle. The function $\mathsf{OR}(A)$
    returns 1 if there exists a string on which $A$ returns 1, and $0$
    otherwise.
  \end{quote}
\end{defn}

The question is then what the deterministic query complexity of the
$\mathsf{OR}$ function is.

\begin{thm}
  The $\mathsf{OR}$ problem has a deterministic query complexity of $2^{n}$.
\end{thm}

\begin{proof}
  First, note that any algorithm that determines the $\mathsf{OR}$ problem can
  stop as soon as it queries $A$ and gets an output of $1$. Hence, for any
  algorithm $M$, let $\{s_{i}\}$ be the sequence of queries $M$ makes to $A$ on
  the assumption that it always recieves a response of $0$. If
  $\abs{\{s_{i}\}} \le 2^{n}$, there exists some $s \in \{0, 1\}^{n}$ not queried.
  In that case, $M$ will not be able to distinguish the zero oracle from the
  oracle that outputs $1$ only when given $s$. Hence, $M$ must query every
  string of length $n$ and thus the query complexity is $2^{n}$.
\end{proof}

From this, we get that the $\mathsf{OR}$ problem cannot be solved any better
than by enumerative checking. This makes intuitive sense because none of the
results we get by querying $A$ imply anything about what $A$ will do on other
values, since $A$ can be an arbitrary function. Later on (in
Section~\ref{sec:alg-query-complexity}), we will look at what happens when we
give ourselves access to a \emph{polynomial}, where querying one point could
tell us information about others.

For the next two definitions, since their Turing machines include some element
of randomness, we only require that they succeed with a $2/3$ probability. This
is in line with most definitions of complexity classes involving random
computers.

\begin{defn}[{\cite[17]{AW09}}]\label{def:rand-qc}\index{query complexity!randomized}
  Let $f: \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{randomized query complexity} of $f$, which we write $D(f)$, is the
  minimum number of queries made by any randomized algorithm with access to an
  oracle $A$ that evaluates $f(A)$ with probability at least $2/3$.
\end{defn}

% TODO: Talk about how quantum oracles are weird?

\begin{defn}[{\cite[17]{AW09}}]\label{def:quant-qc}\index{query complexity!quantum}
  Let $f: \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{quantum query complexity} of $f$, which we write $D(f)$, is the
  minimum number of queries made by any quantum algorithm with access to an
  oracle $A$ that evaluates $f(A)$ with probability at least $2/3$.
\end{defn}
% TODO: Examples

\section{Relativization of $\P$ vs.\ $\NP$}\label{sec:rel-p-np}

% TODO? move to relativization section as an example?
An important example of relativization is that of $\P$ and $\NP$. While the
question of if $\P = \NP$ is still open, we aim to show that \emph{regardless of
the answer}, the result does not algebrize. To do this, we show that there are
some oracles $A$ where $\P^{A} = \NP^{A}$, and some where $\P^{A} \ne \NP^{A}$.

Additionally, it should be noted that the similarity of relativization to
algebrization means that the structure of these proofs will return in
Section~\ref{sec:alg-p-np} when we show the algebrization of $\P$ and $\NP$.

\subsection{Equality}

The more straightforward of the two proofs is the oracle where
$\P^{A} = \NP^{A}$, so we shall begin with that.

\begin{thm}[{\cite[Theorem 2]{BGS75}}]\label{thm:p-np-rel}
  There exists an oracle $A$ such that $\P^{A} = \NP^{A}$.
\end{thm}

\begin{proof}
  For this, we can let $A$ be any $\PSPACE$-complete language. By letting our
  machine in $\P$ be the reducer from $A$ to any other language in $\PSPACE$, we
  therefore get that $\PSPACE \subseteq \P^{A}$. Similarly, if we have a problem in
  $\NP^{A}$, we can verify it in polynomial space without talking to $A$ at all
  (by having our machine include a determiner for $A$). Hence, we have that
  $\NP^{A} \subseteq \NPSPACE$. Further, a celebrated result of Savitch~\cite{Sav70} is
  that $\PSPACE = \NPSPACE$. Combining all these results, we get the chain
  \begin{equation}
    \NP^{A} \subseteq \NPSPACE = \PSPACE \subseteq \P^{A} \subseteq \NP^{A}.
  \end{equation}
  This is a circular chain of subset relations, which means everything in the
  chain must be equal. Hence, $\P^{A} = \NP^{A} = \PSPACE$.
\end{proof}

For a slightly more intuitive view of what this proof is doing, what we have
done is found an oracle that is so powerful that it dwarfs any amount of
computation our actual Turing machine can do. Hence, the power of our machine is
really just the same as the power of our oracle, and since we have given both
the $\P$ and $\NP$ machine the same oracle, they have the same power.

\subsection{Inequality}

Having shown that an oracle exists where $\P^{A} = \NP^{A}$, we now endeavor to
find one where $\P^{A} \ne \NP^{A}$. This piece of the proof is less simple than
the previous section, and it uses a diagonalization argument to construct the
oracle. Before we dive in to the main proof, however, we need to define a few
preliminaries.

\begin{defn}[{\cite[436]{BGS75}}]\label{def:l(x)}\index{L(X)@$L(X)$}
  Let $X$ be an oracle. The language $L(X)$ is the set
  \begin{equation*}
    L(X) = \{x \mid \text{there is } y \in X \text{ such that } \abs{y} = \abs{x}\}.
  \end{equation*}
\end{defn}

Our eventual goal will be to construct a language $X$ such that
$L(X) \in \NP^{X} \setminus \P^{X}$. Of particular note is that we can rather nicely put a
upper bound on the complexity of $L(X)$ when given $X$ as an oracle, regardless
of the value of $X$. This fact is what gives us the freedom to construct $X$ in
such a way that $L(X)$ will not be in $\P^{X}$.

\begin{lemma}[{\cite[436]{BGS75}}]\label{lem:l(x)-in-np}
  For any oracle $X$, $L(X) \in \NP^{X}$.
\end{lemma}

\begin{proof}
  Let $S$ be a string of length $n$. If $S \in L(X)$, then a witness for $S$ is
  any string $S'$ such that $\abs{S} = \abs{S'}$ and $S' \in X$. Since a machine
  with query access to $X$ can query whether $S'$ is in $X$ in one step, it
  follows that we can verify that $S \in L(X)$ in polynomial time.
\end{proof}

With this lemma as a base, we can now move on to our main theorem.

\begin{thm}[{\cite[Theorem 3]{BGS75}}]\label{thm:p-np-nrel}
  There exists an oracle $A$ such that $\P^{A} \ne \NP^{A}$.
\end{thm}

\begin{proof}
  Our goal is to construct a set $B$ such that $L(B) \notin \P^{B}$. We shall
  construct $B$ in an interative manner. We do this by taking a sequence
  $\{P_{i}\}$ of all machines that recongize some language in $\P^{A}$, and then
  constructing $B$ such that for each machine in the sequence, there is some
  part of $L(B)$ it cannot recognize. This technique is called
  \emph{diagonalization}, and it is used in many places in computer science
  theory.\footnote{This argument style is named after \emph{Cantor's diagonal
      argument}, which was originally used to prove that the real numbers are
    uncountable~\cite[Thm. 2.14]{Ru76}.} Additionally, we define $p_{i}(n)$ to
  be the maximum running time of $P_{i}$ on an input of length $n$. We give the
  following algorithm to construct $B$:

  \begin{algorithm}[H]
    % FIXME: Do the P_i need to be P machines? Everybody is unclear on this
    \KwIn{A sequence of $\P$ oracle machines $\{P_{i}\}_{i=1}^{\infty}$}
    \KwOut{A set $B$ such that $L(B) \notin \P^{B}$}
    % TODO: Define p_i(n)
    $B(0) \leftarrow \varnothing$\;
    $n_{0} \leftarrow 0$\;
    \For{$i$ starting at $1$}{
      Let $n > n_{i}$ be large enough that
      $p_{i}(n) < 2^{n}$\nllabel{line:def-n}\;
      Run $P_{i}^{B(i-1)}$ on input $0^{n}$\nllabel{line:computation}\;
      \If{$P_{i}^{B(i-1)}$ rejects $0^{n}$}{
        Let $x$ be a string of length $n$ not queried during the above
        computation\nllabel{line:not-queried}\;
        $B(i) \leftarrow B(i-1) \sqcup \{x\}$\;
      }
      $n_{i+1} \leftarrow 2^{n}$\;
    }
    $B \leftarrow \bigcup_{i}B(i)$\;
    \caption{An algorithm for constructing $B$}\label{alg:construct-b}
  \end{algorithm}

  Now that we have presented the algorithm, let us demonstrate its soundness.
  First, note that since $P_{i}$ runs in polynomial time, $p_{i}(n)$ is bounded
  above by a polynomial, and hence there will always exist an $n$ as defined in
  line~\ref{line:def-n}. Next, since there are $2^{n}$ strings of length $n$ and
  since $p_{i}(n) < 2^{n}$, we know that there must be some $x$ to make
  line~\ref{line:not-queried} well-defined. While our algorithm allows $x$ to be
  any string, if it is necessary to be explicit in which we choose, then picking
  $x$ to be the smallest string in lexicographic order is a standard choice.

  We should also briefly mention that this algorithm does not terminate. This is
  okay because we are only using it to construct the set $B$, which does not
  need to be bounded. If this were to be made practical, since the sequence of
  $n_{i}$s is monotonically increasing, the set could be constructed ``lazily''
  on each query by only running the algorithm until $n_{i}$ is greater than the
  length of the query.

  % FIXME: Should this "end goal" section be moved to before the algorithm?
  Next, we demonstrate that $L(B) \notin \P^{B}$. The end goal of our instruction is
  a set $B$ such that if $P_{i}^{B}$ accepts $0^{n}$ then there are no strings
  of length $n$ in $B$, and if $P_{i}^{B}$ rejects, then there is a string of
  length $n$ in $B$. This means that no $P_{i}$ accepts $L(B)$, and hence
  $L(B) \notin \NP^{B}$.

  The central idea behind the proper functioning of our algorithm is that adding
  strings to our oracle \emph{cannot change the output if they are not queried}.
  This is what we do in line~\ref{line:def-n}: we need our input length to be
  long enough to guarantee that a non-queried string exists. Since the number of
  queried strings is no greater than $p_{i}(n)$, and there are $2^{n}$ strings
  of length $n$, there must be some string not queried.

  Next, we run $P_{i}^{B(i-1)}$ on all the strings we have already added. If it
  accepts, then we want to make sure that no string of length $n$ is in $B$;
  that is, $0^{n}$ is not in $L(B)$. Hence, in this particular loop we add
  nothing to $B(i)$. If $P_{i}^{B(i-1)}$ rejects, we then need to make sure that
  $0^{n} \in L(B)$ but in a way that does not affect the output of
  $P_{i}^{B(i-1)}$. Hence, we find a string that $P_{i}^{B(i-1)}$ did not query
  (and thus will not affect the result) and add it to $B(i)$.

  Having done this, we then set $n_{i+1}$ to be $2^{n}$. Since
  $p_{i}(n) < 2^{n}$, it follows that no previous machine could have queried any
  strings of length $n_{i+1}$.\footnote{A word of caution: we only care about
    what $P_{i}$ does on input $n_{i}$, \emph{not any other input}. This is
    because we only need each machine to be incorrect for some $i$, not all
    $i$.} This way, we ensure our previous machines do not accidentally have
  their output change due to us adding a string they queried.

  % TODO: Note about how it's fine that this doesn't actually halt b/c it's just
  % in the construction of the set
  Having run this over all polynomial-time Turing machines, we have a set $L(B)$
  such that no machine in $\P^{B}$ accepts it, which tells us $L(B) \notin \P^{B}$.
  But, Lemma~\ref{lem:l(x)-in-np} already told us $L(B) \in \NP^{B}$. Hence,
  $\P^{B} \ne \NP^{B}$.
\end{proof}

\section{Diagonalization relativizes}\label{sec:diag-relativizes}

Of course, determining that $\P$ vs $\NP$ does not relativize is only important
if the proof techniques used in practice \emph{do} in fact relativize. Rather
unfortunately, it turns out that simple diagonalization is a relativizing
result.

% FIXME: Are there formal definitions of diagonalization?
While diagonalization itself does not have a formal definition, we can still
think about it informally. Looking at our construction of $B$, which we did
using diagonalization, notice that our definition never really cared about how
the $P_{i}$ worked, just about the results it produced. Hence, if it were to be
possible to modify Algorithm~\ref{alg:construct-b} to construct $B \in \NP \setminus \P$,
the proof would remain the same if we were to replace our sequence $\{P_{i}\}$
with a sequence of machines in $\P^{A}$ for some $\PSPACE$-complete $A$.
However, this would lead to a contradiction, as we showed in
Theorem~\ref{thm:p-np-rel} that in that case, $\P^{A} = \NP^{A}$! This tells us
that a simple diagonalization argument would not suffice to determine separation
between $\P$ and $\NP$.

\chapter{Algebrization}\label{chap:algebrization}

Algebrization, originally described by Aaronson and Wigderson~\cite{AW09}, is an
extension of relativization. While relativization deals with oracles that are
Boolean functions, algebrization extends oracles to be a collection of
polynomials over finite fields.

\begin{defn}[{\cite[Def.\ 2.2]{AW09}}]\label{def:ext-oracle}\index{extension oracle}
  % TODO: Reuse definition of extension polynomial from earlier
  Let $A_{m}: \{0, 1\}^{m} \rightarrow \{0, 1\}$ be a Boolean function and let
  $\mathbb{F}$ be a finite field. Then an \emph{extension} of $A_{m}$
  over $\mathbb{F}$ is a polynomial
  $\tilde{A}_{m,\mathbb{F}}: \mathbb{F}^{m} \rightarrow \mathbb{F}$ such that
  $\tilde{A}_{m,\mathbb{F}}(x) = A_{m}(x)$ whevever $x \in \{0, 1\}^{m}$. Also,
  given an oracle $A = (A_{m})$, an extension $\tilde{A}$ of $A$ is a
  collection of polynomials
  $\tilde{A}_{m,\mathbb{F}}: \mathbb{F}^{m} \rightarrow \mathbb{F}$, one for each positive
  integer $m$ and finite field $\mathbb{F}$, such that
  \begin{enumerate}
    \item $\tilde{A}_{m,\mathbb{F}}$ is an extension of $A_{m}$ for all
          $m,\mathbb{F}$, and
    \item there exists a constant $c$ such that
          $\mdeg(\tilde{A}_{m,\mathbb{F}}) \le c$ for all $m, \mathbb{F}$.
          % TODO: Rephrase point 2 in terms of F[x_{1,...,n}^{<= c}]
  \end{enumerate}
\end{defn}

\begin{defn}[{\cite[Def.\ 2.2]{AW09}}]\label{def:ext-oracle-class}
  For any complexity class $\mathcal{C}$ and extension oracle $\tilde{A}$, the complexity
  class $\mathcal{C}^{\tilde{A}}$ is the class of all languages determinable by a Turing
  machine with access to $\tilde{A}$ with the requirements for $\mathcal{C}$.
\end{defn}

Next, we need to formally define what algebrization is.

\begin{defn}[{\cite[Def.\ 2.3]{AW09}}]\label{def:algebrization}\index{algebrization}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \subseteq \mathcal{D}$. We say the result
  $\mathcal{C} \subseteq \mathcal{D}$ \emph{algebrizes} if $\mathcal{C}^{A} \subseteq \mathcal{D}^{\tilde{A}}$ for all oracles $A$ and
  finite field extensions $\tilde{A}$ of $A$. Conversely, if there exists $A$
  and $\tilde{A}$ such that $\mathcal{C} \nsubseteq \mathcal{D}$, we say that the result $\mathcal{C} \subseteq \mathcal{D}$ \emph{does
    not algebrize}.
\end{defn}

\begin{defn}[{\cite[Def.\ 2.3]{AW09}}]\label{def:algebrization-neq}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \nsubseteq \mathcal{D}$. We say the result
  $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{algebrizes} if $\mathcal{C}^{A} \nsubseteq \mathcal{D}^{\tilde{A}}$ for all oracles $A$ and
  finite field extensions $\tilde{A}$ of $A$. Conversely, if there exists $A$
  and $\tilde{A}$ such that $\mathcal{C} \subseteq \mathcal{D}$, we say that the result $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{does
    not algebrize}.
\end{defn}

\section{Algebraic query complexity}\label{sec:alg-query-complexity}

Similarly to how we defined query complexity in
Section~\ref{sec:query-complexity}, our notion of algebrization requires a
definition of \emph{algebraic} query complexity. % TODO: More

\begin{defn}[{\cite[Def. 4.1]{AW09}}]\label{def:aqc}\index{query complexity!algebraic}
  Let $f: \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function, $\mathbb{F}$ be a
  field, and $c$ be a positive integer. Also, let $\mathbb{M}$ be the set of
  deterministic algorithms $M$ such that $M^{\tilde{A}}$ outputs $f(A)$ for
  every oracle $A: \{0, 1\}^{n} \rightarrow \{0, 1\}$ and every finite field extension
  $\tilde{A}: \mathbb{F}^{n} \rightarrow \mathbb{F}$ of $A$ with $\mdeg(\tilde{A}) \le c$.
  Then, the deterministic algebraic query complexity of $f$ over $\mathbb{F}$ is
  defined as
  \begin{equation}
    \tilde{D}_{\mathbb{F}, c}(f) = \min_{M \in \mathcal{M}}\mleft(
      \max_{A, \tilde{A}: \mdeg(\tilde{A}) \le c}T_{M}(\tilde{A})
    \mright),
  \end{equation}
  where $T_{M}(\tilde{A})$ is the number of queries to $\tilde{A}$ made by
  $M^{\tilde{A}}$.
  % TODO: Can this be made more intelligible?
\end{defn}

Our goal here is to find the \emph{worst}-case scenario for the \emph{best}
algorithm that calculates the property $f$. The difference between this and
Definition~\ref{def:det-qc} is twofold: first, our algorithm $M$ has access to
an extension oracle of $A$, and second, that we can limit our $\tilde{A}$ in
its maximum multidegree. For the most part, we will focus on equations with
multidegree 2, which is enough to get the results we want.

As an example, let us look at the same $\mathsf{OR}$ problem we defined in
Definition~\ref{def:or-problem}.

% FIXME:
\begin{thm}[{\cite[Thm.\ 4.4]{AW09}}]\label{thm:or-algebraic}
  $\tilde{D}_{\mathbb{F},2}(\mathsf{OR}) = 2^{n}$ for every field $\mathbb{F}$.
\end{thm}

\begin{proof}
  % TODO
\end{proof}

This gives us a potentially counterintuitive property of algebraic query
complexity: while it would seem that giving our machine a polynomial (and a
polynomial of multidegree only 2, at that) would give us the ability to solve
the hardest problems more quickly, that turns out not to be the case.

Now, while this is true for polynomials of multidegree 2, it turns out that if
we restrict our oracles to being simply \emph{multilinear} polynomials, we do
get a speedup.

\begin{thm}[{\cite[Thm. 3]{JKRS09}}]\label{thm:or-multilinear}
  $\tilde{D}_{\mathbb{F},1}(\mathsf{OR}) = 1$ for every field $\mathbb{F}$ with
  characteristic not equal to $2$.
\end{thm}

\begin{proof}
  % TODO
  Let $A: \{0, 1\}^{n} \rightarrow \{0, 1\}$ and $\tilde{A}$ be our extension polynomial.
  Consider the value of $p(1/2, \ldots, 1/2)$. We aim to show that this value is
  equal to $0$ if and only if $A$ is the zero oracle.

  Consider the function
  \begin{equation}
    p'(x_{1}, \ldots, x_{n}) = p(1 - 2x_{1}, \ldots, 1 - 2x_{n}).
  \end{equation}
  Since $1 - 2x$ is a linear polynomial, it follows that $p'$ is itself a
  multilinear polynomial. Further, since the sum over $\{1, -1\}^{n}$ of a
  non-constant multilinear monomial is 0 as per Lemma~\ref{lem:monomial-sum}, it
  follows that
  \begin{equation}
    \sum_{b \in \{-1, 1\}^{n}}p'(b) = p'(0, \ldots, 0),
  \end{equation}
  i.e., the constant term of $p'$. Further, from our definition of $p'$, we have
  that $p'(0, \ldots, 0) = p(1/2, \ldots, 1/2)$. Hence, we have
  \begin{equation}
    \sum_{b \in \{0, 1\}^{n}}p(b) = p(1/2, \ldots, 1/2).
  \end{equation}
  Since $p(b) \ge 0$ for all $b \in \{0, 1\}^{n}$, it follows that $p(1/2, \ldots, 1/2)$
  is 0 if and only if $p(b) = 0$ for all $b \in \{0, 1\}^{n}$, i.e. exactly when
  $A$ is the zero function.
\end{proof}

% TODO: Deterministic & nondeterministic AQC
% TODO: Work through the result of Juma et al. (JKRS09) that the OR problem can
% be solved with 1 query for multilinear polynomials? More generally, perhaps
% use the JKRS proofs instead of the AW proofs if they're cleaner?
% Ok yeah, having read it, I definitely want JKRS09 Thm 3 b/c it's so slick

\section{Algebrization of $\P$ vs.\ $\NP$}\label{sec:alg-p-np}

As with relativization, an important application of algebrization is in regards
to the $\P$ vs.\ $\NP$ problem.

\begin{lemma}\label{lem:multilinear-is-pspace}
  Let $L$ be a $\PSPACE$-complete language, with corresponding oracle $A$. Let
  $\tilde{A}$ be the unique multilinear extension oracle of $A$. Then the
  language
  \begin{equation}
    \tilde{L} = \bigcup_{n \in \mathbb{N}}\{(x_{1}, \ldots, x_{n}, z) \in \mathbb{F}^{n+1} \mid \tilde{A}(x_{1}, \ldots, x_{n}) = z\}
  \end{equation}
  is $\PSPACE$-complete.
\end{lemma}

\begin{proof}
  First, we provide a polynomial-time reduction from $L$ to $\tilde{L}$. Since
  for all $x \in \{0, 1\}^{n}$, $\tilde{A}(x) = 1$ if and only if $x \in L$, it
  follows that
  \begin{equation}
    \begin{aligned}
      f: \Sigma^{*} &\rightarrow \Sigma^{*} \\
      x &\mapsto (x, 1)
    \end{aligned}
  \end{equation}
  is a polynomial-time reduction from $L$ to $L'$.

  \begin{algorithm}[H]
    \KwIn{$(x_{1}, \ldots, x_{n}, z) \in \mathbb{F}^{n+1}$}
    \KwOut{Whether $\tilde{A}(x_{1}, \ldots, x_{n}) = z$}
    $z' \leftarrow 0$\;
    \For{$k \in \{0, 1\}^{n}$}{
      Simulate $L$ on input $k$\;
      \If{$k \in L$}{
        $z' \leftarrow z' + \delta_{k}(x_{1}, \ldots, x_{n})$\;
      }
    }
    \Return{whether $z = z'$}\;
    \caption{Determiner for $\tilde{L}$}\label{alg:l-tilde-det}
  \end{algorithm}

  First, we demonstrate that the above algorithm runs in $\PSPACE$. Calculating
  polynomials is itself a $\PSPACE$ problem, and simulating a $\PSPACE$ machine
  is itself $\PSPACE$, hence Algorithm~\ref{alg:l-tilde-det} is a $\PSPACE$
  algorithm.

  Next, we show that Algorithm~\ref{alg:l-tilde-det} determines $\tilde{L}$.
  First, from Corollary~\ref{cor:low-degree-boolean}, we know that for any $n$,
  \begin{equation}
    \tilde{A}_{n}(x_{1}, \ldots, x_{n}) = \sum_{\beta \in L}\delta_{\beta}(x_{1}, \ldots, x_{n}).
  \end{equation}
  Hence, the only thing the above algorithm does is calculate
  $\tilde{A}_{n}(x_{1}, \ldots, x_{n})$ and then compares it to the value we were
  given. As such, it determines $\tilde{L}$.

  Since there is a reduction from $L$ to $\tilde{L}$, we know that $L$ is no
  harder than $\tilde{L}$, and Algorithm~\ref{alg:l-tilde-det} demonstrates that
  $\tilde{L} \in \PSPACE$. Hence, $\tilde{L}$ is $\PSPACE$-complete.
\end{proof}

With that as a base, we can now move on to the main theorem. As before, the more
straightforward proof is the oracle where $\P^{\tilde{A}} = \NP^{A}$, so we
begin with that.

\begin{thm}[{\cite[Theorem 5.1]{AW09}}]\label{thm:p-np-alg}
  There exist $A$, $\tilde{A}$ such that $\NP^{A} = \P^{\tilde{A}}$.
\end{thm}

\begin{proof}
  % TODO: Pull the lemma from BFL into this paper
  For this theorem, we use the same technique we did in our proof of
  Theorem~\ref{thm:p-np-rel}: find a $\PSPACE$-complete language $A$ and work
  from there. If we let $\tilde{A}$ be the unique multilinear extension of $A$,
  Lemma~\ref{lem:multilinear-is-pspace} tells us $\tilde{A}$ is
  $\PSPACE$-complete. Hence, reusing our argument from
  Theorem~\ref{thm:p-np-rel}, we have
  \begin{equation}
    \NP^{\tilde{A}} = \NP^{\PSPACE} = \PSPACE = \P^{A}.
  \end{equation}
  % TODO: Don't just say "reusing our argument": actually spell it out
  % NOTE: This uses BFL90 which introduces concepts of MIP & such; how do we
  % work that in with the MIP work later in the thesis?
\end{proof}

Now it is time for the other case.

\begin{thm}[{\cite[Theorem 5.3]{AW09}}]\label{thm:p-np-nalg}
  There exist $A$, $\tilde{A}$ such that $\NP^{A} \ne \P^{\tilde{A}}$.
\end{thm}

\begin{proof}
  Like in Theorem~\ref{thm:p-np-nrel}, we aim to ``diagonalize'': iterate over
  all $\P^{\tilde{A}}$ machines to construct a language that none of them can
  recognize. Also like before, we will do this by constructing an oracle
  extension $\tilde{A}$ such that $L(A) \notin \P^{\tilde{A}}$. Since we only give an
  algebraic extension to $\P$ and not $\NP$, we can resuse the result from
  Lemma~\ref{lem:l(x)-in-np} that $L(B) \in \NP^{A}$. We shall construct
  $\tilde{A}$ using the following algorithm:
  % FIXME: Mention that until set, A_{n,F} outputs 0
  \begin{algorithm}[H]
    % FIXME: Do the P_i need to be P machines? Everybody is unclear on this
    \KwIn{A sequence of $\P$ oracle machines $\{P_{i}\}_{i=1}^{\infty}$}
    \KwOut{An extension oracle $\tilde{A}$ such that $L(A) \notin \P^{\tilde{A}}$}
    % TODO: Define p_i(n)
    $\tilde{A} \leftarrow \varnothing$\;
    $n_{0} \leftarrow 0$\;
    \For{$i$ starting at $1$}{
      Let $n > n_{i}$ be large enough that
      $p_{i}(n) < 2^{n}$\;
      % TODO: Think of this as a collection of oracles instead of indices?
      $T_{j} \leftarrow \bigcup_{j < i}S_{j}$\;
      Run $P_{i}^{\tilde{A}}$ on input $0^{n}$\;
      \eIf{$P_{i}^{B(i-1)}$ rejects $0^{n}$}{
        Let $\mathcal{Y}_{\mathbb{F}}$ be the set of all $y \in \mathbb{F}^{n_{i}}$ queried
        during the above computation\;
        \tcp{See Lemma~\ref{lem:multiquad-adversary} for why we can do this}
        Let $w \in \{0, 1\}^{n}$ such that the following
        works\nllabel{line:def-w}\;
        \For{all $\mathbb{F}$}{
          Set $\tilde{A}_{n_{i},\mathbb{F}}$ to be a multiquadratic polynomial
          such that $\tilde{A}_{n_{i},\mathbb{F}}(w) = 1$ and
          $\tilde{A}_{n_{i},\mathbb{F}}(y) = 0$ for all
          $y \in \mathcal{Y}_{\mathbb{F}} \cup (\{0, 1\}^{n_{i}} \setminus \{w\})$\nllabel{line:set-a}\;
          % FIXME: Be more clear about what this means
        }
      }{
        Set $\tilde{A}_{n_{i},\mathbb{F}} = 0$ for all $\mathbb{F}$\;
      }
      $n_{i+1} \leftarrow 2^{n}$\;
    }
    $B \leftarrow \bigcup_{i}B(i)$\;
    \caption{An algorithm for constructing $\tilde{A}$}\label{alg:construct-a-tilde}
  \end{algorithm}
  As before, we will start by demonstrating soundness and then move on to why
  the constructed oracle provides the separation we seek.
  % TODO
\end{proof}

\printbibliography[heading=bibintoc]{}

\printindex{}

\end{document}
