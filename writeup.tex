\documentclass[english,12pt]{reedthesis}

\usepackage[T1]{fontenc}
\usepackage{babel}

\usepackage{appendix}
\usepackage{amsfonts, amscd, amssymb, amsthm, amsmath}
\usepackage{mathtools} %xmapsto etc
\usepackage{pdfsync} %leaves makers for tex searching
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{complexity}
\usepackage{mleftright}
\usepackage{xcolor}
\usepackage{soul}

\usepackage[linesnumbered,algochapter]{algorithm2e}
\usepackage{biblatex}
\usepackage{imakeidx}
\usepackage{microtype}
\usepackage{tikz}
\usepackage{csquotes}
\usepackage{fewerfloatpages}

% FIXME: Really bad hack to get cleveref working
% This is unnecessary on literally every other LaTeX setup except mine
% When working again, will need to reset theorem defs to point to the thm counter
\usepackage{aliascnt}

\usepackage[colorlinks]{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}

\usetikzlibrary{arrows.meta}
\usetikzlibrary{chains}
\usetikzlibrary{hobby}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{through}
\usetikzlibrary{trees}

\setlength{\headheight}{14.5pt}

\hyphenation{prob-a-bilis-ti-cal-ly}

%%% Theorems %%%---------------------------------------------------------
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newaliascnt{lemma}{thm}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\newaliascnt{prop}{thm}
\newtheorem{prop}[prop]{Proposition}
\aliascntresetthe{prop}
\newaliascnt{cor}{thm}
\newtheorem{cor}[cor]{Corollary}
\aliascntresetthe{cor}
\theoremstyle{definition}
\newtheorem*{def*}{Definition}
\newaliascnt{defn}{thm}
\newtheorem{defn}[defn]{Definition}
\aliascntresetthe{defn}
\theoremstyle{remark}
\newtheorem{example}{Example}[thm]
\newtheorem{remark}[thm]{Remark}
\newtheorem{subrem}[example]{Remark}

\definecolor{plum}{HTML}{8105C1}
\definecolor{pumpkin}{HTML}{E47604}
\definecolor{rose}{HTML}{C10091}
\definecolor{dgreen}{HTML}{25A75B}
\definecolor{dblue}{HTML}{0066FF}
\definecolor{cornflower}{HTML}{3256C3}
\definecolor{viridian}{HTML}{099A97}
\definecolor{alert}{HTML}{3256C3}

\DeclareMathOperator{\Acc}{Acc}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\colsp}{colsp}
\DeclareMathOperator{\comp}{comp}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\dimn}{dim}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\ev}{ev}
\def\f{\varphi}
\def\half{\hbox{$\frac12$}}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\img}{img}
\DeclareMathOperator{\Inn}{Inn}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\mdeg}{mdeg}
\DeclareMathOperator{\out}{out}
\DeclareMathOperator{\oin}{in}
\DeclareMathOperator{\rad}{rad}
\DeclareMathOperator{\Rep}{Rep}
\DeclareMathOperator{\rev}{rev}
\DeclareMathOperator{\RM}{RM}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\rk}{rank}
\def\normeq{\trianglelefteq}
\DeclareMathOperator{\nul}{nullity}
\DeclareMathOperator{\per}{per}
\DeclareMathOperator{\Sim}{Sim}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Syl}{Syl}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\View}{View}
\def\vep{\varepsilon}
\DeclareMathOperator{\lcm}{lcm}

\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\ang}{\langle}{\rangle}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\DeclarePairedDelimiter\bra{\langle}{\rvert}
\DeclarePairedDelimiter\ket{\lvert}{\rangle}
\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1\,\delimsize\vert\,\mathopen{}#2}
\DeclarePairedDelimiterX\braopket[3]{\langle}{\rangle}{#1\,\delimsize\vert\,\mathopen{}#2\,\delimsize\vert\,\mathopen{}#3}

\newcommand{\middlemid}{%
  \ensuremath{\;\middle\vert\;}
}

\newcommand{\dblang}[1]{%
  \ensuremath{\left\langle\!\left\langle#1\right\rangle\!\right\rangle}
}

\newcommand{\comment}[1]{%
  \text{\phantom{(#1)}} \tag{#1}%
}

\newcommand{\commath}[1]{%
  \phantom{(#1)} \tag{#1}%
}

\newcommand{\pcpp}[4]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{rand.\ complexity:} & #1 \\
    \text{query complexity:} & #2 \\
    \text{prox.\ param.:} & #3 \\
    \text{soundness error:} & #4
  \end{array}}\mright]%
}

\newcommand{\pcppr}[6]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{rand.\ complexity:} & #1 \\
    \text{query complexity:} & #2 \\
    \text{prox.\ param.:} & #3 \\
    \text{soundness error:} & #4 \\
    \text{RS error:} & #5 \\
    \text{robustness param:} & #6
  \end{array}}\mright]%
}

\newcommand{\ipcp}[5]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{round complexity:} & #1 \\
    \text{PCP length:} & #2 \\
    \text{comm.\ complexity:} & #3 \\
    \text{query complexity:} & #4 \\
    \text{soundness error:} & #5
  \end{array}}\mright]%
}

\newcommand{\pzkipcp}[6]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{round complexity:} & #1 \\
    \text{PCP length:} & #2 \\
    \text{comm.\ complexity:} & #3 \\
    \text{query complexity:} & #4 \\
    \text{query bound:} & #5 \\
    \text{soundness error:} & #6
  \end{array}}\mright]%
}

\newcommand{\pzkpcp}[4]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{rand.\ complexity:} & #1 \\
    \text{query complexity:} & #2 \\
    \text{query bound:} & #3 \\
    \text{soundness error:} & #4
  \end{array}}\mright]%
}

\newcommand{\pzkpcpr}[6]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{rand.\ complexity:} & #1 \\
    \text{query complexity:} & #2 \\
    \text{query bound:} & #3 \\
    \text{soundness error:} & #4 \\
    \text{RS error:} & #5 \\
    \text{robustness param:} & #6
  \end{array}}\mright]%
}

\newcommand{\pzkpcppr}[7]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{rand.\ complexity:} & #1 \\
    \text{query complexity:} & #2 \\
    \text{query bound:} & #3 \\
    \text{soundness error:} & #4 \\
    \text{proximity:} & #5 \\
    \text{RS error:} & #6 \\
    \text{robustness param:} & #7
  \end{array}}\mright]%
}

\newcommand{\ldipcp}[6]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{round complexity:} & #1 \\
    \text{PCP length:} & #2 \\
    \text{comm.\ complexity:} & #3 \\
    \text{query complexity:} & #4 \\
    \text{oracle:} & #5 \\
    \text{soundness error:} & #6
  \end{array}}\mright]%
}

\newcommand{\mipstar}[4]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{number of provers:} & #1 \\
    \text{round complexity:} & #2 \\
    \text{comm.\ complexity:} & #3 \\
    \text{soundness error:} & #4
  \end{array}}\mright]%
}

\newcommand{\pcpr}[4]{%
  \ensuremath%
  \mleft[{\footnotesize\begin{array}{r l}
    \text{query complexity:} & #1 \\
    \text{random complexity:} & #2 \\
    \text{robustness parameter:} & #3 \\
    \text{robust-soundness error:} & #4
  \end{array}}\mright]%
}

\newclass{\CktVal}{CktVal}
\newclass{\IPCP}{IPCP}
\makeatletter % complexity doesn't support hyphens by default...
\newcommand{\PZKPCP}{%
  \PZK\complexity@hyphenleft\PCP
}
\newcommand{\PZKIPCP}{%
  \PZK\complexity@hyphenleft\IPCP
}
\newcommand{\PZKMIP}{%
  \PZK\complexity@hyphenleft\MIP
}
\newcommand{\PZKPCPP}{%
  \PZK\complexity@hyphenleft\PCPP
}
\makeatother

\newlang{\CktSAT}{CktSAT}
\newlang{\ECC}{ECC}
\newlang{\GNI}{GNI}
\newlang{\OR}{OR}
\newlang{\OSAT}{O3SAT}
\newlang{\PCPP}{PCPP}
\newlang{\PolySim}{PolySim}
\newlang{\Prime}{Prime}
\newlang{\Sum}{Sum}

\SetKw{Accept}{accept}
\SetKw{Reject}{reject}
\SetKwBlock{Proof}{proof}{end}
\SetKwBlock{Verifier}{verifier}{end}
\SetKwInput{KwOracle}{Oracle}

\newcommand{\shrug}[1][]{%
  \begin{tikzpicture}[baseline,x=0.8\ht\strutbox,y=0.8\ht\strutbox,line width=0.125ex,#1]
    \def\arm{(-2.5,0.95) to (-2,0.95) (-1.9,1) to (-1.5,0) (-1.35,0) to (-0.8,0)};
    \draw \arm;
    \draw[xscale=-1] \arm;
    \def\headpart{(0.6,0) arc[start angle=-40, end angle=40,x radius=0.6,y radius=0.8]};
    \draw \headpart;
    \draw[xscale=-1] \headpart;
    \def\eye{(-0.075,0.15) .. controls (0.02,0) .. (0.075,-0.15)};
    \draw[shift={(-0.3,0.8)}] \eye;
    \draw[shift={(0,0.85)}] \eye;
    % draw mouth
    \draw (-0.1,0.2) to [out=15,in=-100] (0.4,0.95);
  \end{tikzpicture}
}

\makeatletter
\renewenvironment{algomathdisplay}
 {\[}
 {\@endalgocfline\vspace{-\baselineskip}\]{\DontPrintSemicolon\;}}
\makeatother

\makeatletter
\renewenvironment{abstract}{%
  \if@twocolumn
    \@restonecoltrue\onecolumn
  \else
    \@restonecolfalse
  \fi
  \chapter*{}
  \begin{center}
  {\fontsize{14}{16}\selectfont \bfseries Abstract}
  \end{center}
  \fontsize{12}{14}\selectfont
}{\clearpage \if@restonecol\twocolumn\fi}%
\makeatother

\addbibresource{bibliography.bib}

\makeindex[intoc]

\title{Extending Zero-Knowledge PCPs Beyond NP}
\author{Patrick Norton}

\approvedforthe{Committee}
\thedivisionof{The Established Interdisciplinary Committee for \\}
\division{Mathematics and Computer Science}
\department{Mathematics and Computer Science}
\advisor{Zajj Daugherty}
\altadvisor{Adam Groce}

\begin{document}

\maketitle

\tableofcontents

\listofalgorithms

\begin{abstract}
  This expository paper examines the interaction between zero-knowledge proofs
  and probabilistically-checkable proofs (PCPs). To start, we build up much of
  the language and tools of modern complexity theory, in particular the
  variations on interactive proofs and how they can be made zero-knowledge. We
  also build up much of the algebraic language necessary for working with
  zero-knowledge proofs and PCPs. Next, we show a proof of the PCP theorem,
  which relates the power of probabilistically-checkable proofs and the class
  $\NP$. After that, we unpack several pieces of recent literature on
  zero-knowledge probabilistically-checkable proofs, including a zero-knowledge
  version of the aforementioned PCP theorem.
\end{abstract}

\chapter{Introduction}

Complexity theory, to wit, is the study of what problems are easy and what
problems are hard. More specifically, it is the study of computational problems
and how they can be classified according to the amount of effort needed to solve
them. While there are many facets of complexity theory, the one we will be
focusing on is that of computational proof systems: different mechanisms by
which one computer can prove a statement to another computer in a way that
removes reasonable doubt. Along with that, we look at proof systems that reveal
no potentially-sensitive information, besides the exact statement we care about
proving. We show that in several useful cases, this can be done without making
the problem substantially harder.

The type of computational proof system we will be working with the most is
called a probabilistically-checkable proof. The idea behind these is that
instead of reading the entire proof end-to-end, in the way we would for a
``standard'' mathematical proof, we only check some random portion of it. The
question therefore becomes how little of the proof we can check while still
reliably getting the right answer, and with the ability to catch out ``liar''
proofs that purport to prove some incorrect statement. It has been shown that
for many problems, one needs to read only a very small portion of the proof in
order to still be correct; we also show that this can be done in a way that
reveals no potentially-sensitive information. We further show that by combining
a probabilistically-checkable proof with a different proof system, we can gain
even more power, still while leaking nothing sensitive.

\section{What is a computational problem?}

In this thesis, we focus on \emph{decision problems}: problems where we are
given some input text, and then we have to decide either yes or no. We care
about these because mathematically, they are the simplest type of problem.
Importantly for us, the yes/no answer for a given input never changes, and every
input value has some correct answer.

The next layer in setting up a computational problem is to define an
\emph{alphabet}. Every input text needs to be written down in some particular
form, and it is important to know how it is written ahead of time. The only true
restriction on our alphabet is that it must be finite (an infinite amount of
symbols would make it really hard to do any useful analysis), but in practice,
because we are dealing with computers our alphabet almost always consists of
just the symbols 0 and 1.

The second important property of computational problems is that of input length.
Of course, when we ask for the answer to a question, our query could be as long
as we like. However, in general we expect longer questions to take longer to
process, so our computation time is almost always written in terms of the input
length. Knowing that, determining the input length is simple---it is just the
number of alphabet letters in the query.

\section{Turing machines}

A \emph{Turing machine} is the principal model we use to underpin our
computations. A Turing machine is a theoretical box that has access to some
arbitrarily-long \emph{tape}\footnote{The naming of a tape comes by analogy to a
  ticker tape or audio tape, not masking tape.}, a list of blank spaces to which
it can read and write data, one space at a time. The computation of the machine
proceeds in individual steps---in each step the machine can know what is currently
on the tape, and what state the machine is currently in; after it receives this
information it may choose to ``halt'' (i.e., stop running and make a decision),
or otherwise it may write a single bit to the tape and then move its view one
square left or right.

It may at first seem like this is a rather silly definition of a computer---after
all our computers have things like screens and keyboards, and there is
definitely not an infinite tape sticking out the side. However, it turns out
this is still useful to us! As we discussed in the last section, computational
problems do not always line up with what we consider computational tasks in the
real world, and just answering yes/no requires neither a screen nor keyboard.

Further objection might be taken to the fact that to access different parts of
the tape, we need to slowly move the machine over, step by step. This concern is
less easy to intuitively dissuade, but it turns out that at the level of
granularity we will care about, this is not actually a concern. Essentially, we
only care about classifying problems at a very coarse level, and at some point
the differences in our computational model pale in comparison to the differences
in the complexity of the problem itself.\footnote{This is no accident---these
  classes were picked to some extent \emph{because} of their independence of
  model, not in spite of it.} Since it is indistinguishable for our purposes
from a normal computer, we use it because it is mathematically nicer than many
other models of computation.

This classical notion of a Turing machine has of course spawned many variants to
reflect different ways of interacting in the real world. Perhaps the most famous
is the \emph{quantum} Turing machine, which attempts to emulate the
computational abilities of a computer that can leverage specific quantum
effects. A variant we will be focusing on somewhat in this work is the
\emph{interactive} Turing machine, a machine with the ability to pass messages
back and forth with another interactive machine. Another variant we will be
focusing on is the \emph{oracle} Turing machine, a machine with access to some
``oracle'' (a function that can answer some hard problem in a single step).

\section{Relativization and algebrization}

Oracle machines have a particular use in how they interact with the most famous
open hypothesis in complexity theory: $\P$ vs.\ $\NP$. In brief, this asks
whether any problem that can be verified quickly (which we call $\NP$) can also
be solved quickly (which we call $\P$). A good example of this kind of problem
is Sudoku: taking a solved board and verifying if it is actually valid does not
take that long (all you need to do is go through each row, column, and 3x3
square and check every number appears exactly once), but taking an unsolved one
and coming up with a solution is much harder.

Unfortunately, what these oracle machines tell us is that we are not very close
to solving $\P$ vs.\ $\NP$. What these have shown us is that entire classes of
techniques---in particular \emph{every single} proof technique that had been tried
for problems similar to $\P$ vs.\ $\NP$---would not work here.

Even worse for us, this has now happened twice! After the first
paper~\cite{BGS75} showing this came out (calling its technique
\emph{relativization}), mathematicians got to work looking for
relativization-proof techniques. Over time they found many, until another
paper~\cite{AW09} came out, building on the previous with a new technique called
\emph{algebrization}. Similarly to before, this showed that every
relativization-proof technique was not algebrization-proof, sending researchers
back on a new series of quests for better proof techniques.

The general idea behind relativization is based on observing what problems
become easy when we add different oracles to a Turing machine. It is worth
noting that no problem ever becomes harder when we add an oracle: a machine can
always choose to never talk to an oracle and then it could solve any problem
just the same as a non-oracle machine. However, if we give a machine a powerful
enough oracle, it turns out that any problem that it could verify quickly would
also be solvable quickly. But we can also pick a particularly tricky oracle to
show the converse---a machine with that oracle can verify some problems quickly
that we are able to prove that it cannot solve quickly. What this all means is
that any proof of the $\P$ vs.\ $\NP$ problem needs to have its logic break
somewhere if we introduce an oracle: if we can replace every instance of
``$\P$'' with ``$\P$ with some oracle'' (and similarly for $\NP$) and the logic
holds up, we have created a contradiction no matter what we do.

Algebrization works similarly to relativization, but with a slightly different
model of an oracle. In this case, instead of a normal oracle (which we can think
of as a function that returns either a 0 or a 1), we think of our oracle as a
large collection of polynomials. Because we can now get back arbitrary numbers
from our oracle, instead of just 0 or 1, our oracle gains more power. Again,
this leads us to be able to construct oracles similar to before, where under
some oracle every problem that is easy to verify is easy to solve, and under
some other oracle this is false. This gives us the same issue as with
relativization. It turns out that every proof technique devised \emph{after} the
discovery of relativization is still vulnerable to algebrization, so once again
this reset the proof technique search.

\section{Computational proof systems}

Alongside using computational models to help us determine how to prove
statements, we can also look at computers as a model of proof themselves. In
this model, we have to shift our thinking a little bit. So far, we have been
thinking of our inputs as an arbitrary piece of text, where every piece of text
has either ``yes'' or ``no'' associated with it. Now, we would like to think of
the input as being some \emph{statement}, where the idea is that the response is
``yes'' if the statement is true, and ``no'' if the statement is false. More
specifically, we will be working to classify all the statements of a given type:
for example, all statements of the form ``$x$ is even'', where $x$ is replaced
with some integer. In this example, we would want our machine to output ``yes''
when we are given an even number, and ``no'' when we are not.

Interactive proofs operate with a pair of the interactive Turing machines
mentioned earlier, passing messages between them. We call the two machines the
``prover'' and the ``verifier''. The two machines are not interachangable---we say
the prover can take as much time as it needs, and only the verifier is required
to be fast. A reasonable response to this would be to wonder: if the prover can
be arbitrarily powerful, why could it not just compute the correct answer itself
and then just send ``yes'' or ``no'' to the verifier? Well, the compromise we
get for giving the prover unlimited power is we lose the prover's
\emph{trustworthiness}. While our proofs still define a correct prover, and our
verifier needs to reliably work correctly when talking with the correct prover,
we also require that our verifier can reliably \emph{reject} any other prover
that may be trying to trick it into giving a ``yes'' answer when it should be
outputting a ``no''.

Something to notice in the last paragraph is that we said ``reliably'', not
``always'' when talking about the verifier's outputs. This is the other
compromise we make with interactive proofs: randomness. Randomness in itself is
not a compromise---we could always ignore the randomness capabilities if it gets
in the way. However, the compromise that comes with randomness is
non-deterministic results. After all, in the real world we only ever get a
``yes'' or ``no'' response, and not any sort of information about how improbable
that result might be. Further, we said before that any input has exactly one
correct response, and so if our results are non-deterministic then at least
\emph{sometimes} our computer must necessarily be wrong in its conclusion. One
might be tempted to work around this by allowing our laptop to use randomness
internally, but say that its \emph{result} must be deterministic. While this
would be nice, it turns out that doing this means our computer is no more
powerful than if we did not use randomness at all. In brief, if the result is
the same no matter what random bits we roll, then if we just replaced every
random roll with a non-random result of, e.g., 4,\footnote{As chosen by fair
  dice roll: \url{https://xkcd.com/221/}.} then we would have a machine that
uses no randomness and yet outputs the same answer as our random machines.

Interactive proofs are one model of a proof system, but it is not the only one.
Probabilistically-checkable proofs (PCPs) are another model, and one that we
will be looking at quite a lot. Here, instead of a verifier that responds to our
requests, we have a static \emph{proof} (often modeled as a particular oracle)
that we can request some limited number of bits from. Similarly to with
interactive proofs, we not only care about what happens when our verifier is
given the proper, ``honest'' proof, but we also want it to reliably be able to
sniff out false proofs of false statements and reject them as well.

\section{Zero-knowledge}

The traditional method of proving a statement to someone else is to reveal
enough information about each step along the way that the provee can recreate
each logical step. This necessarily means revealing lots of additional
information that is not the statement being proven. One thing that
mathematicians have wondered is whether it is possible to prove a statement
\emph{without} revealing any of the information besides the specific statement
to be proven.

Intuitively, it would be reasonable to expect this is impossible. After all, one
definitely cannot just state something and expect people to believe it is a
mathematical truth. However, when we say zero-knowledge, we do not necessarily
mean that \emph{no} knowledge is leaked, just that no complicated (i.e.,
something we can not compute efficiently on our own) information is leaked. In
this model, what we allow for is the verifier to challenge the prover: they can
make requests, with the idea that if the prover is reliably answering them
correctly, then it must be the case that the prover has proven the statement.

Perhaps the most important property of these interactions is that they are
\emph{not} generally acceptable as strict mathematical proofs---for any set of
queries and responses, it is always possible that a sneaky prover could answer
them all correctly by accident. However, for a zero-knowledge proof it must
always be statistically unlikely for a prover to answer correctly, and thus with
enough tests, it will eventually become a near impossibility for any
correctly-answering prover to be lying.

Alongside their mathematical properties, zero-knowledge proofs have many
real-world applications. One common example is that of authentication: in
general, I would like to be able to prove that I have the password for my
account without sending the password publicly. After all, if I sent my password
publicly anyone listening in to my conversations could just write it down, and
then it is no longer much of a password. However, if we have a zero-knowledge
proof that I have my password, it would leak no information about what my
password is, and thus I can log in with peace of mind.

A zero-knowledge requirement can be applied to just about any model of
computation that involves multiple machines communicating in some fashion. To
this end, we will be looking at zero-knowledge versions of nearly every
computational model we discuss in this thesis. While the specifics of how we
define zero-knowledge differ slightly from model to model, the broad strokes are
the same: no information may ever be leaked through the messages between the
machines.

\section{The PCP theorem}

The main way we measure the difficulty of probabilistically-checkable proofs is
through \emph{query complexity}: in brief, how much (or how little) of the proof
we need to look at in order to get a reliable response. In general, we would
like to minimize this---we want to find the most efficient way to answer the
question posed. It turns out, luckily for us, that if a problem is easy to
verify (i.e., it is in the class $\NP$ we mentioned earlier), then the query
complexity required is astonishingly low~\cite{AS98}. More specifically, there
exists a proof for which we only need to read \emph{3 bits} that will allow us
to answer the question reliably~\cite{Has97}. This statement is what we call the
\emph{PCP theorem}.

This result is astonishing, even to some seasoned computer scientists. There are
various proofs of this, including one we will work through in this thesis. While
the proof itself is somewhat complicated, we still replicate it because it will
closely parallel our technique for proving another result later on.

Of course, this is a thesis about \emph{zero-knowledge} PCPs, and so we would be
remiss were we not to look for a zero-knowledge version of the PCP theorem. As
we see later on, there is such a statement for zero-knowledge PCPs~\cite{GOS25}.
While it does not give us the exact constant of 3 (at least, not yet), it does
tell us that there is some constant number of queries that will always work.
While it might be a little disappointing that we do not get an exact number, the
fact that it is constant at all is still a huge result. Remember, we in computer
science like to measure things by asymptotic complexity, so a constant
complexity is as low as we can go.\footnote{Technically, there are sub-constant
  complexities out there, but they would require the number of queries to go
  \emph{down} as the input gets longer, which doesn't really make sense here.}

\chapter{Preliminaries}

\section{Turing machines}

Central to our definitions of complexity is that of a Turing machine. This is
the most common mathematical model of a computer, and is the jumping-off point
for many variants. There are many ways to think of a Turing machine, but the
most common is that of a small machine that can read and write to an
arbitrarily-long ``tape'' according to some finite set of rules. We give a more
formal definition below, and then we will attempt to take this definition into a
more manageable form.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[>=Stealth]
    \node[draw,rectangle,inner ysep=10pt,inner xsep=10pt] (TM) at (-1,0) {control};
    \foreach [count=\i] \c in {1,0,0,1,\textvisiblespace,\textvisiblespace} {
      \pgfmathsetmacro{\xstart}{\i/2};
      \pgfmathsetmacro{\xend}{\xstart+1/2};
      \draw (\xstart,-1) rectangle node[midway] {\c} (\xend,-1.5);
    }
    \path (3.75,-1) rectangle node[midway] {$\cdots$} (4,-1.5);
    \draw[->] (TM) -| (1.25,-1);
  \end{tikzpicture}
  \caption{A Turing machine}\label{fig:tm}
\end{figure}

\begin{defn}[{\cite[Def.\ 3.1]{Sip97}}]\label{def:TM}\index{Turing machine}
  A \emph{Turing machine} (abbreviated TM) is a 7-tuple
  $(Q, \Sigma, \Gamma, \delta, q_{0}, q_{a}, q_{r})$ where $Q$, $\Sigma$, and $\Gamma$ are all finite
  sets, and
  \begin{enumerate}
    \item $Q$ is the set of \emph{states},
    \item $\Sigma$ is the \emph{input alphabet},
    \item $\Gamma$ is the \emph{tape alphabet},
    \item $\delta\colon Q \times \Gamma \rightarrow Q \times \Gamma \times \{L, R\}$ is the \emph{transition function},
    \item $q_{0} \in Q$ is the \emph{start state},
    \item $q_{a} \in Q$ is the \emph{accept state},
    \item $q_{r} \in Q$ is the \emph{reject state}, with $q_{a} \ne q_{r}$.
  \end{enumerate}
\end{defn}

While we have this formalism here as a useful reference, even here we will most
frequently refer to Turing machines in a more intuitionisitc form. There are
several ways we will think about Turing machines.

The first way to think about a Turing machine is as a little computing box with
a tape. We let the box read and write to the tape, and each step it can move the
tape one space in either direction. At some point, the machine can decide it is
done, in which case we say it ``halts''; however it does not necessarily need to
halt. For this paper, we will only think about machines that \emph{do} halt, and
in particular we will care about how many it takes us to get there. Further, we
will use this informalism as a base from which we can define our Turing machine
variants intuitively, without needing to deal with the (potentially extremely
convoluted) formalism.

Another way we think about a Turing machine is as an algorithm. Perhaps the
foundational paper of modern computer science theory, the \emph{Church-Turing
  thesis}~\cite{Tur36}, states that any actually-computable algorithm has an
equivalent Turing machine, and vice versa. We will use this fact liberally; in
many cases we will simply describe an algorithm and not deal with putting it
into the context of a Turing machine. If we have explained the algorithm well
enough that a reader can execute it (as we endeavor to do), then we know a
Turing machine must exist.

\begin{defn}[{\cite[178]{Sip97}}]%
  \label{def:nondeterministic-tm}\index{Turing machine!nondeterministic}
  A \emph{nondeterministic Turing machine} is a Turing machine, but where the
  transition function has signature
  \begin{equation*}
    \delta\colon Q \times \Gamma \rightarrow \mathcal{P}(Q \times \Gamma \times \{L, R\}),
  \end{equation*}
  where $\mathcal{P}(X)$ is the power set of $X$; that is, the set of all subsets of $X$.
\end{defn}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
    level 1/.style={sibling distance=50mm},
    level 2/.style={sibling distance=25mm},
    level 3/.style={sibling distance=15mm},
    edge from parent/.style={draw,-Stealth}
  ]
    \node {Start}
      child {node {}
        child {node {$\times$}}
        child {node {}
          child {node {$\times$}}
          child {node {$\checkmark$}}}}
      child {node {}
        child {node {}
          child {node {$\checkmark$}}}
        child {node {}
          child {node {$\times$}}
          child {node {$\times$}}}};
  \end{tikzpicture}
  \caption{Computation graph of a nondeterministic Turing machine}\label{fig:ntm}
\end{figure}

With a nondeterministic machine, instead of returning a single action, we return
a set of them. The idea is that these represent a collection of possible actions
we could take given the present state. As the computation progresses, these
compound on each other, until every possible branch has reached a terminal
state. At this point, we check if \emph{any} of the branches are in the accept
state, in which case we accept, and reject otherwise. We can see an example of
this in \cref{fig:ntm}. This computation graph would accept since it has two
branches ending in $\checkmark$; contrastingly, if every graph ended in $\times$, then it
would reject.

\begin{defn}\label{def:multitape-tm}\index{Turing machine!multitape}
  A \emph{multitape Turing machine} is a Turing machine, but where the
  transition function has signature
  \begin{equation*}
    \delta\colon Q \times \Gamma^{n} \rightarrow Q \times \Gamma^{n} \times \{L, R\}^{n},
  \end{equation*}
  for some $n \in \mathbb{N}$.
\end{defn}

\begin{defn}\label{def:prob-tm}\index{Turing machine!probabilistic}
  A \emph{probabilistic Turing machine} is a Turing machine, but with two
  transition functions $\delta_{1}$ and $\delta_{2}$, instead of one.
\end{defn}

In each step, instead of applying its sole transition function, a probabilistic
machine flips a coin and then applies either $\delta_{1}$ or $\delta_{2}$, based on its
result. This is the mechanism behind which we can introduce randomness into our
machines. The end result of this is that the result of this machine is not a
fixed answer, but a \emph{random variable}, something we will discuss in more
detail in \cref{sec:statistics}.

\section{Complexity classes}\label{sec:comp-class}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \pgfmathsetmacro{\s}{sqrt(2)/4};
    \draw (0, 0) circle [radius=0.5];
    \node at (0, 0) {$\P$};
    \draw (\s,\s) ellipse [x radius=1,y radius=0.75,rotate=45];
    \node[anchor=south west] at (\s,\s) {$\NP$};
    \draw (-\s,\s) ellipse [x radius=1,y radius=0.75,rotate=-45];
    \node[anchor=south east] at (0.1-\s,\s) {$\BPP$};
    \draw (0, 1) circle [radius=1.5];
    \node at (0, 1.75) {$\PSPACE$};
    \draw (0, 1.5) circle [radius=2];
    \node at (0, 3) {$\EXP$};
    \draw (0, 2) circle [radius=2.5];
    \node at (0, 4) {$\NEXP$};
    \draw (0, 2.5) circle [radius=3];
    \node at (0, 5) {$\RE$};
  \end{tikzpicture}
  \caption{The relationship between the complexity classes for this
    paper}\label{fig:comp-class}
\end{figure}

Complexity classes are the main mechanism we use to think about the hardness of
problems in computer science. We will build complexity classes out of languages,
which have a natural correspondence to decision problems that we will soon see.
To define a language, first we need some new notation.

\begin{defn}\label{def:sigma-star}
  Let $\Sigma$ be a finite set. The set $\Sigma^{*}$ is the set of all tuples with
  elements in $\Sigma$. That is,
  \begin{equation*}
    \Sigma^{*} = \bigcup_{i=1}^{\infty}\Sigma^{i}.
  \end{equation*}
\end{defn}

Next, we can define what a language actually is.

\begin{defn}\label{def:language}\index{language}\index{alphabet}
  Let $\Sigma$ be a finite set. A \emph{language} (over the \emph{alphabet} $\Sigma$) is a
  subset of $\Sigma^{*}$.
\end{defn}

Most frequently, we will be using the alphabet $\{0, 1\}$, as we like to think
of computers as binary machines. However, there will be a few places where we
need to be explicit about our alphabet, and we will be sure to flag those as
they arise.

Important to us is the relationship between languages and decision problems. The
important thing about this correspondence is that once we have it, we no longer
need to care about the difference between the two. That is, anything that we can
say about languages, we can say about decision problems, and vice versa. As
such, for the rest of this thesis we will no longer be differentiating between
the two.

\begin{defn}
  Let $L$ be a language over an alphabet $\Sigma$. The \emph{decision problem
    corresponding to $L$} is the decision problem where the answer of an input
  $x \in \Sigma^{*}$ is true if $x \in L$ and false if $x \notin L$.
\end{defn}

Finally, we get to define a complexity class.

\begin{defn}\index{complexity class}
  A \emph{complexity class} is a set of languages.
\end{defn}

While this formal definition is very broad, in practice every complexity class
we will see is a set of languages grouped by some complexity-theoretic property.
We start with a relatively straightforward example of a complexity class: the
class of languages that a Turing machine can recognize. First, we need to
define what recognition is in order to make a complexity class out of it.

\begin{defn}[{\cite[Def.\ 3.2]{Sip97}}]\label{def:recognition}\index{recognize}
  A language $L$ is \emph{recognized} by a Turing machine $M$ if for all strings
  $s \in L$, $M$ halts in the accept state when given $s$ as input.
\end{defn}

Now, since our complexity classes are about \emph{languages}, we naturally wish
to extend our notion of recognition to a statistic on languages.

\begin{defn}\label{def:turing-recognizable}\index{Turing-recognizable language}
  A language $L$ is \emph{Turing-recognizable} (frequently just
  \emph{recognizable}) if it is recognized by some Turing machine.
\end{defn}

Now that we have a property of languages, it is straightforward for us to turn
it into a complexity class.

\begin{defn}\label{def:re}\index{RE@$\RE$}
  The class $\RE$ is the class of all Turing-recognizable languages.
\end{defn}

For most other classes, we want our Turing machines to halt on \emph{all}
inputs, not just those in the class. From a practical perspective, this is
useful because it tells us that we can be certain about whether any given string
is in the given language. From here on, we will generally care about how much of
some resource our machines take when making their decision, as opposed to
whether or not they can.

\subsection{Time complexity}\index{time complexity}

The most intuitive (and most important) notion of complexity is that of time
complexity. Time complexity is the answer of the question of how long it takes
to solve a problem. We begin with an abstract base for our time classes, and
will then introduce some specific ones that we care about. Before that, though,
we need to talk about \emph{how} we compare times.

For many problems, the length of time taken is not particularly straightforward.
While \emph{in general}, problems will tend to get more difficult to solve as
their inputs get longer, this is not necessarily true for any given input
length. One could imagine a problem where there is a special-case fast algorithm
for inputs of lengths that are a power of $2$ (for example), but any other
length takes much longer to compute. Given that, one might wonder how we would
compare this to a problem that takes slightly less time than the
non-special-case of the previous problem, but does so consistently. This is the
framework to which we will apply big-$O$ notation.

\begin{defn}[{\cite[47]{CLRS}}]\label{def:big-o}\index{O-notation@$O$-notation}
  Let $f, g\colon \mathbb{N} \rightarrow \mathbb{N}$ be functions. Then $g(n) \in O(f(n))$ if and only if
  there exists $c, n_{0} \in \mathbb{N}$ such that $0 \le f(n) \le cg(n)$ for all $n \ge n_{0}$.
\end{defn}

Big-$O$ notation is widely used across computer science to compare attributes of
problems. In general, the way we will think of functions like $f$ and $g$ is
representing the maximum number of steps (or maximum amount of some other
resource) used by our computer when given an input of length $n$. When we think
about functions in this context, $g(x) \in O(f(x))$ translates roughly to ``$g(x)$
is no harder than $f(x)$''. At first, this might seem slightly confusing, since
$2f(x) \in O(f(x))$, but $2f(x)$ definitely represents a harder problem than
$f(x)$. While this is true, it is not harder in a meaningful way to us: scalar
multiples are often not preserved by a change in our computational model, and
thus we want to ignore them whenever possible.

In addition to big-$O$, we will want one more class of functions, this being the
one corresponding to all polynomials. We want this because there is no function
$f$ such that $O(f(n))$ corresponds to exactly the set of polynomials:
$f(n) \in O(f(n))$ always, so $f$ must be a polynomial, but for any $k$,
$n^{k+1} \notin O(n^{k})$. Because of this, we need a separate definition, and cannot
simply reuse our big-$O$.

\begin{defn}\label{def:poly}\index{poly(n)@$\poly(n)$}
  The set $\poly(n)$ is
  \begin{equation*}
    \poly(n) = \bigcup_{i=1}^{\infty}O(n^{k}).
  \end{equation*}
\end{defn}

Now, we can finally begin to start looking at actual complexity classes. We will
start off by showing a general correspondence between complexity classes and
big-$O$ classes of functions. The idea is that the complexity class
corresponding to a function is the set of all languages that are computable in
that time. Because we have multiple definitions of Turing machines, there are
multiple such correspondences. The two we will care about in particular are
those for deterministic and nondeterministic Turing machines.

\begin{defn}[{\cite[Def.\ 1.19]{AB09}}]\label{def:dtime}\index{DTIME@$\DTIME$}
  Let $f\colon \mathbb{N} \rightarrow \mathbb{N}$ be a function. The class $\DTIME(f(n))$ is the class of
  all languages computable by a deterministic Turing machine in $O(f(n))$ steps.
\end{defn}

\begin{defn}[{\cite[Def.\ 2.5]{AB09}}]\label{def:ntime}\index{NTIME@$\NTIME$}
  Let $f\colon \mathbb{N} \rightarrow \mathbb{N}$ be a function. The class $\NTIME(f(n))$ is the class of
  all languages computable by a nondeterministic Turing machine in $O(f(n))$
  steps.
\end{defn}

While $\DTIME$ and $\NTIME$ are useful bases to start from, it is rare that we
deal with these classes directly. Instead, we will deal with some classes with
slightly nicer mathematical properties, which we will build using $\DTIME$ and
$\NTIME$ as a base.

\begin{defn}[{\cite[Def.\ 1.20]{AB09}}]%
  \label{def:p}\index{P@$\P$}\index{polynomial time}
  The complexity class $\P$ is the class
  \[
    \P = \bigcup_{c > 0}\DTIME(n^{c}).
  \]
  If a language is in $\P$, we say it is \emph{polynomial-time}.
\end{defn}

The class $\P$ is perhaps the most important complexity class. Mathematically,
we care about $\P$ because it is closed under composition: a polynomial-time
algorithm iterated a polynomial number of times is still in $\P$. Further, $\P$
turns out to generally be invariant under change of (deterministic) computation
model, which allows us to reason about $\P$ problems easily without needing to
resort to the formal definition of a Turing machine. More philosophically, $\P$
generally represents the set of ``efficient'' algorithms in the real
world.\footnote{It is worth mentioning that this is a \emph{mathematical}
  efficiency---there are plenty of algorithms in $\P$ that a real-world computer
  scientist would never dare to call efficient.} For that matter, in this paper
if we ever refer to an algorithm as ``efficient'' we mean that is
polynomial-time.

\begin{thm}\label{thm:polynomial-is-p}
  The language
  \[
    \{(p, x, y) \mid p \text{ a polynomial and } p(x) = y\}
  \]
  is in $\P$.
\end{thm}

\begin{proof}
  We can calculate whether a string is in this language by calculating $p(x)$
  (which we can do in polynomial time), and then comparing it to $y$.
\end{proof}

As we have defined $\P$ in terms of $\DTIME$, the question arises of whether
there is an equivalent in terms of $\NTIME$. Naturally, there is, and we call it
$\NP$.

\begin{defn}[{\cite[Cor.\ 7.22]{Sip97}}]\label{def:np}\index{NP@$\NP$}
  The complexity class $\NP$ is the class
  \[
    \NP = \bigcup_{c > 0}\NTIME(n^{c}).
  \]
\end{defn}

While this definition demonstrates how $\NP$ is similar to $\P$, there are other
equivalent ones that we can use. In particular, we very often like to think of
$\NP$ in terms of deterministic \emph{verifiers}. Since nondeterministic
machines do not exist in real life, this definition gives a practical meaning to
$\NP$.

By way of an example, we briefly sketch out a very important $\NP$ language.
While this language is important in general and makes for a good example, it is
only tangentially important to the rest of this thesis. For this reason, we will
only be providing an informal sketch of the relevant proof; there are many more
formal treatments, for example in~\cite{Sip97}.

\begin{defn}[{\cite[299]{Sip97}}]\label{def:bool-formula}\index{Boolean formula}
  A \emph{Boolean formula} is any expression where the only operations used are
  the three Boolean operations logical and ($\wedge$), logical or ($\vee$), and logical
  not ($\neg$).
\end{defn}

\begin{defn}[{\cite[299]{Sip97}}]\label{def:sat}\index{SAT@$\SAT$}
  The language $\SAT$ is the language of Boolean formulas with at least one
  assignment of Boolean values to its variables such that the Boolean formula
  evaluates to $1$.
\end{defn}

\begin{thm}\label{thm:sat-is-np}
  The language $\SAT$ is in $\NP$.
\end{thm}

\begin{proof}
  Nondeterministically pick an assignment of values to variables, then evaluate
  the formula. Since evaluating the formula can be done efficiently, then our
  nondeterministic algorithm will complete in polynomial time. Further, at least
  one branch will accept if and only if at least one assignment of values
  results in the formula outputting 1.
\end{proof}

The definition of $\NP$ we gave in \cref{def:np} is not the only useful one.
We have already referred to $\NP$ a few times as ``the set of languages that can
be verified easily'', and the definition we are about to give is the definition
that corresponds to that. Before we do that, however, we we need to define what
it means for a Turing machine to verify a language---after all, so far all we have
seen formally defined is recognition.

\begin{defn}[{\cite[Def.\ 7.18]{Sip97}}]%
  \label{def:verifier}\index{verifier}\index{certificate}
  A \emph{verifier} for a language $L$ is an algorithm $A$ where for any
  $x \in \Sigma^{*}$, $x \in L$ if and only if there exists a $c \in \Sigma^{*}$ (which we call
  the \emph{certificate}) such that $A$ accepts when given $(x, c)$ as input.
\end{defn}

With our definition of a verifier in hand, we are now free to give our alternate
definition of $\NP$.

\begin{thm}[{\cite[Def.\ 7.19]{Sip97}}]\label{thm:np-verifier}
  $\NP$ is exactly the class of all languages verifiable by a polynomial-time
  Turing machine.
\end{thm}

As an example, the language $\SAT$ we defined in \cref{def:sat} can be verified
efficiently, where the certificate is an accepting set of variables. Since we
can evaluate a Boolean formula efficiently, if we already have an accepting set
of variables we can therefore verify it in polynomial time.

The precise relationship between $\P$ and $\NP$ is not very well-known. We know
$\P \subseteq \NP$, as any deterministic algorithm corresponds exactly to a
nondeterministic algorithm of the same speed. However, whether $\P = \NP$ is
still unknown, and one of the largest open problems in complexity theory.

The next step up from polynomial complexities is that of exponential
complexities. For these, instead of having the classes bounded above by a
polynomial, we have the classes bounded above by $2$ to the power of a
polynomial. While we use $2$ as the base, the value of the base turns out not to
matter since for any $a, b > 1$,
\begin{equation}
  a^{n^{c}} = b^{n^{c}\log_{b}(a)} \in O\mleft(b^{n^{c+1}}\mright).
\end{equation}

\begin{defn}[{\cite[\defaultS 2.6.2]{AB09}}]%
  \label{def:exp-nexp}\index{EXP@$\EXP$}\index{NEXP@$\NEXP$}
  The complexity class $\EXP$ is the class
  \[
    \EXP = \bigcup_{c > 0}\DTIME\mleft(2^{n^{k}}\mright).
  \]
  Similarly, the complexity class $\NEXP$ is the class
  \[
    \NEXP = \bigcup_{c > 0}\NTIME\mleft(2^{n^{k}}\mright).
  \]
\end{defn}

It is immediate that $\P \subseteq \EXP$ and $\NP \subseteq \NEXP$ (since the exponential
classes allow the use of more of the same resource). Of slightly less-trivial
interest is the relationship between $\NP$ and $\NEXP$.

\begin{thm}\label{thm:np-exp}
  $\NP \subseteq \EXP$.
\end{thm}

\begin{proof}
  If a nondeterministic machine solves a problem in $p(n)$ steps, it follows
  that the total number of branches is less than $a^{p(n)}$, where $a$ is the
  maximum number of nondeterministic choices we make at any given step. Hence,
  we can simulate the machine deterministically by simply enumerating every
  branch, giving us a total computation time of $p(n)a^{p(n)}$, which is in
  $O(2^{q(n)})$ for some other polynomial $q(n)$. Hence any $\NP$ problem is in
  $\EXP$.
\end{proof}

It is perhaps illustrative to see an example of a problem in $\NEXP$. Here, we
present an language whose relationship to $\NEXP$ is very similar to how $\SAT$
is related to $\NP$. Unlike $\SAT$, we will be seeing a lot of this language
throughout the rest of the thesis, and finding algorithms for it will be a major
part of several upcoming chapters.

\begin{defn}[{\cite[Def.\ 14.1]{CFGS22}}]\label{def:oracle-3sat}\index{O3SAT@$\OSAT$}
  The \emph{oracle 3-satisfiability problem}, denoted $\OSAT$, is the language
  of all triplets $(r, s, B)$, where $r, s \in \mathbb{N}^{+}$ and
  $B\colon \{0, 1\}^{r+3s+3} \rightarrow \{0, 1\}$ a boolean function, such that there
  exists a boolean function $A\colon \{0, 1\}^{s} \rightarrow \{0, 1\}$ having
  the property that for all $z \in \{0, 1\}^{r}$ and
  $b_{1}, b_{2}, b_{3} \in \{0, 1\}^{s}$,
  \begin{equation*}
    B(z, b_{1}, b_{2}, b_{3}, A(b_{1}), A(b_{2}), A(b_{3})) = 1.
  \end{equation*}
\end{defn}

\begin{thm}\label{thm:o3sat-in-nexp}
  $\OSAT \in \NEXP$.
\end{thm}

\begin{proof}
  We present the following non-deterministic algorithm to determine if
  $(r, s, B) \in \OSAT$:

  \begin{algorithm}[H]
    \KwIn{A triplet $(r, s, B)$}
    \KwOut{Whether or not $(r, s, B) \in \OSAT$}
    Nondeterministically choose a function $A\colon \{0, 1\}^{s} \rightarrow \{0, 1\}$\;
    \For{$z \in \{0, 1\}^{r}$}{
      \For{$b_{1}, b_{2}, b_{3} \in \{0, 1\}^{s}$}{
        Compute $B(z, b_{1}, b_{2}, b_{3}, A(b_{1}), A(b_{2}), A(b_{3}))$\;\nllabel{line:compute-b}
        \If{the above is not $1$}{
          \Reject\;
        }
      }
    }
    \Accept\;
    \caption{A $\NEXP$-time algorithm for determining $\OSAT$}\label{alg:osat-nexp}
  \end{algorithm}

  First, we need to show that \cref{alg:osat-nexp} is in $\NEXP$.
  Nondeterministically choosing a function from $\{0, 1\}^{s}$ can be done in
  time $2^{s}$; and the two loops will run a total of $2^{r}2^{s}$ times,
  respectively. Computation of a function can be done in polynomial time
  relative to its length; hence the runtime of this function is in exponential
  time relative to $r + s$.

  One might be tempted to think that since we are given a function $B$ as input,
  that our function $A$ can be no longer than $\poly(\abs{B})$, but this is not
  necessarily true. We are given $B$ in $3\SAT$ form, and thus there are
  expressions of $B$ that are polynomial with respect to $r + s$. Despite this,
  there are polynomial-length $B$ instances whose $A$ is \emph{not} polynomial
  in length (since that is an arbitrary function); since runtime complexity is
  about the worst case there thus exist inputs that cannot be computed in
  polynomial time relative to their length.

  Next, we want to show that \cref{alg:osat-nexp} actually recognizes $\OSAT$.
  If $(r, s, B) \in \OSAT$, then there exists a function $A$ such that for any
  values $z$ and $b_{i}$, $B$ evaluates to $1$. Since we nondeterministically
  choose $A$, one of our nondeterministic branches will pick it, and in that
  branch every one of the computations in line~\ref{line:compute-b} will be $1$.
  Thus, that branch will accept and therefore so will the entire machine.

  If $(r, s, B) \notin \OSAT$, then no matter what nondeterministic branch we are on,
  the value of $A$ means that the computation in line~\ref{line:compute-b} will
  be $0$. Hence, every branch will reject and thus \cref{alg:osat-nexp} as a
  whole will.

  Since \cref{alg:osat-nexp} runs in exponential time and will accept if and
  only if its input is in $\OSAT$, it follows that $\OSAT \in \NEXP$.
\end{proof}

\subsection{Space complexity}\index{space complexity}

Similar to time complexity, space complexity is the question of how much space
on its memory tape a machine needs in order to compute a problem. In many ways,
our definitions of space complexity are analogous to those for time complexity
that we have already defined. In particular, $\DSPACE$ will correspond nicely to
$\DTIME$, and $\NSPACE$ to $\NTIME$.

\begin{defn}[{\cite[Def.\ 4.1]{AB09}}]%
  \label{def:dspace-nspace}\index{DSPACE@$\DSPACE$}\index{NSPACE@$\NSPACE$}
  Let $f\colon \mathbb{N} \rightarrow \mathbb{N}$ be a function. A language $L$ is in $\DSPACE(f(n))$ if
  there exists a deterministic Turing machine $M$ such that the number of
  locations on the tape that are non-blank at some point during the execution of
  $M$ is in $O(f(n))$.

  Similarly, a language $L$ is in $\NSPACE(f(n))$ if there exists a
  nondeterministic Turing machine $M$ such that the number of locations on the
  tape that are non-blank at some point during the execution of $M$ is in
  $O(f(n))$.
\end{defn}

Analogously to $\P$ and $\NP$, our two main classes of space complexity are
$\PSPACE$ and $\NPSPACE$.

\begin{defn}[{\cite[Def.\ 4.5]{AB09}}]%
  \label{def:pspace-npspace}\index{PSPACE@$\PSPACE$}\index{NPSPACE@$\NPSPACE$}
  The complexity class $\PSPACE$ is the class
  \[
    \PSPACE = \bigcup_{c > 0}\DSPACE(n^{c}),
  \]
  and the complexity class $\NPSPACE$ is the class
  \[
    \NPSPACE = \bigcup_{c > 0}\NSPACE(n^{c}).
  \]
\end{defn}

Unlike with $\P$ and $\NP$, the relationship between $\PSPACE$ and $\NPSPACE$ is
well known. Due to the complexity of the proof of the theorem, we will not prove
it here, as it is mostly not relevant to what we will be doing.

\begin{thm}[{Savitch's theorem;~\cite{Sav70}}]\label{thm:savitch}\index{Savitch's theorem}
  $\PSPACE = \NPSPACE$.
\end{thm}

Upon seeing this, one might ask why it is that we believe $\P \ne \NP$ if we know
that $\PSPACE = \NPSPACE$, given they are defined analogously. The answer to
this question boils down to the fact that we are able to reuse space, while we
are not able to reuse time. Space on the tape that is no longer needed can be
overwritten, while time that is no longer needed is gone forever.

Since $\PSPACE$ and $\NPSPACE$ are equal classes, it is relatively rare to see
$\NPSPACE$ referred to. Here, we will only refer to it when it makes a class
relationship clearer; most frequently when comparing $\NPSPACE$ to some other
nondeterministic class.

\subsection{Completeness}

Even within a complexity class, not all problems are created equal. The notion
of \emph{completeness} gives us a mathematically-rigorous way to talk about
which problems in a class are the hardest. Since putting upper bounds on hard
problems naturally puts those same bounds on any easier problems, complete
problems can be useful in reasoning about the relationship between complexity
classes.

\begin{defn}[{\cite[Def.\ 7.29]{Sip97}}]\label{def:p-reduction}\index{polynomial-time reduction}
  A language $A$ is \emph{polynomial-time reducible} to a language $B$ if a
  polynomial-time computable function $f\colon \Sigma^{*} \rightarrow \Sigma^{*}$ exists
  such that for all $w \in \Sigma^{*}$, $w \in A$ if and only if $f(w) \in B$.
\end{defn}

Polynomial-time reductions are important because they give us a way to say that
$A$ is \emph{no harder} than $B$. In particular, if we have an algorithm $M$
that determines $B$, we can construct the following algorithm that determines
$A$ with only a polynomial amount of additional work:

\begin{algorithm}[H]
  \KwIn{A string $w \in \Sigma^{*}$}
  \KwOut{Whether $w \in A$}
  Compute $f(w)$\;
  Use $M$ to check whether $f(w) \in B$\;
  \KwRet{the result of $M$}\;
  \caption{An algorithm to reduce $A$ to $B$}
\end{algorithm}

\begin{defn}[{\cite[Def.\ 7.34]{Sip97}}]\label{def:np-complete}\index{NP-complete@$\NP$-complete}
  A language $L$ is $\NP$-complete if $L \in \NP$ and every $A \in \NP$ is
  polynomial-time reducible to $L$.
\end{defn}

This is a practical use of our polynomial-time reductions: since an
$\NP$-complete language has a reduction from every other language in $\NP$, it
follows that it is \emph{at least as hard} as any other language in $\NP$. Of
particular interest to complexity theorists is the fact that $\P = \NP$ if and
only if \emph{any} $\NP$-complete language is in $\P$.

\begin{example}\label{ex:sat-is-complete}\index{Cook-Levin theorem}
  A famous result of Cook~\cite{Cook71}, also proved around the same time by
  Levin and thus called the \emph{Cook-Levin theorem}, is that the $\SAT$
  problem we defined earlier in \cref{def:sat} is $\NP$-complete.
\end{example}

The notion of completeness is very important to complexity theorists. Since
these are the ``hardest'' problems in $\NP$, this means that if we can do
anything interesting to an $\NP$-complete problem, we can leverage these
reductions to do that interesting thing to \emph{any} other problem in $\NP$
with only a little (i.e.\ polynomial) more effort. This will come in especially
handy when we want to prove that complexity classes are equal or that $\NP$ is a
subset of some other complexity class---since most complexity classes allow for
things to change polynomially, we only need to prove that a single
$\NP$-complete element is in the other class for the subset relation to follow.

% TODO: Cite NEXP not being closed under EXP reductions
Along with completeness for $\NP$, we have a notion of completeness for $\NEXP$.
While you might expect that the reducibility constraints might loosen (i.e.\
allow more complex reductions) since $\NEXP$ is more complex for $\NP$, but this
turns out not to be the case. In particular, while it might initially seem
logical to allow for $\EXP$-reductions, it turns out that $\NEXP$ is not closed
under $\EXP$-reductions, which makes a notion of completeness challenging.
Despite this, we can still learn interesting things about $\NEXP$ by studying
completeness under polynomial reductions.

\begin{defn}\label{def:nexp-complete}\index{NEXP-complete@$\NEXP$-complete}
  A language $L$ is $\NEXP$-complete if $L \in \NEXP$ and every $A \in \NEXP$ is
  polynomial-time reducible to $\NEXP$.
\end{defn}

$\NEXP$-completeness has many of the same nice properties of $\NP$-completeness.
Of particular interest to us will again be the ease with which
$\NEXP$-completeness allows us to determine subset relations, simply by proving
the inclusion of a single complete language.

For our language $\OSAT$ we defined earlier (in \cref{def:oracle-3sat}), we
actually have an even \emph{stronger} notion of completeness, with a
polynomial-time reduction for arbitrary time functions $T(n)$. We will get the
standard $\NEXP$-completeness of $\OSAT$ as a corollary, but we will actually
find the stronger characterization here useful in later chapters.

\begin{thm}[{Cook-Levin}]\label{thm:cook-levin-general}
  Let $M$ be a $\NTIME(T(n))$ Turing machine, with $T \in \Omega(n)$. Then, there
  exists a polynomial-time reduction $R_{M}$ such that for any input
  $x \in \{0, 1\}^{n}$, $R_{m}(x) \in \OSAT$ if and only if there exists a $w$ with
  $M(x, w) = 1$. Furthermore, $R_{m}(x)$ outputs a formula in $O(\log(T(n)))$
  variables, with size $\poly(n, \log(T(n)))$.
\end{thm}

\begin{cor}[{\cite[Proposition 4.2]{BFL90}}]\label{thm:o3sat-nexp-complete}
  The language $\OSAT$ is $\NEXP$-complete.
\end{cor}

\begin{proof}
  Let $T(n) = 2^{n^{k}}$ for some $k$. Then, the output formula of $R_{m}(x)$ is
  a formula in $O(\log(2^{n^{k}})) = O(n^{k})$ variables, with size
  \begin{equation}
    \poly(n, \log(2^{n^{k}})) = \poly(n^{k}) = \poly(n).
  \end{equation}
  This is the normal definition of $\OSAT$; hence it is $\NEXP$-complete.
\end{proof}

Just as we have $\NP$-completeness and $\NEXP$-completeness for time complexity,
we also have notions of completeness for space complexity. Since
$\PSPACE = \NPSPACE$, instead of calling the class $\NPSPACE$-complete, we call
it $\PSPACE$-complete.

\begin{defn}[{\cite[Def.\ 8.8]{Sip97}}]\label{def:pspace-complete}\index{PSPACE-complete@$\PSPACE$-complete}
  A language $L$ is $\PSPACE$-complete if $L \in \PSPACE$ and every $A \in \PSPACE$
  is polynomial-time reducible to $L$.
\end{defn}

While this definition is mostly analagous to that of $\NP$-completeness, one
might wonder why we use a time complexity for our reduction when $\PSPACE$ is a
space-complexity class. This is because if we were to use space complexity, we
would want to use $\PSPACE$-reductions, but that would make every language in
$\PSPACE$ trivially $\PSPACE$-complete. Since that is not a useful definition,
we instead restrict ourselves to polynomial-time reductions.

\subsection{Randomized complexity}\label{sec:rand-complexity}

All the complexity classes we have seen so far are \emph{non-randomized}: they
do not allow a Turing machine to consult any source of randomness. In these
models, a Turing machine's output is always the same when it is given the same
input. However, there exist Turing machines that can consult a random-bit
generator. Because of their different capabilities, we can define separate
complexity classes to the languages corresponding to those machines.

The main probabilistic class we care about is $\BPP$ (short for ``bounded-error
probabilistic polynomial''). This class is similar to $\P$ in that it requires
its machines to run in polynomial time, but it has one large difference. Because
these are probabilistic machines, we want to be able to leverage that
randomness. As such, we do not insist that we \emph{always} accept when given a
string in the language, only most of the time. So long as we accept with a
probability noticeably greater than $1/2$ (and reject similarly), we can always
boost the probability arbitrarily high by repeating the simulation multiple
times and taking the majority vote.

\begin{defn}\label{def:bpp}\index{BPP@$\BPP$}
  A language $L$ is in $\BPP$ if there exists a probabilistic Turing machine $M$
  such that
  \begin{enumerate}
    \item $M$ runs in polynomial time,
    \item for all $x \in L$, $M$ accepts $x$ with probability at least $2/3$,
    \item for all $x \notin L$, $M$ rejects $x$ with probability at least $2/3$.
  \end{enumerate}
\end{defn}

Next, it is important to see how $\BPP$ relates to the other complexity classes.
We have also included $\BPP$ on the Venn diagram in \cref{fig:ip-class-venn}
earlier. It is important to note that neither of the following inclusions are
known to be strict.

\begin{thm}\label{thm:p-subset-bpp}
  $\P \subseteq \BPP$.
\end{thm}

\begin{proof}
  A deterministic Turing machine is equivalent to a randomized machine that
  never consults its oracle. In this way, a polynomial-time Turing machine
  fulfills all the requirements of \cref{def:bpp}: it runs in polynomial time,
  it accepts each $x \in L$ with probability $1 > 2/3$, and it rejects each
  $x \notin L$ with probability $1 > 2/3$.
\end{proof}

\begin{thm}\label{thm:bpp-subset-pspace}
  $\BPP \subseteq \PSPACE$.
\end{thm}

\begin{proof}
  Consider a $\BPP$ machine $M$ that reads $r$ random bits on input $x$. Then,
  consider a machine $M'$ that loops through every $\rho \in \{0, 1\}^{r}$ and
  simulates $M$ on input $x$ and random coins $r$, finally accepting if at least
  $2/3$ of the instances accepted and rejecting if $2/3$ of the instances
  rejected. This machine is deterministic and would accept the same language as
  $M$, but it would run in polynomial space, since simulating $M$ is polynomial
  space and after each iteration we can throw out the scratchwork, only keeping
  the total accept/reject count.
\end{proof}

\subsection{Impagliazzo's five worlds}\label{sec:five-worlds}

The question of $\P$ vs $\NP$ is a large and important one, and one that is not
particularly close to being resolved. However, the possible resolutions to this
central dilemma are more nuanced than just ``yes'' or ``no''. In particular,
there are many, many results that depend on the answer to the $\P$ vs $\NP$
question, either a positive result or a negative one.

A 1995 paper by Impagliazzo~\cite{Imp95} talks about this problem in more depth.
In this paper, he identifies five ``worlds'', ranging the full spectrum from
``$\P = \NP$ efficiently'' all the way to ``$\P \ne \NP$ and cryptography is
possible in full''. This is relevant to us because we will be working with
zero-knowledge a lot for this paper, and as we will see in
\cref{sec:zero-knowledge}, several important building blocks of zero-knowledge
proofs are impossible in many of Impagliazzo's worlds. For this reason, for the
rest of this paper we will be assuming we are in ``cryptomania'', his most
cryptographic world. The most important assumption we will be pulling from this,
besides that $\P \ne \NP$, is that one-way functions (which we will be defining in
more depth in \cref{sec:commitment-scheme}) exist.

\section{Polynomials}\label{sec:polynomial}

Many of the theorems we will be working with will make heavy use of various
properties of polynomials. We will prove several of those here.

\begin{defn}[{\cite{Knu92}}]\label{def:iverson-bracket}\index{Iverson bracket}
  Let $P$ be a mathematical statement. The function $[P]$ is the the function
  \begin{equation}\label{eqn:iverson-bracket}
    [P] = \begin{cases}
      1 & P \text{ is true} \\
      0 & \text{otherwise.}
    \end{cases}
  \end{equation}
  This is called the \emph{Iverson bracket}, after its inventor Kenneth Iverson,
  who originally included it in the programming language APL\footnote{The
    original notation used parentheses, but square brackets are much less
    ambiguous, so that has become the standard and what we will use
    here.}~\cite[11]{APL}.
\end{defn}

As an example, we can use the Iverson bracket to define the Kronecker delta
function\index{Kronecker delta function} as
\[
  \delta_{ij} = [i = j].
\]

Traditionally, many people think of polynomials as being functions over the real
numbers: a polynomial takes a real number as input and gives a real number as
output. We would like to generalize this notion to be as broad as possible:
what kind of other mathematical things can we make a polynomial out of? To some
extent, all we really need to make a polynomial out of something is to allow
ourselves to do addition and multiplication on it.

\begin{defn}[{\cite[Def.\ 2.6.1]{Swan21}}]\label{def:field}\index{field}
  A \emph{field} is a set $\mathbb{F}$ with functions
  $+, \cdot\colon \mathbb{F} \rightarrow \mathbb{F}$ such that there exist elements $0, 1 \in F$
  and for all $a, b, c \in F$,
  \begin{enumerate}
    \item $a + 0 = a = 0 + a$.
    \item $a + (b + c) = (a + b) + c$.
    \item $a + b = b + a$.
    \item $a \cdot 1 = a = 1 \cdot a$.
    \item $a + (b \cdot c) = a \cdot b + a \cdot c$.
    \item $a \cdot (b \cdot c) = (a \cdot b) \cdot c$.
    \item $a \cdot b = b \cdot a$.
    \item There exists $d \in F$ such that $a + d = d + a = 0$.
    \item If $a \ne 0$, there exists $d \in F$ such that $a \cdot d = d \cdot a = 1$.
    \item $0 \ne 1$.
  \end{enumerate}
\end{defn}

These ten conditions presented above are what we need for addition and
multiplication to ``work nicely'', so to speak. The real numbers are our
favorite example of a field, but there exist many others. Because we generally
like finite sets in computer science, we will often be restricting ourselves to
work with specifically finite fields. Our favorite examples of these are the
modular-arithmetic fields $\mathbb{Z}/p\mathbb{Z}$, where $p$ is prime. These work like the
integers, but with addition and multiplication done mod $p$ instead of freely.

Much of our work will deal with multivariate polynomials. For a given field
$\mathbb{F}$, we will denote the set of $m$-variable polynomials over
$\mathbb{F}$ with $\mathbb{F}[x_{1, \ldots, m}]$. The terminology of multivariate
polynomials generally coincides with that of single-variable polynomials, but
not always. Hence, we will introduce some important multivariable-specific
terminology and definitions here. First up, perhaps the most important property
of a polynomial is its degree. Degree is well-defined for multivariate
polynomials just as it is for monovariate ones, but there is a second,
closely-related definition we call \emph{multidegree}.

\begin{defn}[{\cite[8]{AW09}}]\label{def:mdeg}\index{multidegree}
  The \emph{multidegree} of a multivariate polynomial $p$, written $\mdeg(d)$,
  is the maximum degree of any variable $x_{i}$ of $p$.

  We denote by $\mathbb{F}[x_{1, \ldots, m}^{\le d}]$ the subset of
  $\mathbb{F}[x_{1, \ldots, m}]$ of polynomials with multidegree at most $d$.
  Similarly, if we want to refer to polynomials with degree at most $d$, we will
  write $\mathbb{F}^{\le d}[x_{1, \ldots, m}]$.
\end{defn}

It is worth noting that for monovariate polynomials, multidegree and degree
coincide. The difference between multidegree and degree is subtle, but
important. To help make the difference more clear, we will often refer to degree
as \emph{total degree}\index{total degree} instead of just degree. We shall
illustrate the difference with a simple example. We also need two special cases
of these polynomials, which we will want to quickly be able to reference
throughout the paper.

\begin{defn}[{\cite[8]{AW09}}]\label{def:mlin}\index{multilinear}\index{multiquadratic}
  A polynomial is \emph{multilinear} if it has multidegree at most 1. Similarly,
  a polynomial is \emph{multiquadratic} if it has multidegree at most 2.
\end{defn}

Consider the polynomial $x_{1}^{2}x_{2} + x_{2}^{2}$. The multidegree of this
polynomial is 2, while its degree is 3. The polynomial
$x_{1}x_{2} + 4x_{2}x_{3} + x_{1}x_{2}x_{3}$ is multilinear. The polynomial
$x_{1}^{2}x_{2}x_{3} - 2x_{1}x_{3} + 3x_{2}^{2}$ is multiquadratic.

From here, we need to define the notion of an \emph{extension polynomial}. This
gives the ability to take an arbitrary multivariate function defined on a subset
of a field and extend it to be a multivariate polynomial over the \emph{whole}
field.

\begin{defn}[{\cite[8]{AW09}}]\label{def:ext-poly}\index{extension polynomial}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f\colon H^{m} \rightarrow \mathbb{F}$ be a function. An \emph{extension polynomial} of
  $f$ is any polynomial $f' \in \mathbb{F}[x_{1, \ldots, m}]$ such that $f(h) = f'(h)$
  for all $h \in H$.
\end{defn}

\begin{example}
  Define $H = \{0, 1\}^{3} \subseteq \mathbb{R}^{3}$. Further define
  \begin{align*}
    f\colon H^{3} &\rightarrow \mathbb{R} \\
    (a, b, c) &\mapsto a \oplus b \oplus c,
  \end{align*}
  where $\oplus$ is the xor function; equivalently addition mod $2$. Then one
  extension polynomial of $f$ is the function
  \begin{equation*}
    f'(x, y, z) = xyz - (x - y)(y - z)(z - x).
  \end{equation*}
  A second extension polynomial of $f$ is the function
  \begin{equation*}
    f''(x, y, z) = x + y + z - 2xy - 2yz - 2xz + 4xyz.
  \end{equation*}
\end{example}

There are (at least) two important things to be gleaned from this example.
First, extension polynomials are not unique: $f'$ and $f''$ are not equal to
each other (they are not even of the same multidegree). Second, $f''$ is in fact
multilinear, which might be a somewhat lower multidegree than expected given we
need to interpolate 8 different points. It turns out that this is not
particularly unusual: while our choice of $H$ is particularly nice, functions
from this particular $H$ still happen to be quite nice in general. Even for
less-nice values of $H$, extension polynomials need only to be of a surprisingly
low multidegree. Since polynomials of lower degree are generally easier to
compute, we would like to see exactly what these low-degree extension
polynomials look like and how they work.

\begin{defn}[{\cite[\defaultS 5.1]{CFGS22}}]\label{def:low-deg-ext}\index{low-degree extension}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f\colon H^{m} \rightarrow \mathbb{F}$ be a function. A \emph{low-degree extension}
  $\tilde{f}$ of $f$ is an extension of $f$ with multidegree at most
  $\abs{H} - 1$.
\end{defn}

It turns out that this is the minimum possible degree of any extension
polynomial. Further, it turns out that for any $f$, there is a \emph{unique}
low-degree extension. Neither of these statements are particularly important for
our further work, so we will not endeavor to prove them here. Something of
practical use to us is an explicit formula for the low-degree extension, which
we shall now calculate.

\begin{thm}[{\cite[\defaultS 5.1]{CFGS22}}]\label{thm:low-deg-ext-exists}
  Let $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$, $m \in \mathbb{N}$ a number, and
  $f\colon H^{m} \rightarrow \mathbb{F}$. Then a low-degree extension $\tilde{f}$ of $f$
  is the function
  \begin{equation}
    \tilde{f}(x) = \sum_{\beta \in H^{m}}\delta_{\beta}(x)f(\beta),
  \end{equation}
  where $\delta$ is the polynomial
  \begin{equation}\label{eqn:delta-poly}
    \delta_{x}(y) = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
        \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
      \mright)\mright).
  \end{equation}
\end{thm}

\begin{proof}
  First, we must show $\tilde{f}$ has multidegree $\abs{H} - 1$. First, note
  that $\tilde{f}$ is a linear combination of some $\delta_{x}$es; hence asking about
  the multidegree of $\tilde{f}$ is really just asking about the multidegree of
  $\delta_{x}$. Looking at $\delta_{x}$, the innermost product has $\abs{H} - 1$ terms,
  each with the same $y_{i}$; thus those terms have multidegree $\abs{H} - 1$.
  Summing terms preserves their multidegree, and the outer product iterates over
  the variables, thus it preserves multidegree as well. Thus, $\delta_{x}$ has
  multidegree $\abs{H} - 1$.

  To understand why $\tilde{f}(x)$ agrees with $f(x)$ on $H$, we first should
  look at $\delta_{\beta}(x)$. In particular, for all $x, y \in H^{m}$,
  \begin{equation}\label{eqn:delta-is-delta}
    \delta_{y}(x) = [x = y] = \delta_{xy}.
  \end{equation}
  This can be shown through some algebra which we have worked through in full
  detail in \cref{app:ext-poly}. This is the reason why we have named the
  polynomial in \cref{eqn:delta-poly} as we have; it functions as the Kronecker
  delta function over the set $H^{m}$.

  Taking the above statement, we get that for all $x \in H^{m}$, the only nonzero
  term of $\tilde{f}(x)$ is the term where $\beta = x$; thus $\tilde{f}(x) = f(x)$.
  Hence, $\tilde{f}$ is a low-degree extension of $f$.
\end{proof}

Of particular interest to us will be the case of low-degree extensions where
$H = \{0, 1\}$. Since every field contains both $0$ and $1$, this will allow us
to construct a set consisting of an extension for \emph{every} field. Further,
since $\abs{H} = 2$ here, it means our low-degree extensions will be
multilinear. Not only do we thus constrain our polynomial to have a very low
multidegree, the $\delta$ function also dramatically simplifies in this case, which
makes it much easier to reason about.

\begin{cor}[{\cite[\defaultS 4.1]{AW09}}]\label{cor:low-degree-boolean}
  Let $\mathbb{F}$ be a finite field, $m \in \mathbb{N}$ a number, and
  $f\colon \{0, 1\}^{m} \rightarrow \mathbb{F}$. Then
  \begin{equation}\label{eqn:low-deg-ext-small}
    \tilde{f}(x) = \sum_{\beta \in \{0, 1\}^{m}}\delta_{\beta}(x)f(\beta)
  \end{equation}
  is a low-degree extension of $f$, where $\delta$ is the polynomial
  \begin{equation}\label{eqn:delta-poly-small}
    \delta_{y}(x) = \mleft(\prod_{i:y_{i}=1}x_{i}\mright)\mleft(\prod_{i:y_{i}=0}(1 - x_{i})\mright).
  \end{equation}
\end{cor}

As we can see, the form of $\delta$ in \cref{eqn:delta-poly-small} is much more
manageable than the form in \cref{eqn:delta-poly}, and it is perhaps more
immediately apparent here why $\delta$ has the property it does. Further, since this
equation has no division, it turns out that it is valid for arbitrary
(non-trivial) rings, while the more complex equation is only valid for fields.
We show the algebra that brings us from the first to the second in
\cref{app:ext-poly}.

The form of $\delta_{y}$ defined in \cref{eqn:delta-poly-small} has further use to us
than just being simpler. In particular, these $\delta_{y}$ form a basis of
multilinear polynomials (and hence a generating set for the ring of all
polynomials). This is a particularly useful basis because it allows us to reason
about multilinear polynomials based solely on their outcomes on the Boolean
cube.\footnote{As an aside, this fact provides a relatively slick proof of the
  special case of our unproven statement earlier that low-degree extensions are
  both of minimal degree and unique.}

\begin{thm}[{\cite[\defaultS 4.1]{AW09}}]\label{thm:delta-poly-basis}
  For any field $\mathbb{F}$, the set $\{\delta_{x} \mid x \in \{0, 1\}^{n}\}$ forms a
  basis for the vector space of multilinear polynomials
  $\mathbb{F}^{n} \rightarrow \mathbb{F}$.
\end{thm}

\begin{proof}
  Since $\delta_{y}(x) = 0$ for all $y \ne x \in \{0, 1\}^{n}$, it follows that the only
  way to get
  \begin{equation}
    \sum_{y \in \{0, 1\}^{n}}a_{y}\delta_{y} = 0
  \end{equation}
  is to have each $a_{y} = 0$. Hence the set of $\delta_{x}$ is linearly independent.
  Further, the vector space of multilinear polynomials has $2^{n}$ dimensions;
  since there are $2^{n}$ distinct $\delta_{x}$ polynomials, it follows that they
  form a basis.
\end{proof}

Now, we can use this fact to prove some cases where our low-degree extensions
turn out to have a particularly low degree. Unfortunately, these do have a lot
of qualifiers to them, but they will be useful in later theorems (in particular
\cref{lem:multiquad-adversary}).

\begin{lemma}[{\cite[Lemma 4.2]{AW09}}]\label{thm:multilin-extension}
  Let $\mathbb{F}$ be a field and $Y \subseteq \mathbb{F}^{n}$ be a set of $t$ points.
  Then, there exists a multilinear polynomial $m: \mathbb{F}^{m} \rightarrow \mathbb{F}$
  such that
  \begin{enumerate}
    \item $m(y) = 0$ for all $y \in Y$, and
    \item $m(z) = 1$ for at least $2^{n} - t$ Boolean points $z$.
  \end{enumerate}
\end{lemma}

This lemma should look similar to \cref{cor:low-degree-boolean}, but it does
have a few key differences. The main difference is that in that lemma,
\emph{every} point we care about is in $\{0, 1\}^{n}$, but here the points in
$Y$ (that we can pick) are not necessarily Boolean. The compromise is that we do
not get to pick the $z$ in the condition; we know how many of them there will
be, but not necessarily what they are.\footnote{Until we actually construct the
  polynomial, that is.}

\begin{proof}
  As per \cref{thm:delta-poly-basis}, we know that we can write
  \begin{equation}
    m(x) = \sum_{z \in \{0, 1\}^{n}}m_{z}\delta_{z}(x)
  \end{equation}
  for some constants $m_{z} \in \mathbb{F}$. Using this, for each $y_{i} \in Y$ we
  can write $m(y_{i}) = 0$ as a linear equation relating the $m_{z}$. Since
  there are $2^{n}$ $m_{z}$ and $t$ equations, it follows that our solution
  space has dimension $2^{n} - t$. Hence, we can find a solution to these
  equations with at least $2^{n} - t$ coefficients equal to $1$. For any $z$
  where $m_{z} = 1$, it follows that $m(z) = 1$; thus our conditions hold.
\end{proof}

We can use this proof to give us another polynomial construction. For this,
after we pick our $y$, instead of getting $2^{n} - t$ points in $\{0, 1\}^{n}$
to be $1$, we get exactly one of them equal to $1$ and then \emph{every} other
Boolean point evaluates to $0$. We will be using this as an ``adversary''
polynomial: the idea is \emph{almost} everywhere relevant to us it evaluates to
$0$, but there is a single tricky Boolean point where it evaluates to $1$.

\begin{lemma}[{\cite[Lemma 4.3]{AW09}}]\label{thm:multiquad-extension}
  Let $\mathbb{F}$ be a field and $Y \subseteq \mathbb{F}^{n}$ be a set of $t$ points
  $y_{1}, \ldots, y_{t}$. Then for at least $2^{n} - t$ Boolean points
  $w \in \{0, 1\}^{n}$, there exists a multiquadratic extension polynomial
  $p\colon \mathbb{F}^{n} \rightarrow \mathbb{F}$ such that
  \begin{enumerate}
    \item $p(y_{i}) = 0$ for all $y_{i} \in Y$,
    \item $p(w) = 1$,
    \item $p(z) = 0$ for all Boolean $z \ne w$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Define $m$ to be the multilinear polynomial from
  \cref{thm:multilin-extension}. Let $w \in \{0, 1\}^{n}$ be any of the
  $2^{n} - t$ such points where $m(w) = 1$. Then, define
  \begin{equation}
    p(x) = m(x)\delta_{w}(x),
  \end{equation}
  where $\delta$ is the multilinear polynomial from \cref{eqn:delta-poly-small}.
  Since $p(x)$ is the product of two multilinear polynomials, it is quadratic;
  from the definition of $\delta$ it is equal to $0$ for all
  $z \in \{0, 1\}^{n} \setminus \{w\}$, and from the definition of $m$ it is equal to $0$
  for all $y_{i} \in Y$.
\end{proof}

Now that we have it for one polynomial, we would like to extend this to a
collection of polynomials. It may be a little strange to be looking at a
collection of polynomials over different fields, but this construction will come
in useful later on. In particular, keep this in mind when we define
\emph{extension oracles} in \cref{def:ext-oracle}; there, we again will be
dealing with functions over a collection of fields.

\begin{lemma}[{\cite[Lemma 4.5]{AW09}}]\label{lem:multiquad-adversary}
  Let $\mathcal{F}$ be a collection of fields. Let $f\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ be a
  Boolean function, and for every $\mathbb{F} \in \mathcal{F}$, let
  $p_{\mathbb{F}}\colon \mathbb{F}^{n} \rightarrow \mathbb{F}$ be a multiquadratic
  polynomial over $\mathbb{F}$ extending $f$. Also let
  $\mathcal{Y}_{\mathbb{F}} \in \mathbb{F}^{n}$ for each $\mathbb{F} \in \mathcal{F}$, and define
  $t = \sum_{\mathbb{F} \in \mathcal{F}}\abs{\mathcal{Y}_{\mathbb{F}}}$.

  Then, there exists a subset $B \subseteq \{0, 1\}^{n}$, with $\abs{B} \le t$, such that
  for all Boolean functions $f'\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ that agree with
  $f$ on $B$, there exist multiquadratic polynomials
  $p_{\mathbb{F}}'\colon \mathbb{F}_{n} \rightarrow \mathbb{F}$ (one for each
  $\mathbb{F} \in \mathcal{F}$) such that
  \begin{enumerate}
    \item $p_{\mathbb{F}}'$ extends $f'$, and
    \item $p_{\mathbb{F}}'(y) = p_{\mathbb{F}}(y)$ for all $y \in \mathcal{Y}_{\mathbb{F}}$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Call $z \in \{0, 1\}^{n}$ \emph{good} if for every $\mathbb{F} \in \mathcal{F}$ there exists
  a multiquadratic polynomial
  $u_{\mathbb{F},z}\colon \mathbb{F}^{n} \rightarrow \mathbb{F}$ such that
  \begin{enumerate}[label=(\alph*)]
    \item\label{item:zero-in-y} $u_{\mathbb{F},z}(y) = 0$ for all
          $y \in \mathcal{Y}_{\mathbb{F}}$,
    \item\label{item:delta-one} $u_{\mathbb{F},z}(z) = 1$, and
    \item\label{item:delta-zero} $u_{\mathbb{F},z} = 0$ for all
          $w \in \{0, 1\}^{n} \setminus \{z\}$.
  \end{enumerate}
  From \cref{thm:multiquad-extension}, each $\mathbb{F} \in \mathcal{F}$ can prevent at most
  $\abs{\mathcal{Y}_{\mathbb{F}}}$ points from being good. Since
  $t = \abs{\mathcal{Y}_{\mathbb{F}}}$, there are at least $2^{n} - t$ good points.

  Let $G$ be the set of good points, and thus let $B = \{0, 1\}^{n} \setminus G$ be the
  set of not-good points. Define
  \begin{equation}\label{eqn:p-prime}
    p'_{\mathbb{F}}(x) = p_{\mathbb{F}}(x) + \sum_{z \in G}(f'(z) - f(z))u_{\mathbb{F},z}(x).
  \end{equation}
  Now, all we need is to show that $p'_{\mathbb{F}}(x)$ satisfies the two
  conditions from the theorem statement.

  First, we show that $p_{\mathbb{F}}'$ extends $f'$; that is,
  $p_{\mathbb{F}}'(x) = f'(x)$ for all $x \in \{0, 1\}^{n}$. There are two cases
  here: $x \in G$ and $x \in B$. If $x \in B$, then the sum term of \cref{eqn:p-prime}
  is $0$; hence $p'_{\mathbb{F}}(x) = p_{\mathbb{F}}(x)$. Since
  $p_{\mathbb{F}}(x)$ extends $f(x)$, and since $f(x) = f'(x)$ on $B$, this
  means $p'_{\mathbb{F}}(x) = f'(x)$. If $x \in G$, then the only term of the sum
  where $u_{\mathbb{F},z}(x)$ is nonzero is where $x = G$, as per
  \cref{item:delta-one,item:delta-zero} above. Hence, we have
  \[
    p'_{\mathbb{F}}(x) = p_{\mathbb{F}}(x) + f'(x) - f(x),
  \]
  and since $p_{\mathbb{F}}(x) = f(x)$, it follows that
  $p'_{\mathbb{F}}(x) = f'(x)$.

  Next, we show that $p_{\mathbb{F}}'(y)$ and $p_{\mathbb{F}}(y)$ agree for all
  $y \in \mathcal{Y}_{\mathbb{F}}$. Since by \cref{item:zero-in-y} above, we have that
  $u_{\mathbb{F},z}(y) = 0$, it follows that the entire sum term is zero.
  Therefore, $p'_{\mathbb{F}}(y) = p_{\mathbb{F}}(y)$ for all
  $y \in \mathcal{Y}_{\mathbb{F}}$.

  As such, we have constructed a polynomial $p'_{\mathbb{F}}$ and a set $B$ that
  satisfy our conditions of the theorem.
\end{proof}

What this lemma tells us is that if we get a collection of extension polynomials
to some function, for any collection of points we pick we can construct a
``false'' function $f'$ and give it a false collection of extensions, such that
anywhere on $\mathcal{Y}$, our ``false'' $f'$ agrees with $f$. The terminology ``false''
may make more sense in the framework where we think about $\mathcal{Y}$ as being exactly
the set of points we can test from $f_{\mathbb{F}}$ (i.e., we can see
$f_{\mathbb{F}}(\mathcal{Y})$ but no other values of $f_{\mathbb{F}}$): in this context,
we would be completely unable to tell the difference between $f_{\mathbb{F}}$
and $f'_{\mathbb{F}}$, but they would be different polynomials, and even
stronger, $f$ and $f'$ would be different functions!

Next, we introduce an interesting lemma about the sum of multilinear monomials
over a set. So far, we have seen a lot about how surprisingly flexible
multilinear polynomials are: we can construct multilinear polynomials that can
extend any Boolean function, we can construct polynomials that send any
(smallish) set of points to zero but some particular other points to one, and
more. But here, we can provide a restriction on the power of multilinear
polynomials: no matter what we do, their sum over $\{\pm 1\}^{n}$ will always be
zero.

% NOTE: JKRS09 is actually even stronger than this (we need it only to be linear
% in at least one variable)
\begin{lemma}[{\cite[Lemma 7]{JKRS09}}]\label{lem:monomial-sum}
  Let $m(x_{1}, \ldots, x_{n})$ be a multilinear monomial. Over a field of
  characteristic other than 2, we have
  \begin{equation}
    \sum_{b \in \{-1, 1\}^{n}}m(b) = 0.
  \end{equation}
\end{lemma}

\begin{proof}
  For some $x_{i}$, we can write $m = x_{i} \cdot m'$, where the degree of $x_{i}$
  in $m'$ is 0. Then
  \begin{align*}
    \sum_{b \in \{1, -1\}^{n}}m(b)
    &= \sum_{a \in \{-1, 1\}}\sum_{b' \in \{1, -1\}^{n-1}}a \cdot m'(b') \\
    &= \sum_{a \in \{-1, 1\}} a \cdot \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) \\
    &= \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) - \mleft(\sum_{b' \in \{1, -1\}^{n-1}} m'(b')\mright) \\
    &= 0.
  \end{align*}
\end{proof}

We note briefly that this proof only actually requires $m$ to be linear in one
variable; we have given the lemma as-is since it is still strong enough for our
purposes and slightly easier to explain.

\section{Statistics}\label{sec:statistics}

In this paper, we will be dealing quite a bit with computers that have access to
randomness. Because these computers now have access to randomness, their outputs
are no longer deterministic: they can return different results depending on the
exact rolls of their random dice. To talk about computers in this context, we
will need to use a few ideas from statistics.

\begin{defn}\label{def:random-var}\index{random variable}\index{probability space}
  A \emph{random variable} is a function from some \emph{probability space} $\Omega$
  to a set of outcomes $O$.
\end{defn}

In this thesis, we will assume $\Omega$ is finite; this makes much of the statistics
we plan to do easier. While random variables very frequently look like variables
(and we will rather frequently treat them as such), they are actually functions.
One particular example of this distinction is in applying functions to random
variables: we will denote it as $\varphi(X)$, for some function $\varphi$ and random
variable $X$, but the result of this is itself a random variable whose actual
value is $\varphi \circ X$.

\begin{defn}\label{def:event}\index{event}
  An \emph{event} is a subset of a sample space $\Omega$.
\end{defn}

While we define an event as a simple subset, we will often think of an event as
being a condition on some random variable. If $X$ is a random variable with
probability space $\Omega$ and outcome set $O$, the event $X = o$ for some $o \in O$ is
the set $\{X(\omega) = e \mid \omega \in \Omega\}$.

\begin{defn}\label{def:probability}\index{probability}
  Let $E$ be an event. The \emph{probability} of $E$ is
  \begin{equation*}
    \mathbb{P}[E] = \frac{\abs{E}}{\abs{\Omega}}.
  \end{equation*}
\end{defn}

Next, we introduce the notion of statistical independence. In the abstract, two
events are statistically independent if one event occurring does not affect the
likelihood of the other occurring.

\begin{defn}\label{def:stat-indep}\index{statistical independence}
  Two events are \emph{statistically independent} if
  \[
    \mathbb{P}[A \cap B] = \mathbb{P}[A]\mathbb{P}[B].
  \]
\end{defn}

When we have a random variable, one of the things we would like to measure is
what the average outcome is. We capture this value through the notion of an
expected value.

\begin{defn}\label{def:expected-val}\index{expected value}
  The \emph{expected value} of a random variable $X$ is the value
  \begin{equation*}
    \mathbb{E}[X] = \frac{1}{\abs{\Omega}}\sum_{\omega \in \Omega}X(\omega).
  \end{equation*}
\end{defn}

Note that expected value requires the outcome set to be a field; by default we
will assume it to be $\mathbb{R}$ in this paper, unless specified otherwise. Next, we
give some nice properties of probability and expected value. First, Boole's
inequality tells us that the probability of any one of a set of events occurring
is no more than the sum of the probabilities of each of the individual events.

\begin{thm}[{Boole's inequality}]\label{thm:boole-inequality}\index{Boole's inequality}
  For any countable set of events $\{A_{i}\}$, we have
  \begin{equation}
    \mathbb{P}\mleft(\bigcup_{i=1}^{\infty}A_{i}\mright) \le \sum_{i=1}^{\infty}\mathbb{P}(A_{i}).
  \end{equation}
\end{thm}

Next, Jensen's inequality tells us how concave-down functions relate to random variables.

\begin{thm}[{Jensen's inequality}]\label{thm:jensen-inequality}\index{Jensen's inequality}
  Let $\varphi$ be a function that is concave down on its domain. Then, for any random
  variable $X$,
  \begin{equation}\label{eqn:jensen-inequality}
    \mathbb{E}[\varphi(X)] \le \varphi(\mathbb{E}[X]).
  \end{equation}
\end{thm}

Now, we introduce a specific lemma about how independence relates to vector
spaces. In essence, what we are planning to show is that linear independence is
the same as statistical independence. To do this, first we must briefly
introduce one piece of terminology: that of the restriction of a vector.

\begin{defn}\label{def:restrict-vs}\index{restriction!of a vector space}
  Let $V \subseteq \mathbb{F}^{D}$ be a vector space (with corresponding basis
  $\{e_{i} \mid i \in D\}$) and subset $S \subseteq D$. We define the \emph{restriction} of
  $V$ to $S$ to be the set
  \begin{equation}
    V|_{S} = \spn_{V}(\{e_{i} \mid i \in S\}).
  \end{equation}
  Similarly, for a vector $v \in V$ we define the restriction $v|_{S}$ to be the
  vector $(v_{i})_{i \in S} \in V|_{S}$.
\end{defn}

Now, we may state our theorem.

\begin{thm}[{\cite[Claim 2]{CFGS22}}]\label{thm:lin-indep-stat-indep}
  Let $\mathbb{F}$ be a finite field and $D$ a finite set. Let
  $V \subseteq \mathbb{F}^{D}$ be a vector space, and let $v$ be a uniform random
  variable over $V$. For any subdomains $S, S' \subseteq D$, the restrictions $v|_{S}$
  and $v|_{S'}$ are statistically dependent if and only if there exist constants
  $c \in V|_{S}$ and $d \in V|_{S'}$ such that
  \begin{enumerate}
    \item There exists $w \in V$ such that $c \cdot w|_{S} \ne 0$, and
    \item For all $w \in V$, $c \cdot w|_{S} = d \cdot w|_{S'}$.
  \end{enumerate}
\end{thm}

\begin{proof}
  Let $x \in \mathbb{F}^{S}$ and $x' \in \mathbb{F}^{S'}$. Define
  \begin{equation}
    p_{x,x'} = \underset{v \in V}{\mathbb{P}}[v|_{S} \mid v|_{S} =  x \text{ or } v|_{S'} = x'].
  \end{equation}
  Further let $B \subseteq \mathbb{F}^{D}$ be a basis for $V$. Define
  \begin{align*}
    B_{S} &= \{b|_{S} \mid b \in B\} \\
    B_{S''} &= \{b|_{S''} \mid b \in B\} \\
  \end{align*}
  Finally, define $B_{S,S'} \in M_{\abs{S}+\abs{S'},d}(\mathbb{F})$ be the matrix
  where each row vector is $b|_{S}$ concatenated with $b|_{S'}$, for each
  $b \in B$. Hence,
  \begin{equation}
    p_{x,x'} = \underset{z \in \mathbb{F}^{d}}{\mathbb{P}}[B_{S,S'} \cdot z = (x, x')].
  \end{equation}

  For any matrix $A \in M_{m,n}(\mathbb{F})$,
  \begin{equation}
    \underset{z \in \mathbb{F}^{n}}{\mathbb{P}}[Az = b] =
    \begin{cases}
      \abs{\mathbb{F}}^{-\rk(A)} & b \in \img(A) \\
      0 & \text{otherwise}.
    \end{cases}
  \end{equation}
  If $b$ is not in the image of $A$, then by the definition of the image there
  is no $z$ such that $Az = b$. Otherwise, we know that there are
  $\abs{\mathbb{F}}^{\rk(a)}$ elements in the image of $A$, and a random $z$
  has an equal chance of falling on any of them; hence the probability of $Az$
  being equal to $b$ is the inverse of that.

  Next, note that $\img(B_{S,S'}) \subseteq \img(B_{S}) \times \img(B_{S'})$, and by the
  definition of rank, these are equal if and only if
  $\rk(B_{S,S'}) = \rk(B_{S}) + \rk(B_{S'})$. Hence,
  \begin{equation}
    p_{x,x'} = \underset{v \in V}{\mathbb{P}}[v|_{S} = x]\underset{v \in V}{\mathbb{P}}[v|_{S'} = x']
  \end{equation}
  if and only if $\rk(B_{S,S'}) = \rk(B_{S}) + \rk(B_{S'})$. By the rank-nullity
  theorem, this is true if and only if
  $\nul(B_{S,S'}^{T}) \subseteq \nul(B_{S}^{T}) \times \nul(B_{S'}^{T})$. Lastly, note that
  the two conditions in the theorem statement are true exactly when there exists
  $c \in \mathbb{F}^{S}$ and $d \in \mathbb{F}^{S'}$ such that $c \notin \nul(B_{S}^{T})$
  but $(c, -d) \in \nul(B_{S'}^{T})$.
\end{proof}

\section{Error-correcting codes}

Error-correcting codes are a concept with broad applications in both theoretical
and practical computer science. An error-correcting code is some function
applied to a string, such that any two elements of the image of that function
are sufficiently far from each other (for some definition of ``far'' that we
will see soon). These are called ``error-correcting'' because if a valid output
is edited a relatively small amount, the large distance to other valid outputs
means that the edited string is not valid, and what the original output was can
still be guessed reasonably effectively, since it is highly likely to still be
the closest valid string.

To work with error-correcting codes, we first need to define a notion of
distance. Computer scientists use several notions of distance, all of which can
be useful in different contexts. We will be using one of the simpler ones,
because it is more mathematically elegant (as opposed to being more practically
useful) and works well with the definitions we will be using in the rest of the
text.

\begin{defn}[{\cite{Ham50}}]\label{def:hamming-dist}\index{Hamming distance}
  Let $x, y \in \Sigma^{n}$ be strings of the same length. We say the \emph{Hamming
    distance} between $x$ and $y$ is the value
  \[
    \Delta(x, y) = \frac{\abs{\{i \in [n] \mid x_{i} \ne y_{i}\}}}{n}.
  \]
\end{defn}

Note that $\Delta(x, y) \in [0, 1]$ for any strings $x$ and $y$. This normalization is
not strictly necessary in general, since $\Delta$ is only defined between strings of
equal length (and there do exist cases where it is much nicer to keep the
distance as a natural number). In our case, however, we would like to keep the
distances all in the same bounded range; hence we normalize.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \foreach [count=\i] \c in
        {1,\textcolor{pumpkin}{0},0,0,\textcolor{pumpkin}{1},\textcolor{pumpkin}{0},0,1} {
      \pgfmathsetmacro{\xval}{\i*0.5};
      \draw (\xval,0.5) rectangle ++(0.5,0.5) node [midway] {\c};
    }
    \foreach [count=\i] \c in
        {1,\textcolor{pumpkin}{1},0,0,\textcolor{pumpkin}{0},\textcolor{pumpkin}{1},0,1} {
      \pgfmathsetmacro{\xval}{\i*0.5};
      \draw (\xval,0) rectangle ++(0.5,0.5) node [midway] {\c};
    }
  \end{tikzpicture}
  \caption{Two strings with Hamming distance $3/8$}\label{fig:hamming-dist}
\end{figure}

The definition of Hamming distance between two strings also generalizes to the
notion of distance from a set. Informally, we say the distance from a string to
a set is simply the distance to the closest element of the set.

\begin{defn}\label{def:far}\index{far}
  Let $\varepsilon > 0$. A vector $x \in \Sigma^{n}$ is \emph{$\varepsilon$-far} from a set $S \subseteq \Sigma^{n}$ if
  \[
    \min_{y \in S}(\Delta(x, y)) \ge \varepsilon.
  \]
\end{defn}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[>=Stealth]
    \coordinate (Z) at (0,0) {};
    \coordinate (X) at (1,-0.25);
    \draw[use Hobby shortcut,closed=true]
    (0,0) .. (0,1.5) .. (1,1) .. (1.5,2) .. (-.5,3) .. (-1,1.5) .. (-1,.5);
    \draw[->] (X) -- (Z) node[midway,below] {$\varepsilon$};
    \fill (X) circle (1pt) node[right] {$x$};
    \node (S) at (0.25, 2.25) {$S$};
    \node [draw,dashed] at (1,-0.25) [circle through=(Z)] {};
  \end{tikzpicture}
  \caption{A point $x$ that is $\varepsilon$-far from a set $S$}\label{fig:epsilon-far}
\end{figure}

Now that we have some notions of distance, we can define a useful
error-correcting code. Like with distance metrics, there are lots of useful
error-correcting codes; we will pick one that has particularly nice mathematical
properties.

\begin{defn}\label{def:rm-set}\index{RM@$\RM$}
  Let $\mathbb{F}$ be a finite field, and let $k, r, m \in \mathbb{N}^{+}$. Then the set
  $\RM^{k}[\mathbb{F}, r, m]$ is
  \begin{equation}
    \RM^{k}[\mathbb{F}, r, m] = \mleft\{
      \begin{aligned}
        F\colon \mathbb{F}^{m} &\rightarrow \mathbb{F}^{k} \\
        x &\mapsto (p_{1}(x), \ldots, p_{k}(x))
      \end{aligned}
      \middlemid
      p_{i} \in \mathbb{F}^{\le d}[X_{1, \ldots, m}]
    \mright\}.
  \end{equation}
\end{defn}

In a strict set-theoretic sense, this means
$\RM^{k}[\mathbb{F}, r, m] = \mleft(\mathbb{F}^{\le d}[X_{1, \ldots, m}]\mright)^{k}$;
however, we will be referring to this set in a specific context: that of
encoding values. More specifically, when we talk about
$\RM^{k}[\mathbb{F}, r, m]$, we are often referring to a specific encoding of
these polynomials as strings. Something to note is that while we have almost
always been using the language $\{0, 1\}$ so far, here we will be using the
language $\mathbb{F}^{k}$.

\begin{defn}\label{def:rm-code}\index{Reed-Muller code}
  Let $\varphi$ be an injection from a set $S$ to $\RM^{k}[\mathbb{F}, r, m]$.Then the
  \emph{Reed-Muller code} of a value $s \in S$ is the vector
  \begin{equation}
    (\varphi(s)(x))_{x \in \mathbb{F}^{m}} \in (\mathbb{F}^{k})^{\abs{\mathbb{F}}^{m}}.
  \end{equation}
\end{defn}

Less formally, what this means is that for our input $s \in S$, we associate it
with some polynomial $F$; then our encoding is the \emph{entire evaluation
  table} of $F$ over its domain. If we consider our alphabet to be
$\mathbb{F}^{k}$ (i.e., the codomain of $F$), then the Hamming distance from $F$
to any other arbitrary function $F'\colon \mathbb{F}^{m} \rightarrow \mathbb{F}^{k}$ is exactly
the proportion of inputs $x$ on which $F'(x) \ne F(x)$. The following lemma
formalizes this notion.

\begin{lemma}\label{lem:rm-distance}
  Let $F \in \RM[\mathbb{F}, m, d]$, and let
  $F'\colon \mathbb{F}^{m} \rightarrow \mathbb{F}^{k}$, where $F'$ is also laid out as an
  evaluation table, in the same order as $F$. Then, the Hamming distance
  $\Delta(F, F')$, when written over the alphabet $\mathbb{F}^{k}$, is
  \begin{equation}\label{eqn:rm-distance}
    \frac{\abs*{\{x \in \mathbb{F}^{m} \mid F(x) \ne F'(x)\}}}{\abs{\mathbb{F}}^{m}}.
  \end{equation}
\end{lemma}

\begin{proof}
  First, note that since our alphabet is exactly the codomain of $F$ (and $F'$),
  each entry in the evaluation table has length $1$. Hence, there is a bijection
  between inputs $x \in \mathbb{F}^{m}$ and indexes in the evaluation table.
  Further, two items in the table are equal if and only if $F(x) = F'(x)$.
  Hence, the total number of distinct values in their representations is exactly
  $\abs{\{x \in \mathbb{F}^{m} \mid F(x) \ne F(x)\}}$. The total length of both
  evaluation tables is exactly the number of inputs, i.e.
  $\abs{\mathbb{F}^{m}} = \abs{\mathbb{F}}^{m}$. Hence, by the definition of the
  Hamming distance, \cref{eqn:rm-distance} is true.
\end{proof}

\begin{cor}\label{cor:rm-probability}
  Let $F \in \RM[\mathbb{F}, m, d]$, and let
  $F'\colon \mathbb{F}^{m} \rightarrow \mathbb{F}^{k}$. Then,
  \begin{equation}\label{eqn:rm-probability}
    \Delta(F, F') = \mathbb{P}[F(x) \ne F'(x)].
  \end{equation}
\end{cor}

\begin{proof}
  The probability over $x$ that $F(x) \ne F'(x)$ is exactly the number of inputs
  in which $F(x) \ne F'(x)$, divided by the number of values $x$ can be. We know
  $x$ can be any of $\abs{\mathbb{F}}^{m}$ choices, and by definition the number
  of values of $x$ where $F(x) \ne F'(x)$ is
  $\abs{\{x \in \mathbb{F}^{m} \mid F(x) \ne F(x)\}}$. Hence, we get that
  \cref{eqn:rm-distance} is exactly $\mathbb{P}[F(x) \ne F'(x)]$, and thus by
  \cref{lem:rm-distance}, \cref{eqn:rm-probability} is true.
\end{proof}

We will be using Reed-Muller codes as a way to ``space out'' our values---that is,
we will be using \cref{cor:rm-probability} to our advantage when we deal with
proofs of proximity. We will talk about these in much more depth in
\cref{sec:pcpp}, but the important point is that these have restrictions on how
close different valid inputs can be to each other, and we will want to use these
to help keep them relatively far apart.

\chapter{Relativization and algebrization}

An important prerequisite to understanding algebrization is the similar, but
simpler, concept of \emph{relativization}, also called \emph{oracle separation}.
To do this, we first must define an \emph{oracle}.
\begin{defn}[{\cite[Def.\ 2.1]{AW09}}]\label{def:oracle}\index{oracle}
  An \emph{oracle} $A$ is a collection of Boolean functions
  $A_{m}\colon \{0, 1\}^{m} \rightarrow \{0, 1\}$, one for each natural number $m$.
\end{defn}
There are several ways to think of an oracle; this will extend the most
naturally when it comes time to define an extension oracle in
\cref{def:ext-oracle}.

Another way to think of an oracle is as a subset $A \subseteq \{0, 1\}^{*}$. This allows
us to think of $A$ as a language. Since we can do this, it gives us the ability
to think of the complexity of the oracle. If we want to think about the subset
in terms of our functions, we can write $A$ as
\begin{equation}
  A = \bigcup_{m \in \mathbb{N}}\mleft\{x \in \{0, 1\}^{m} \mid A_{m}(x) = 1\mright\}.
\end{equation}
We will use the Iverson bracket defined in \cref{def:iverson-bracket} for this
purpose: allowing us to think of $A$ as the set and $[A]$ as the function.

The third way to think of an oracle is as a list of bits---this is how a Turing
machine thinks of an oracle. In this context, we consider the oracle to be a
string of $2^{n}$ bits $b_{i}$, where $b_{i}$ is the result of $A$ when given
the binary representation of $i$ as input. We will mostly not think of $A$ this
way explicitly, but for practical purposes this is how an oracle is encoded
whenever we pass it to a Turing machine as an input.

\begin{example}\label{ex:oracle-function}
  Let $m = 3$. The function
  \begin{equation}
    \begin{aligned}
      f\colon \{0, 1\}^{3} &\rightarrow \{0, 1\} \\
      abc &\mapsto b
    \end{aligned}
  \end{equation}
  is an oracle function. We can think of $f$ as corresponding to the set
  $\{010, 011, 110, 111\}$.
\end{example}

\begin{example}\label{ex:oracle-full}
  For each $n \in \mathbb{N}$, define
  \begin{equation}
    \begin{aligned}
      f_{n}\colon \{0, 1\}^{n} &\rightarrow \{0, 1\} \\
      a_{1}a_{2} \cdots a_{n} &\mapsto a_{n}.
    \end{aligned}
  \end{equation}
  Then the set $\{f_{n}\}$ forms an oracle, whose corresponding language is the
  set of all binary representations of odd numbers.
\end{example}

An oracle is not particularly interesting mathematical object on its own (after
all, it is simply a set of arbitrary Boolean functions); its utility comes from
when it interacts with a Turing machine. A normal Turing machine does not have
the facilities to interact with an oracle, so we need to define a small
extension to a standard Turing machine to allow for this.

\begin{defn}[{\cite[Def.\ 3.6]{AB09}}]\label{def:tm-oracle}\index{Turing machine!with oracle}
  A \emph{Turing machine with an oracle} is a Turing machine with an additional
  tape, called the \emph{oracle tape}, as well as three special states:
  $q_{\text{query}}$, $q_{\text{yes}}$, and $q_{\text{no}}$. Further, each
  machine is associated with an oracle $A$. During the execution of the machine,
  if it ever moves into the state $q_{\text{query}}$, the machine then (in one
  step) takes the output of $A$ on the contents of the oracle tape, moving into
  $q_{\text{yes}}$ if the answer is 1 and $q_{\text{no}}$ if the answer is 0.
\end{defn}

Of course, the question now becomes how we can effectively use an oracle in an
algorithm. The previously-mentioned conception of an oracle as a set of strings
is useful here. If we consider the set of strings as being a \emph{language} in
its own right, then querying the oracle is the same as determining whether a
string is in the language, just in one step. If the language is computationally
hard, this means our machine can get a significant power boost from the right
oracle.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[
    start chain = going right,
    node distance = 0pt,
    grid/.style={draw, minimum width=2em, minimum height=2em,
      outer sep=0pt, on chain},
    ]
    \node[grid] (1) {$\mathcal{O}(0)$};
    \node[grid] (2) {$\mathcal{O}(1)$};
    \node[grid] (3) {$\mathcal{O}(00)$};
    \node[grid] (4) {$\mathcal{O}(01)$};
    \node[grid] (5) {$\mathcal{O}(10)$};
    \node[grid] (6) {$\mathcal{O}(11)$};
    \node[grid] (7) {$\mathcal{O}(000)$};
    \node[grid] (8) {$\cdots$};
  \end{tikzpicture}
  \caption{Bit representation of an oracle}\label{fig:oracle-bits}
\end{figure}

\begin{defn}[{\cite[Def.\ 2.1]{AW09}}]\label{def:oracle-class}
  For any complexity class $\mathcal{C}$, the complexity class $\mathcal{C}^{A}$ is the class of all
  languages determinable by a Turing machine with access to $A$ using the amount
  of resources defined for $\mathcal{C}$.
\end{defn}

We will be using this definition in many places, so we should take a moment to
look at it in more depth. First, it is important to realize that $\mathcal{C}^{A}$ is a
set of \emph{languages}, not \emph{machines}: despite the notation, augmenting
$\mathcal{C}$ with an oracle does not modify any languages, it just adds new ones that are
computable. Second, since a machine can always ignore its oracle, it follows
that adding an oracle can only increase the number of languages in the class,
never decrease it.

\begin{lemma}\label{thm:relativizing-increases}
  For any complexity class $\mathcal{C}$ and oracle $A$, $\mathcal{C} \subseteq \mathcal{C}^{A}$.
\end{lemma}

\begin{proof}
  Let $L \in \mathcal{C}$ and $M$ be a machine that determines $L$. Then the oracle machine
  $M'$ that simulates $M$ on its input and makes no queries to the oracle will
  also accept exactly $L$. Since $M'$ is a $\mathcal{C}^{A}$ machine for any oracle $A$,
  it follows that $L \in \mathcal{C}^{A}$ and hence $\mathcal{C} \in \mathcal{C}^{A}$.
\end{proof}

While the above lemma tells us that $\mathcal{C} \subseteq \mathcal{C}^{A}$ always, another interesting
question is when $\mathcal{C} = \mathcal{C}^{A}$. We do have a notion for this, called
\emph{lowness}. Lowness can be defined for both individual languages and
complexity classes; we will define both here.

% TODO: Cite all these
\begin{defn}\label{def:low}\index{low}
  A language $L$ is \emph{low} for a class $\mathcal{C}$ if $\mathcal{C}^{L} = \mathcal{C}$. A complexity
  class $\mathcal{D}$ is \emph{low} for a class $\mathcal{C}$ if each language in $\mathcal{D}$ is low for
  $\mathcal{C}$.
\end{defn}

Of particular interest to us will be classes that are low for \emph{themselves}.
We care about these classes because they can use other problems from the same
class as a subroutine without issue; in particular recursion and iteration both
work here. Thankfully, both $\P$ and $\PSPACE$ are low for themselves (it turns
out $\NP$ is probably not); this allows us to easily write algorithms that
recurse for classes in both of our most common classes.

\begin{thm}\label{thm:p-pspace-low}
  $\P$ and $\PSPACE$ are low for themselves.
\end{thm}

\begin{proof}
  Let $L \in \P$ and let $K \in \P^{L}$. Let $M(L)$ be a determiner for $L$ and
  $M(K)$ be a determiner for $K$. Further, let $\hat{M}(K)$ be a determiner of
  $K$, but with access to $L$ as an oracle. Our goal is to show that $K \in \P$.

  Let $p_{L}(n)$ be a polynomial upper bound of the runtime of $M(L)$ on an
  input of length $n$, and let $p_{\hat{K}}(n)$ be similar. Since $M(K)$ can
  call $M(L)$ no more than $p_{\hat{K}}(n)$ times, it follows that
  $p_{K}(n) \le p_{\hat{K}}(p_{L}(n))$. Hence, the runtime of $M(K)$ is bounded
  above by a polynomial, and thus $K \in \P$.

  The proof for $\PSPACE$ is very similar to that of $\P$, but with space
  instead of time. Since memory usage is bounded above by some polynomial, and
  polynomials are closed under composition, it follows that $\PSPACE$ is low for
  itself.
\end{proof}

\section{Defining relativization}

We are now ready to define what relativization is. First, note that
relativization is a statement about a \emph{result}: we talk about inclusions
relativizing, not sets themselves.

% TODO: Cite
\begin{defn}\label{def:relativization}\index{relativization}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \subseteq \mathcal{D}$. We say the result
  $\mathcal{C} \subseteq \mathcal{D}$ \emph{relativizes} if $\mathcal{C}^{A} \subseteq \mathcal{D}^{A}$ for all oracles $A$. Conversely,
  if there exists $A$ such that $\mathcal{C} \nsubseteq \mathcal{D}$, we say that the result $\mathcal{C} \subseteq \mathcal{D}$
  \emph{does not relativize}.
\end{defn}

\begin{defn}\label{def:relativization-ne}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \nsubseteq \mathcal{D}$. We say the result
  $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{relativizes} if $\mathcal{C}^{A} \nsubseteq \mathcal{D}^{A}$ for all oracles $A$. Conversely,
  if there exists $A$ such that $\mathcal{C} \subseteq \mathcal{D}$, we say that the result $\mathcal{C} \nsubseteq \mathcal{D}$
  \emph{does not relativize}.
\end{defn}

We start with a very straightforward example of a relativizing result.

\begin{lemma}\label{lem:pa-subset-npa}
  For any oracle $A$, $\P^{A} \subseteq \NP^{A}$. Equivalently, the result $\P \subseteq \NP$
  relativizes.
\end{lemma}

\begin{proof}
  Since any deterministic Turing machine is also a nondeterministic machine, it
  follows that a machine that solves a $\P^{A}$ problem is also an $\NP^{A}$
  machine. Hence, $\P^{A} \subseteq \NP^{A}$.
\end{proof}

This result tells us that not \emph{everything} is weird in the world of
relativization (although we will soon do our best to find all the weird bits):
if we have a machine that can do more operations without an oracle, it can still
do more operations with an oracle. Further, for the question of $\P$ vs.\ $\NP$
that we will discuss in \cref{sec:rel-p-np}, this means that the question we
care about is whether $\NP \subseteq^{?} \P$ relativizes. As such, the question we are
asking simplifies to determining where $\P^{A} = \NP^{A}$ and where
$\P^{A} \subsetneq \NP^{A}$.

Now that we have talked about set inclusions relativizing, we need to define the
other side of the coin: \emph{proofs} can relativize as well as results.
Unfortunately, this needs to be a somewhat informal definition as formally
delineating different types of proof is far beyond the scope of this paper.
However, the definition we offer here will be sufficient for our purposes.

\begin{defn}\label{def:relativizing-result}
  We say a \emph{proof relativizes} if it is not made invalid if the relevant
  classes are replaced with oracle classes, i.e., a proof that $\mathcal{C} \subseteq \mathcal{D}$
  \emph{relativizes} if the same proof can be used to show $\mathcal{C}^{A} \subseteq \mathcal{D}^{A}$ for
  all oracles $A$ with minimal modifications.
\end{defn}

This gives us a reason to care about relativization as a concept: if our proofs
are relativizing then we know not to try to use them to prove nonrelativizing
results. In particular, we will show in \cref{sec:rel-p-np} that the famous $\P$
vs.\ $\NP$ problem will not relativize regardless of the outcome, and then in
\cref{sec:rel-conseq} we will show that the common proof technique of
diagonalization \emph{does} in fact relativize.

\section{Query complexity}\label{sec:query-complexity}

Now that we have given ourselves a reason to care about oracles and how they
interact with Turing machines, we now turn to the question of how a machine can
gain information about the oracle it queries. We will do this with the notion of
\emph{query complexity}.

The goal of query complexity is to ask questions about some Boolean function
$A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ by querying $A$ itself. For this, we will
interchangeably think of $A$ as a \emph{function} as well as a bit string of
length $N = 2^{n}$, where each string element is $A$ applied to the $i$th string
of length $n$, arranged in some lexicographical order.

We can further think of the property itself as being a Boolean function; a
function that takes as input the bit-string representation of $A$ and outputs
whether or not $A$ has the given property. We will call the function
representing the property $f$. When viewed like this, $f$ is a function from
$\{0, 1\}^{N}$ to $\{0, 1\}$. We define three types of query complexity for
three of the most common types of computing paradigms: deterministic,
randomized, and quantum. Nondeterministic query complexity is interesting, but
it is outside the scope of this paper.

\begin{defn}[{\cite[17]{AW09}}]\label{def:det-qc}\index{query complexity!deterministic}
  Let $n \in \mathbb{N}$ and let $A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ be an oracle. We can write
  $A$ using $2^{n}$ bits, where bit $i$ is the output of $A$ when given the
  binary representation of $i$ as input. Define $N = 2^{n}$, and let
  $f\colon \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{deterministic query complexity} of $f$, which we write $D(f)$, is the
  minimum number of queries made by any deterministic algorithm with access to
  an oracle $A$ that determines the value of $f(A)$.
\end{defn}

To make this more clear, let us give an example problem.

\begin{defn}\label{def:or-problem}\index{OR@$\OR$}
  The $\OR$ problem is the following oracle problem:
  \begin{quote}
    Let $A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ be an oracle. The function $\OR(A)$
    returns 1 if there exists a string on which $A$ returns 1, and $0$
    otherwise.
  \end{quote}
\end{defn}

The question is then what the deterministic query complexity of the $\OR$
function is.

\begin{thm}
  The $\OR$ problem has a deterministic query complexity of $2^{n}$.
\end{thm}

\begin{proof}
  First, note that any algorithm that determines the $\OR$ problem can stop as
  soon as it queries $A$ and gets an output of $1$. Hence, for any algorithm
  $M$, let $\{s_{i}\}$ be the sequence of queries $M$ makes to $A$ on the
  assumption that it always receives a response of $0$. If
  $\abs{\{s_{i}\}} \le 2^{n}$, there exists some $s \in \{0, 1\}^{n}$ not queried.
  In that case, $M$ will not be able to distinguish the zero oracle from the
  oracle that outputs $1$ only when given $s$. Hence, $M$ must query every
  string of length $n$ and thus the query complexity is $2^{n}$.
\end{proof}

From this, we get that the $\OR$ problem cannot be solved any better than by
enumerative checking. This makes intuitive sense because none of the results we
get by querying $A$ imply anything about what $A$ will do on other values, since
$A$ can be an arbitrary function. In \cref{sec:alg-query-complexity}, we will
look at what happens when we give ourselves access to a \emph{polynomial}, where
querying one point could tell us information about others.

For the next two definitions, since their Turing machines include some element
of randomness, we only require that they succeed with a $2/3$ probability. This
is in line with most definitions of complexity classes involving random
computers.

\begin{defn}[{\cite[17]{AW09}}]\label{def:rand-qc}\index{query complexity!randomized}
  Let $f\colon \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function. Then the
  \emph{randomized query complexity} of $f$, which we write $D(f)$, is the
  minimum number of queries made by any randomized algorithm with access to an
  oracle $A$ that evaluates $f(A)$ with probability at least $2/3$.
\end{defn}

\section{Relativization of $\P$ vs.\ $\NP$}\label{sec:rel-p-np}

An important example of relativization is that of $\P$ and $\NP$. While the
question of if $\P = \NP$ is still open, we aim to show that \emph{regardless of
  the answer}, the result does not relativize. To do this, we show that there
are some oracles $A$ where $\P^{A} = \NP^{A}$, and some where
$\P^{A} \ne \NP^{A}$.

Additionally, it should be noted that the similarity of relativization to
algebrization means that the structure of these proofs will return in
\cref{sec:alg-p-np} when we show the algebrization of $\P$ and $\NP$.

The more straightforward of the two proofs is the oracle where
$\P^{A} = \NP^{A}$, so we shall begin with that.

\begin{thm}[{\cite[Theorem 2]{BGS75}}]\label{thm:p-np-rel}
  There exists an oracle $A$ such that $\P^{A} = \NP^{A}$.
\end{thm}

\begin{proof}
  For this, we can let $A$ be any $\PSPACE$-complete language. By letting our
  machine in $\P$ be the reducer from $A$ to any other language in $\PSPACE$, we
  therefore get that $\PSPACE \subseteq \P^{A}$. Similarly, if we have a problem in
  $\NP^{A}$, we can verify it in polynomial space without talking to $A$ at all
  (by having our machine include a determiner for $A$). Hence, we have that
  $\NP^{A} \subseteq \NPSPACE$. Further, a celebrated result of Savitch~\cite{Sav70}
  (which we briefly discussed as \cref{thm:savitch}) is that
  $\PSPACE = \NPSPACE$. Combining all these results, we get the chain
  \begin{equation}
    \NP^{A} \subseteq \NPSPACE = \PSPACE \subseteq \P^{A} \subseteq \NP^{A}.
  \end{equation}
  This is a circular chain of subset relations, which means everything in the
  chain must be equal. Hence, $\P^{A} = \NP^{A} = \PSPACE$.
\end{proof}

For a slightly more intuitive view of what this proof is doing, what we have
done is found an oracle that is so powerful that it dwarfs any amount of
computation our actual Turing machine can do. Hence, the power of our machine is
really just the same as the power of our oracle, and since we have given both
the $\P$ and $\NP$ machine the same oracle, they have the same power.

Having shown that an oracle exists where $\P^{A} = \NP^{A}$, we now endeavor to
find one where $\P^{A} \ne \NP^{A}$. This piece of the proof is less simple than
the previous section, and it uses a diagonalization argument to construct the
oracle. Before we dive in to the main proof, however, we need to define a few
preliminaries.

\begin{defn}[{\cite[436]{BGS75}}]\label{def:l(x)}\index{L(X)@$L(X)$}
  Let $X$ be an oracle. The language $L(X)$ is the set
  \begin{equation*}
    L(X) = \{x \mid \text{there is } y \in X \text{ such that } \abs{y} = \abs{x}\}.
  \end{equation*}
\end{defn}

For example, consider the language $X = \{0, 11, 0100\}$. The language $L(X)$ is
the language consisting of all strings of length $1$, $2$, and $4$.

Our eventual goal will be to construct a language $X$ such that
$L(X) \in \NP^{X} \setminus \P^{X}$. Of particular note is that we can rather nicely put a
upper bound on the complexity of $L(X)$ when given $X$ as an oracle, regardless
of the value of $X$. This fact is what gives us the freedom to construct $X$ in
such a way that $L(X)$ will not be in $\P^{X}$.

\begin{lemma}[{\cite[436]{BGS75}}]\label{lem:l(x)-in-np}
  For any oracle $X$, $L(X) \in \NP^{X}$.
\end{lemma}

\begin{proof}
  Let $S$ be a string of length $n$. If $S \in L(X)$, then a witness for $S$ is
  any string $S'$ such that $\abs{S} = \abs{S'}$ and $S' \in X$. Since a machine
  with query access to $X$ can query whether $S'$ is in $X$ in one step, it
  follows that we can verify that $S \in L(X)$ in polynomial time.
\end{proof}

With this lemma as a base, we can now move on to our main theorem.

\begin{thm}[{\cite[Theorem 3]{BGS75}}]\label{thm:p-np-nrel}
  There exists an oracle $A$ such that $\P^{A} \ne \NP^{A}$.
\end{thm}

\begin{proof}
  Our goal is to construct a set $B$ such that $L(B) \notin \P^{B}$. We shall
  construct $B$ in an interactive manner. We do this by taking a sequence
  $\{P_{i}\}$ of all machines that recognize some language in $\P^{A}$, and then
  constructing $B$ such that for each machine in the sequence, there is some
  part of $L(B)$ it cannot recognize. This technique is called
  \emph{diagonalization},\index{diagonalization} and it is used in many places
  in computer science theory.\footnote{This argument style is named after
    \emph{Cantor's diagonal argument}, which was originally used to prove that
    the real numbers are uncountable~\cite[Thm. 2.14]{Ru76}.} Additionally, we
  define $p_{i}(n)$ to be the maximum running time of $P_{i}$ on an input of
  length $n$. We aim to show that \cref{alg:construct-b} constructs $B$.

  \begin{algorithm}[htbp]
    \KwIn{A sequence of $\P$ oracle machines $\{P_{i}\}_{i=1}^{\infty}$}
    \KwOut{A set $B$ such that $L(B) \notin \P^{B}$}
    $B(0) \leftarrow \varnothing$\;
    $n_{0} \leftarrow 0$\;
    \For{$i$ starting at $1$}{
      Let $n > n_{i}$ be large enough that
      $p_{i}(n) < 2^{n}$\;\nllabel{line:def-n}
      Run $P_{i}^{B(i-1)}$ on input $0^{n}$\;\nllabel{line:computation}
      \If{$P_{i}^{B(i-1)}$ rejects $0^{n}$}{
        Let $x$ be a string of length $n$ not queried during the above
        computation\;\nllabel{line:not-queried}
        $B(i) \leftarrow B(i-1) \sqcup \{x\}$\;
      }
      $n_{i+1} \leftarrow 2^{n}$\;
    }
    $B \leftarrow \bigcup_{i}B(i)$\;
    \caption{An algorithm for constructing $B$}\label{alg:construct-b}
  \end{algorithm}

  To begin, let us demonstrate the algorithm's soundness. First, note that since
  $P_{i}$ runs in polynomial time, $p_{i}(n)$ is bounded above by a polynomial,
  and hence there will always exist an $n$ as defined in line~\ref{line:def-n}.
  Next, since there are $2^{n}$ strings of length $n$ and since
  $p_{i}(n) < 2^{n}$, we know that there must be some $x$ to make
  line~\ref{line:not-queried} well-defined. While our algorithm allows $x$ to be
  any string, if it is necessary to be explicit in which we choose, then picking
  $x$ to be the smallest string in lexicographic order is a standard choice.

  We should also briefly mention that this algorithm does not terminate. This is
  okay because we are only using it to construct the set $B$, which does not
  need to be bounded. If this were to be made practical, since the sequence of
  $n_{i}$s is monotonically increasing, the set could be constructed ``lazily''
  on each query by only running the algorithm until $n_{i}$ is greater than the
  length of the query.

  Next, we demonstrate that $L(B) \notin \P^{B}$. The end goal of our instruction is
  a set $B$ such that if $P_{i}^{B}$ accepts $0^{n}$ then there are no strings
  of length $n$ in $B$, and if $P_{i}^{B}$ rejects, then there is a string of
  length $n$ in $B$. This means that no $P_{i}$ accepts $L(B)$, and hence
  $L(B) \notin \NP^{B}$.

  The central idea behind the proper functioning of our algorithm is that adding
  strings to our oracle \emph{cannot change the output if they are not queried}.
  This is what we do in line~\ref{line:def-n}: we need our input length to be
  long enough to guarantee that a non-queried string exists. Since the number of
  queried strings is no greater than $p_{i}(n)$, and there are $2^{n}$ strings
  of length $n$, there must be some string not queried.

  Next, we run $P_{i}^{B(i-1)}$ on all the strings we have already added. If it
  accepts, then we want to make sure that no string of length $n$ is in $B$;
  that is, $0^{n}$ is not in $L(B)$. Hence, in this particular loop we add
  nothing to $B(i)$. If $P_{i}^{B(i-1)}$ rejects, we then need to make sure that
  $0^{n} \in L(B)$ but in a way that does not affect the output of
  $P_{i}^{B(i-1)}$. Hence, we find a string that $P_{i}^{B(i-1)}$ did not query
  (and thus will not affect the result) and add it to $B(i)$.

  Having done this, we then set $n_{i+1}$ to be $2^{n}$. Since
  $p_{i}(n) < 2^{n}$, it follows that no previous machine could have queried any
  strings of length $n_{i+1}$.\footnote{A word of caution: we only care about
    what $P_{i}$ does on input $n_{i}$, \emph{not any other input}. This is
    because we only need each machine to be incorrect for some $i$, not all
    $i$.} This way, we ensure our previous machines do not accidentally have
  their output change due to us adding a string they queried.

  Having run this over all polynomial-time Turing machines, we have a set $L(B)$
  such that no machine in $\P^{B}$ accepts it, which tells us $L(B) \notin \P^{B}$.
  But, \cref{lem:l(x)-in-np} already told us $L(B) \in \NP^{B}$. Hence,
  $\P^{B} \ne \NP^{B}$.
\end{proof}

\section{Consequences on proof techniques}\label{sec:rel-conseq}

Of course, determining that $\P$ vs $\NP$ does not relativize is only important
if the proof techniques used in practice \emph{do} in fact relativize. Rather
unfortunately, it turns out that simple diagonalization is a relativizing
result.

While diagonalization itself does not have a formal definition, we can still
think about it informally. Looking at our construction of $B$, which we did
using diagonalization, notice that our definition never really cared about how
the $P_{i}$ worked, just about the results it produced. Hence, if it were to be
possible to modify \cref{alg:construct-b} to construct $B \in \NP \setminus \P$, the proof
would remain the same if we were to replace our sequence $\{P_{i}\}$ with a
sequence of machines in $\P^{A}$ for some $\PSPACE$-complete $A$. However, this
would lead to a contradiction, as we showed in \cref{thm:p-np-rel} that in that
case, $\P^{A} = \NP^{A}$! This tells us that a simple diagonalization argument
would not suffice to determine separation between $\P$ and $\NP$.

While we know that diagonalization relativizes, in the years since the Baker,
Gill, and Solovay paper researchers have discovered proof techniques that do not
in fact relativize. One of these techniques is
\emph{arithmetization}\index{arithmetization}, introduced by~\cite{BF91}.

The idea behind arithmetization is that we want to be able to reduce
computational problems to algebraic ones. More specifically, we would like to
reduce our problems to ones involving low-degree polynomials over a finite field
(such as those seen in \cref{sec:polynomial}). In this paper, we will care about
arithmetization for two reasons: because it is a non-relativizing technique (as
we are about to see) and because we will be using it later on in this paper as
an important part of several proofs.

\section{Algebrization}\label{chap:algebrization}

Algebrization, originally described by Aaronson and Wigderson~\cite{AW09}, is an
extension of relativization. While relativization deals with oracles that are
Boolean functions, algebrization extends oracles to be a collection of
polynomials over finite fields. Since any field contains the set $\{0, 1\}$, we
can think about our new oracles as \emph{extending} some specific oracle $A$, so
that both oracles agree on the set $\{0, 1\}^{n} \subseteq \mathbb{F}^{n}$. We formalize
this notion below.

\begin{defn}[{\cite[Def.\ 2.2]{AW09}}]\label{def:ext-oracle}\index{extension oracle}
  Let $A_{m}\colon \{0, 1\}^{m} \rightarrow \{0, 1\}$ be a Boolean function and let
  $\mathbb{F}$ be a finite field. Also, given an oracle $A = (A_{m})$, an
  \emph{extension} $\tilde{A}$ of $A$ is a collection of polynomials
  $\tilde{A}_{m,\mathbb{F}}\colon \mathbb{F}^{m} \rightarrow \mathbb{F}$, one for each
  positive integer $m$ and finite field $\mathbb{F}$, such that
  \begin{enumerate}
    \item $\tilde{A}_{m,\mathbb{F}}$ is an extension polynomial of $A_{m}$ for
          all $m,\mathbb{F}$, and
    \item there exists a constant $c$ such that
          $\tilde{A}_{m,\mathbb{F}} \in \mathbb{F}[X_{1, \ldots, n}^{\le c}]$ for all
          $m, \mathbb{F}$.
  \end{enumerate}
\end{defn}

Take note that an oracle can have many different extension oracles, since one
can construct an infinite number of polynomials that go through a set of points.
For this reason, when dealing with oracles in practice, we will also often be
interested in oracles of a particular multidegree, which limits our options for
oracles in potentially-interesting ways.

\begin{example}\label{ex:oracle-function-ext}
  Consider the function we defined in \cref{ex:oracle-function}:
  \begin{equation}
    \begin{aligned}
      f\colon \{0, 1\}^{3} &\rightarrow \{0, 1\} \\
      abc &\mapsto b.
    \end{aligned}
  \end{equation}
  An extension of that function is the polynomial
  \begin{equation}
    \begin{aligned}
      \tilde{f}\colon \mathbb{F}^{3} &\rightarrow \mathbb{F}^{3} \\
      (a,b,c) &\mapsto b.
    \end{aligned}
  \end{equation}
  While this is a relatively trivial polynomial, there are more non-trivial
  ones, for example
  \begin{equation}
    \begin{aligned}
      \tilde{f}\colon \mathbb{F}^{3} &\rightarrow \mathbb{F}^{3} \\
      (a,b,c) &\mapsto a^{3}c^{3} + b^{2} - ac.
    \end{aligned}
  \end{equation}
  Notice that on $\{0, 1\}$, $x^{2} = x$, which allows us to see that
  $\tilde{f}$ is a valid extension of $f$.
\end{example}

\begin{defn}[{\cite[Def.\ 2.2]{AW09}}]\label{def:ext-oracle-class}
  For any complexity class $\mathcal{C}$ and extension oracle $\tilde{A}$, the complexity
  class $\mathcal{C}^{\tilde{A}}$ is the class of all languages determinable by a Turing
  machine with access to $\tilde{A}$ with the requirements for $\mathcal{C}$.
\end{defn}

Next, we need to formally define what algebrization is.

\begin{defn}[{\cite[Def.\ 2.3]{AW09}}]\label{def:algebrization}\index{algebrization}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \subseteq \mathcal{D}$. We say the result
  $\mathcal{C} \subseteq \mathcal{D}$ \emph{algebrizes} if $\mathcal{C}^{A} \subseteq \mathcal{D}^{\tilde{A}}$ for all oracles $A$ and
  finite field extensions $\tilde{A}$ of $A$. Conversely, if there exists $A$
  and $\tilde{A}$ such that $\mathcal{C} \nsubseteq \mathcal{D}$, we say that the result $\mathcal{C} \subseteq \mathcal{D}$ \emph{does
    not algebrize}.
\end{defn}

\begin{defn}[{\cite[Def.\ 2.3]{AW09}}]\label{def:algebrization-neq}
  Let $\mathcal{C}$ and $\mathcal{D}$ be complexity classes such that $\mathcal{C} \nsubseteq \mathcal{D}$. We say the result
  $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{algebrizes} if $\mathcal{C}^{A} \nsubseteq \mathcal{D}^{\tilde{A}}$ for all oracles $A$ and
  finite field extensions $\tilde{A}$ of $A$. Conversely, if there exists $A$
  and $\tilde{A}$ such that $\mathcal{C} \subseteq \mathcal{D}$, we say that the result $\mathcal{C} \nsubseteq \mathcal{D}$ \emph{does
    not algebrize}.
\end{defn}

\section{Algebraic query complexity}\label{sec:alg-query-complexity}

Similarly to how we defined query complexity in \cref{sec:query-complexity}, our
notion of algebrization requires a definition of \emph{algebraic} query
complexity. % TODO: More (connect to previous section)

\begin{defn}[{\cite[Def. 4.1]{AW09}}]\label{def:aqc}\index{query complexity!algebraic}
  Let $f\colon \{0, 1\}^{N} \rightarrow \{0, 1\}$ be a Boolean function, $\mathbb{F}$ be a
  field, and $c$ be a positive integer. Also, let $\mathbb{M}$ be the set of
  deterministic algorithms $M$ such that $M^{\tilde{A}}$ outputs $f(A)$ for
  every oracle $A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ and every finite field
  extension $\tilde{A}\colon \mathbb{F}^{n} \rightarrow \mathbb{F}$ of $A$ with
  $\mdeg(\tilde{A}) \le c$. Then, the \emph{deterministic algebraic query
    complexity} of $f$ over $\mathbb{F}$ is defined as
  \begin{equation}
    \tilde{D}_{\mathbb{F}, c}(f) = \min_{M \in \mathcal{M}}\mleft(
      \max_{A, \tilde{A}: \mdeg(\tilde{A}) \le c}T_{M}(\tilde{A})
    \mright),
  \end{equation}
  where $T_{M}(\tilde{A})$ is the number of queries to $\tilde{A}$ made by
  $M^{\tilde{A}}$.
\end{defn}

Our goal here is to find the \emph{worst}-case scenario for the \emph{best}
algorithm that calculates the property $f$. The difference between this and
\cref{def:det-qc} is twofold: first, our algorithm $M$ has access to
an extension oracle of $A$, and second, that we can limit our $\tilde{A}$ in
its maximum multidegree. For the most part, we will focus on equations with
multidegree 2, which is enough to get the results we want.

As an example, let us look at the same $\OR$ problem we defined in
\cref{def:or-problem}.

\begin{thm}[{\cite[Thm.\ 4.4]{AW09}}]\label{thm:or-algebraic}
  $\tilde{D}_{\mathbb{F},2}(\OR) = 2^{n}$ for every field $\mathbb{F}$.
\end{thm}

\begin{proof}
  First note that $2^{n}$ is an upper bound for the number of queries necessary
  since we can query every point in $\{0, 1\}^{n}$, of which there are $2^{n}$.

  Let $M$ be a deterministic algorithm and let $\mathcal{Y}$ be the set of points queried
  by $M$ in the case where $M$ always receives $0$ as a response. So long as
  $\abs{\mathcal{Y}} < 2^{n}$, there exists by \cref{thm:multiquad-extension} a
  multiquadratic extension polynomial
  $\tilde{A}\colon \mathbb{F}^{n} \rightarrow \mathbb{F}$ such that $\tilde{A}(y) = 0$ for
  all $y \in \mathcal{Y}$ but $\tilde{A}(w) = 1$ for some $w \in \{0, 1\}^{n}$. As such, if
  $M$ queries less than $2^{n}$ points then it would not be able to tell the
  difference between $\tilde{A}$ and the zero function. However, $\OR(A) = 1$
  and $\OR(0) = 0$, so it would get the incorrect answer for one of them. Hence
  if $M$ queries fewer than $2^{n}$ points it cannot solve the $\OR$ problem.
\end{proof}

Note that this works even if $M$ is adaptive: if $M$ ever receives a nonzero
response it (correctly) knows $\OR(A) = 1$, so it can accept immediately. As
such, we know that any contradiction must come when $M$ has only ever seen zeros
as responses.

This gives us a potentially counterintuitive property of algebraic query
complexity: while it would seem that giving our machine a polynomial (and a
polynomial of multidegree only 2, at that) would give us the ability to solve
the hardest problems more quickly, that turns out not to be the case.

Now, while this is true for polynomials of multidegree 2, it turns out that if
we restrict our oracles to being simply \emph{multilinear} polynomials, we do
get a speedup.

\begin{thm}[{\cite[Thm. 3]{JKRS09}}]\label{thm:or-multilinear}
  $\tilde{D}_{\mathbb{F},1}(\OR) = 1$ for every field $\mathbb{F}$ with
  characteristic not equal to $2$.
\end{thm}

\begin{proof}
  Let $A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ and $\tilde{A}$ be our extension
  polynomial. Consider the value of $p(1/2, \ldots, 1/2)$. We aim to show that this
  value is equal to $0$ if and only if $A$ is the zero oracle.

  Consider the function
  \begin{equation}
    p'(x_{1}, \ldots, x_{n}) = p(1 - 2x_{1}, \ldots, 1 - 2x_{n}).
  \end{equation}
  Since $1 - 2x$ is a linear polynomial, it follows that $p'$ is itself a
  multilinear polynomial. Further, since the sum over $\{1, -1\}^{n}$ of a
  non-constant multilinear monomial is 0 as per \cref{lem:monomial-sum}, it
  follows that
  \begin{equation}
    \sum_{b \in \{-1, 1\}^{n}}p'(b) = p'(0, \ldots, 0),
  \end{equation}
  i.e., the constant term of $p'$. Further, from our definition of $p'$, we have
  that $p'(0, \ldots, 0) = p(1/2, \ldots, 1/2)$. Hence, we have
  \begin{equation}
    \sum_{b \in \{0, 1\}^{n}}p(b) = p(1/2, \ldots, 1/2).
  \end{equation}
  Since $p(b) \ge 0$ for all $b \in \{0, 1\}^{n}$, it follows that $p(1/2, \ldots, 1/2)$
  is 0 if and only if $p(b) = 0$ for all $b \in \{0, 1\}^{n}$, i.e.\ exactly when
  $A$ is the zero function.
\end{proof}

\section{Algebrization of $\P$ vs.\ $\NP$}\label{sec:alg-p-np}

As with relativization, an important application of algebrization is in regards
to the $\P$ vs.\ $\NP$ problem.

In order to work with algebrization, first we need a stronger definition than
completeness.

\begin{defn}[{\cite[Def.\ 6.1]{BFL90}}]\label{def:pspace-robust}\index{PSPACE-robust@$\PSPACE$-robust}
  A language $L$ is \emph{$\PSPACE$-robust} if $\P^{L} = \PSPACE^{L}$.
\end{defn}

The idea behind robustness is that a robust language is one that is complex
enough that $\PSPACE$ algorithms that use it as an oracle can not glean any more
information than a $\P$ algorithm with the same oracle. As a side note, the
existence of at least one $\PSPACE$-robust language $L$ implies that the
statement $\P = \PSPACE$ (which is probably true but has not been proven) is
non-relativizing.

\begin{lemma}\label{lem:complete-is-robust}
  Any $\PSPACE$-complete language is also $\PSPACE$-robust.
\end{lemma}

\begin{proof}
  First, we know from \cref{thm:relativizing-increases} that
  $\P^{L} \subseteq \PSPACE^{L}$. Next, let $M \in \PSPACE^{L}$, and we aim to show
  $M \in \P^{L}$. Since $L \in \PSPACE$ and $\PSPACE$ is low for itself, we know
  $M \in \PSPACE$. As such, we know there is a polynomial-time reduction $f$ from
  $M$ to $L$. Hence, we can compute $M$ by running $f$ on the input and then
  testing if that output is in $L$ (using the oracle). Hence, $M \in \P^{L}$ and
  thus $\P^{L} = \PSPACE^{L}$.
\end{proof}

\begin{lemma}[{\cite[Lemma 6.2]{BFL90}}]\label{lem:multilinear-is-pspace}
  Let $L$ be a $\PSPACE$-robust language, with corresponding oracle $A$. Let
  $\tilde{A}$ be the unique multilinear extension oracle of $A$. Then the
  language
  \begin{equation}
    \tilde{L} = \bigcup_{n \in \mathbb{N}}\{(x_{1}, \ldots, x_{n}, z) \in \mathbb{F}^{n+1} \mid \tilde{A}(x_{1}, \ldots, x_{n}) = z\}
  \end{equation}
  is polynomially-equivalent to $L$; that is, $\tilde{L} \in \P^{L}$ and
  $L \in \P^{\tilde{L}}$.
\end{lemma}

To improve clarity, we present a novel proof of this statement; this differs
substantially from that originally presented in~\cite{BFL90}.

\begin{proof}
  First, we provide a polynomial-time reduction from $L$ to $\tilde{L}$. Since
  for all $x \in \{0, 1\}^{n}$, $\tilde{A}(x) = 1$ if and only if $x \in L$, it
  follows that
  \begin{equation}
    \begin{aligned}
      f\colon \Sigma^{*} &\rightarrow \Sigma^{*} \\
      x &\mapsto (x, 1)
    \end{aligned}
  \end{equation}
  is a polynomial-time reduction from $L$ to $L'$.

  \begin{algorithm}[htbp]
    \KwIn{$(x_{1}, \ldots, x_{n}, z) \in \mathbb{F}^{n+1}$}
    \KwOut{Whether $\tilde{A}(x_{1}, \ldots, x_{n}) = z$}
    $z' \leftarrow 0$\;
    \For{$k \in \{0, 1\}^{n}$}{\nllabel{line:for-z-prime}
      Simulate $L$ on input $k$\;
      \If{$k \in L$}{
        \tcp{Compute $d_{k}(x)$}
        $d \leftarrow 1$\;
        \For{$i$ from $1$ to $n$}{\nllabel{line:for-d}
          \eIf{$k_{i} = 1$}{
            $d \leftarrow d \cdot x_{i}$\;
          }{
            $d \leftarrow d \cdot (1 - x_{i})$\;
          }
        }
        $z' \leftarrow z' + d$\;
      }
    }
    \Return{whether $z = z'$}\;
    \caption{Determiner for $\tilde{L}$}\label{alg:l-tilde-det}
  \end{algorithm}

  Next, consider \cref{alg:l-tilde-det}. This algorithm simply calculates the
  value of $\tilde{A}(x_{1}, \ldots, x_{n})$ directly, from the explicit definition
  we gave in \cref{cor:low-degree-boolean}, and then compares it to the value of
  $z$. As such, this is a determiner for $L$.

  We now demonstrate that \cref{alg:l-tilde-det} runs in $\P^{L}$. From the
  definition of $\PSPACE$-robustness, we know that we only need to show that the
  algorithm runs in $\PSPACE^{L}$, a much weaker bound. The inner for-loop runs
  in polynomial \emph{time}, hence it must run in polynomial space. The outer
  for-loop runs for $2^{n}$ iterations, so determining that it is in $\P^{L}$ is
  non-trivial. Beyond the inner loop (which we have already discussed), the only
  thing we do in the outer loop is simulate $L$, which can be done in one step
  with access to an oracle for $L$.

  The only memory we need to simulate this oracle (beyond that for the input) is
  space for $d$ and $z'$. We have already shown $d$ needs polynomial space, so
  what remains is $z'$. Since $A(x_{1}, \ldots, x_{n}) \in \{0, 1\}$, each term in the
  sum in \cref{eqn:low-deg-ext-small} is bounded above by $\delta_{\beta}(x)$. This means
  that the value of $z'$ that we compute is bounded above by
  \begin{equation}
    2^{n}\max_{k \in \{0, 1\}^{n}}\delta_{k}(x).
  \end{equation}
  Since each $\delta_{k}(x)$ can be written in polynomial space, and $2^{n}$ can be
  \emph{written} in polynomial space, it follows that $z'$ can as well. Hence,
  \cref{alg:l-tilde-det} is in $\PSPACE^{L}$, and thus is in $\P^{L}$.

  Next, we show that \cref{alg:l-tilde-det} determines $\tilde{L}$. As mentioned
  earlier, our algorithm computes $\tilde{A}$ directly through the equations
  given in \cref{cor:low-degree-boolean}. First, we show the inner loop
  (beginning on line~\ref{line:for-d}) computes $\delta_{k}(x)$. We compute $\delta$
  directly, through the formula described at \cref{eqn:delta-poly-small}. We do
  this by simply iterating through each $i$ and then multiplying $d$ by either
  $x_{i}$ or $1-x_{i}$, as appropriate.

  Second, in this case \cref{eqn:low-deg-ext-small} simplifies to
  \begin{equation}
    \tilde{A}_{n}(x_{1}, \ldots, x_{n}) = \sum_{\beta \in L}\delta_{\beta}(x_{1}, \ldots, x_{n}).
  \end{equation}
  This is exactly what our outer loop does: computes the sum directly through
  iteration.
  Hence, the only thing the above algorithm does is calculate
  $\tilde{A}_{n}(x_{1}, \ldots, x_{n})$ and then compares it to the value we were
  given. As such, it determines $\tilde{L}$.

  Since there is a reduction from $L$ to $\tilde{L}$, we know that $L$ is no
  harder than $\tilde{L}$, and \cref{alg:l-tilde-det} demonstrates that
  $\tilde{L} \in \PSPACE$. Hence, $\tilde{L}$ is $\PSPACE$-complete.
\end{proof}

With that as a base, we can now move on to the main theorem. As before, the more
straightforward proof is the oracle where $\P^{\tilde{A}} = \NP^{A}$, so we
begin with that.

\begin{thm}[{\cite[Theorem 5.1]{AW09}}]\label{thm:p-np-alg}
  There exist $A$, $\tilde{A}$ such that $\NP^{A} = \P^{\tilde{A}}$.
\end{thm}

\begin{proof}
  For this theorem, we use the same technique we did in our proof of
  \cref{thm:p-np-rel}: find a $\PSPACE$-complete language $A$ and work from
  there. If we let $\tilde{A}$ be the unique multilinear extension of $A$,
  \cref{lem:multilinear-is-pspace} tells us $\tilde{A}$ is $\PSPACE$-complete.
  Hence, as mentioned before, we have
  $\NP^{\tilde{A}} \subseteq \NP^{\PSPACE} \subseteq \NPSPACE$, and since $\NPSPACE = \PSPACE$
  and we know from \cref{thm:p-np-rel} that $\PSPACE \subseteq \P^{A}$, it follows
  \begin{equation*}
    \NP^{\tilde{A}} = \NP^{\PSPACE} = \PSPACE = \P^{A}.
  \end{equation*}
\end{proof}

Now it is time for the other case.

\begin{thm}[{\cite[Theorem 5.3]{AW09}}]\label{thm:p-np-nalg}
  There exist $A$, $\tilde{A}$ such that $\NP^{A} \ne \P^{\tilde{A}}$.
\end{thm}

\begin{proof}
  Like in \cref{thm:p-np-nrel}, we aim to ``diagonalize'': iterate over all
  $\P^{\tilde{A}}$ machines to construct a language that none of them can
  recognize. Also like before, we will do this by constructing an oracle
  extension $\tilde{A}$ such that $L(A) \notin \P^{\tilde{A}}$. Since we only give an
  algebraic extension to $\P$ and not $\NP$, we can reuse the result from
  \cref{lem:l(x)-in-np} that $L(A) \in \NP^{A}$. We shall construct $\tilde{A}$
  using the following algorithm:
  \begin{algorithm}[htbp]
    \KwIn{A sequence of $\P$ oracle machines $\{P_{i}\}_{i=1}^{\infty}$}
    \KwOut{An extension oracle $\tilde{A}$ such that $L(A) \notin \P^{\tilde{A}}$}
    $\tilde{A} \leftarrow \varnothing$\;
    $n_{0} \leftarrow 0$\;
    \For{$i$ starting at $1$}{
      Let $n > n_{i}$ be large enough that
      $p_{i}(n) < 2^{n}$\;
      Run $P_{i}^{\tilde{A}}$ on input $0^{n}$\;
      \eIf{$P_{i}^{B(i-1)}$ rejects $0^{n}$}{
        Let $\mathcal{Y}_{\mathbb{F}}$ be the set of all $y \in \mathbb{F}^{n_{i}}$ queried
        during the above computation\;
        \tcp{See \cref{lem:multiquad-adversary} for why we can do this}
        Let $w \in \{0, 1\}^{n}$ such that the following
        works\;\nllabel{line:def-w}
        \For{all $\mathbb{F}$}{
          Set $\tilde{A}_{n_{i},\mathbb{F}}$ to be a multiquadratic polynomial
          such that $\tilde{A}_{n_{i},\mathbb{F}}(w) = 1$ and
          $\tilde{A}_{n_{i},\mathbb{F}}(y) = 0$ for all
          $y \in \mathcal{Y}_{\mathbb{F}} \cup (\{0, 1\}^{n_{i}} \setminus \{w\})$\;\nllabel{line:set-a}
        }
      }{
        Set $\tilde{A}_{n_{i},\mathbb{F}} = 0$ for all $\mathbb{F}$\;
      }
      $n_{i+1} \leftarrow 2^{n}$\;
    }
    $B \leftarrow \bigcup_{i}B(i)$\;
    \caption{An algorithm for constructing $\tilde{A}$}\label{alg:construct-a-tilde}
  \end{algorithm}
  As before, we will start by demonstrating soundness and then move on to why
  the constructed oracle provides the separation we seek.

  Perhaps the least intuitive section of the above algorithm is the section
  beginning at line~\ref{line:def-w}. We want to leverage
  \cref{lem:multiquad-adversary} to show that such a solution exists. We know
  that $p_{i}(n) < 2^{n}$, and since $p_{i}(n)$ is an upper bound on the number
  of total queries, this tells us that there is at least one
  $w \in \{0, 1\}^{n_{i}}$ not queried. From the definition of $\mathcal{Y}_{\mathbb{F}}$,
  we also therefore know that $\sum_{\mathbb{F}}\mathcal{Y}_{\mathbb{F}} < 2^{n}$. Further
  setting up this lemma, we will let $f$ be the zero function and
  $p_{\mathbb{F}}$ be the zero polynomial.

  From the lemma, we know that there is some $B \in \{0, 1\}^{n}$ with
  $\abs{B} < 2^{n}$ such that for all $f'$ agreeing with $f$ there exists a
  series of $p_{\mathbb{F}}'$ extending $f'$ and agreeing with $p_{\mathbb{F}}$
  on $\mathcal{Y}_{\mathbb{F}}$. As such, if we pick any $w \in \{0, 1\}^{n} \setminus B$, then the
  function $f'(x) = [x = w]$ agrees with $f$ on $B$, and thus we know that there
  exists a series of $p_{\mathbb{F}}'$ that agree with the zero polynomial on
  $\mathcal{Y}_{\mathbb{F}}$ and each non-$w$ Boolean point.

  Now, we know that such a solution exists, and \cref{eqn:p-prime} gives us an
  explicit formula for our $A_{n_{i},\mathbb{F}}$; thus, we know that this is in
  fact computable. Since this algorithm is simply for \emph{constructing} the
  language, we do not care about time or space complexity, so the fact that it
  is computable is enough. In terms of finding the $w$ we need, we can simply
  iterate try the construction for each $w \in \{0, 1\}^{n}$ and stop as soon as
  we are able to construct each polynomial.

  The other component of soundness is determining how we can run $P_{i}$ with
  the extension oracle $\tilde{A}$ when $\tilde{A}$ is not yet fully
  constructed. What we do is when simulating $P_{i}$, we assume that any
  $\tilde{A}_{n_{i},\mathbb{F}}$ that we have not yet queried returns zero on
  all queried inputs. We then make sure that any time we set an
  $\tilde{A}_{n_{i},\mathbb{F}}$, it also returns zero on any point that we
  queried. Further, we ensure that each $n_{i}$ is large enough that no previous
  machine would have queried any string of length $n_{i}$ on its respective
  input; ergo modifying these polynomials would not have any affect on their
  output.

  Next, we show that $L(A)$ is not in $\P^{\tilde{A}}$. As we did in
  \cref{thm:p-np-nrel}, the idea is that for each polynomial-time machine
  $P_{i}$, that machine will return the incorrect result on the string
  $0^{n_{i}}$. We do this in \cref{alg:construct-a-tilde} by simulating $P_{i}$
  on the input, and then adjusting $\tilde{A}$ based on its output. We separate
  this into two cases: the case where $P_{i}^{\tilde{A}}$ rejects $0^{n_{i}}$,
  and the case where it accepts. We shall begin with the case where it accepts.

  When $P_{i}^{\tilde{A}}$ accepts, we want to ensure that no strings of length
  $n_{i}$ are in $A$. The unique low-degree extension of the zero function is
  the zero polynomial; hence, we set $\tilde{A}_{n_{i},\mathbb{F}}$ to be $0$
  for all $\mathbb{F}$. This ensures $\tilde{A}(x) = 0$ for all
  $x \in \{0, 1\}^{n_{i}}$, and thus $A \cap \{0, 1\}^{n_{i}} = \varnothing$. This
  means $0^{n_{i}} \notin L(A)$ and thus $P_{i}^{\tilde{A}}$ is incorrect.

  When $P_{i}^{\tilde{A}}$ accepts, we want to make sure that there is at least
  some string $w \in A \cap \{0, 1\}^{n_{i}}$, but also to make sure that any
  polynomials we add have their values align with what $P_{i}^{\tilde{A}}$
  already saw. As we mentioned earlier, we know that such a polynomial exists,
  and thus we construct it. Since our constructed polynomials tell us that
  $w \in A$, it follows that $0^{n_{i}} \in L(A)$ and hence $P_{i}^{\tilde{A}}$ is
  incorrect there as well.

  Since our argument earlier told us that none of the $P_{i}$ machines would
  have their output affected by any of the polynomials modified outside of the
  corresponding iteration $i$, it follows that no machine $P_{i}$ could
  recognize $L(A)$. Since $P_{i}$ includes every machine recognizing a
  $\P^{\tilde{A}}$ language, it follows that $L(A) \notin \P^{\tilde{A}}$.
\end{proof}

\chapter{Interactive proof systems}\label{chap:ips}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[every text node part/.style={align=center}]
    \draw[rounded corners] (-2.5, -1) rectangle (2.5, 1) node [midway]
    {$\PCP[\log(n), O(1)]$ \\ $\NP$};
    \draw[rounded corners] (-3, -1.5) rectangle (3, 2.5);
    \node[anchor=north] at (0, 2.25) {$\PCP[\poly(n), O(1)]$ \\ $\MIP$ \quad $\NEXP$};
    \draw[rounded corners] (-3.5, -2) rectangle (3.5, 3.5);
    \node[anchor=north] at (0, 3.25) {$\IP$ \quad $\PSPACE$};
    \draw[rounded corners] (-4, -2.5) rectangle (4, 4.5);
    \node[anchor=north] at (0, 4.25) {$\MIP*$ \quad $\RE$};
  \end{tikzpicture}
  \caption{The interactive-proof classes and their
    relationships}\label{fig:ip-class-venn}
\end{figure}

Interactive proof systems are models of computation that involve multiple Turing
machines exchanging messages between each other. In general, the machines are
split into two categories: those that are computationally unbounded but
untrustworthy (the provers), and those that are bounded but trustworthy (the
verifiers). The ``goal'' of the system is to convince the verifier of whether or
not the string is in the language. These systems almost always use randomness as
part of their design: for this reason, almost all of the bounds are ``with high
probability'' bounds and not complete mathematical certainty.

Interactive proof systems turn out to be surprisingly powerful---while the
verifier only runs in polynomial time, it turns out that the interaction with
the untrustworthy computer is still enough to boost the power significantly. The
``classic'' interactive proof model involves exactly two computers (one prover,
one verifier), but many variants exist, all with distinct and interesting
complexity-theoretic characteristics. In this chapter, we will introduce a good
number of these variants and a few of their interesting properties; the goal of
the remaining chapters will be to prove several of the more modern interesting
results involving these systems.

\section{Interactive Turing machines}\label{sec:interactive-tm}

The central mechanism underlying all of the interactive proof systems we will
work with is the interactive Turing machine. This machine is a variant of a
standard Turing machine, but it has the ability to communicate with another
machine as part of its work. When multiple interactive machines work together,
they can produce a joint computation in the same way that a single
non-interactive machine can. From there, an interactive proof is just a pair of
interactive machines working together, with some particular constraints on what
they are allowed to do.

\begin{defn}[{\cite[Def.\ 4.2.1]{Go01}}]\label{def:interactive-tm}\index{Turing machine!interactive}
  An \emph{interactive Turing machine} is a deterministic multi-tape Turing
  machine with the following tapes:
  \begin{itemize}
    \item Input tape (read-only)
    \item Output tape (write-only)
    \item Two communication tapes (one read-only, one write-only)
    \item One-cell switch tape (read-write)
    \item Work tape (read-write)
  \end{itemize}
  In addition to these tapes, an interactive TM has a single bit $\sigma \in \{0, 1\}$
  associated with it, called its \emph{identity}.\index{Turing machine!identity}
  When the content of the switch tape is not equal to the machine's identity,
  the machine performs no computation and is called \emph{idle}.

  In most cases, we will also give the interactive machines a source of
  randomness as well that they can read from. Since this is so common we will
  treat it as the default; if we ever want a machine to not have a source of
  randomness we will explicitly state as such.
\end{defn}

On its own, a single interactive Turing machine is not worth much: in order to
do work with these we need to define how a pair of them interact. The chief
mechanism of interacting Turing machines is that of \emph{shared
  tapes}.\index{shared tape} Shared tapes are tapes where any modifications can
be seen by both Turing machines immediately. While the tapes themselves are
shared, the \emph{heads} are not: the two machines are perfectly capable of
looking at different entries at the same time.

\begin{defn}[{\cite[Def.\ 4.2.2]{Go01}}]\label{def:linked-tms}\index{Turing machine!linked pair}
  A pair of interactive Turing machines $(M, N)$ are \emph{linked} if the
  following are true:
  \begin{enumerate}
    \item The identity of $M$ is distinct from the identity of $N$.
    \item The switch tapes of $M$ and $N$ coincide (i.e., writing to one affects
          the value in both).
    \item The read-only communication tape of $M$ coincides with the write-only
          communication tape of $N$.
    \item The read-only communication tape of $N$ coincides with the write-only
          communication tape of $M$.
  \end{enumerate}
\end{defn}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[>={Stealth[scale=1.25]}]
    \draw (0,0) rectangle ++(3,3) node [pos=0.5] {$\id = 0$};
    \draw (8,0) rectangle ++(3,3) node [pos=0.5] {$\id = 1$};

    \draw (5.25,4) rectangle ++(0.5,0.5) node [pos=0.5] {$\sigma$};
    \draw[<->] (5.25,4) -- (3,3);
    \draw[<->] (5.75,4) -- (8,3);

    \foreach \y in {0,...,4} {
      \pgfmathsetmacro{\yvar}{2.75 - 0.5 * \y}
      \draw (4.5,\yvar) rectangle ++(0.5,-0.5);
      \draw (6,\yvar) rectangle ++(0.5,-0.5);
    }
    \path (4.5,-0.25) rectangle ++(0.5,0.5) node [pos=0.5] {$\vdots$};
    \path (6,-0.25) rectangle ++(0.5,0.5) node [pos=0.5] {$\vdots$};
    \draw[->] (3,1.5) -- (4.5,1.5);
    \draw[->] (8,1.5) -- (6.5,1.5);
    \draw[<-] (3,2.5) to [bend left] (6,2.75);
    \draw[<-] (8,2.5) to [bend right] (5,2.75);

    \node at (5.5,-0.75) {Communication tapes};

    \foreach \x in {0,...,9} {
      \pgfmathsetmacro{\xvar}{3 + 0.5 * \x}
      \draw (\xvar,5) rectangle ++(0.5,0.5);
    }
    \draw[->] (3,5) to (1.5,3);
    \draw[->] (8,5) to (9.5,3);

    \node[above] at (5.5,5.5) {Input tape};

    \foreach \x in {0,...,9} {
      \pgfmathsetmacro{\xvar}{3 + 0.5 * \x}
      \draw (\xvar,-2.5) rectangle ++(0.5,0.5);
    }
    \draw[->] (1.5,0) to (3,-2);
    \draw[->] (9.5,0) to (8,-2);

    \node[below] at (5.5,-2.5) {Output tape};

    \foreach \y in {0,...,4} {
      \pgfmathsetmacro{\yvar}{2.75 - 0.5 * \y}
      \draw (-2,\yvar) rectangle ++(0.5,-0.5);
      \draw (12.5,\yvar) rectangle ++(0.5,-0.5);
    }
    \path (-2,-0.25) rectangle ++(0.5,0.5) node [pos=0.5] {$\vdots$};
    \path (12.5,-0.25) rectangle ++(0.5,0.5) node [pos=0.5] {$\vdots$};
    \draw[<->] (0,1.5) -- (-1.5,1.5);
    \draw[<->] (11,1.5) -- (12.5,1.5);

    \node[right] at (-2,-0.75) {Work tape};
    \node[left] at (13,-0.75) {Work tape};

    \foreach \x in {0,10.25} {
      \draw[rounded corners] (\x,5) rectangle ++(0.75,0.75);
      \foreach \d in {1,2,3}{
        \pgfmathsetmacro{\offset}{\d*0.75/4}
        \pgfmathsetmacro{\circx}{\x+\offset}
        \pgfmathsetmacro{\circy}{5+\offset}
        \filldraw (\circx,\circy) circle (0.05);
      }
    }
    \draw[->] (0.375,5) to (1,3);
    \draw[->] (10.625,5) to (10,3);
  \end{tikzpicture}
  \caption{A linked pair of interactive Turing machines}\label{fig:linked-pair}
\end{figure}

We include a diagram of how a linked pair of Turing machines interact and share
tape as \cref{fig:linked-pair}. The arrows point in the direction data is able
to flow: read-only tapes have an arrow pointing from them and write-only tapes
have an arrow pointing to them.

\begin{defn}[{\cite[Def.\ 4.2.2]{Go01}}]\label{def:joint-comp}\index{joint computation}
  The \emph{joint computation} of a linked pair of interactive Turing machines
  $(M, N)$ is, on a common input string $x$, the series of computation states
  for both $M$ and $N$ when each is given $x$ as its initial input tape and when
  the initial value of the shared switch tape is $0$. The joint computation
  halts when either machine halts and the halting machine is not idle.
\end{defn}

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=0.5,>=Stealth]
    \draw (0,0) rectangle (5,1);
    \draw (5,4) rectangle (8,5);
    \draw (8,0) rectangle (10,1);
    \draw (10,4) rectangle (14,5);
    \draw (14,0) rectangle (16,1);

    \draw[->] (5,1) to[out=75,in=255] node[fill=white,anchor=center,pos=0.5] {$m_{1}$} (5,4);
    \draw[->] (8,4) to[out=255,in=75] node[fill=white,anchor=center,pos=0.5] {$r_{1}$} (8,1);
    \draw[->] (10,1) to[out=75,in=255] node[fill=white,anchor=center,pos=0.5] {$m_{2}$} (10,4);
    \draw[->] (14,4) to[out=255,in=75] node[fill=white,anchor=center,pos=0.5] {$r_{2}$} (14,1);

    \draw[dashed] (0,4.5) to (5,4.5);
    \draw[dashed] (5,0.5) to (8,0.5);
    \draw[dashed] (8,4.5) to (10,4.5);
    \draw[dashed] (10,0.5) to (14,0.5);
    \draw[dashed] (14,4.5) to (16,4.5);
  \end{tikzpicture}
  \caption{The flow of a joint computation of two interactive
    TMs}\label{fig:joint-comp-flow}
\end{figure}

We will denote the joint computation of machines $M$ and $N$ on input $x$ by
$\ang{M, N}(x)$. Since this output is not deterministic (it will depend on the
values of the random bits read by $P$ and $V$), it is important to note that
this is a random variable, not an individual value.

Finally, we need the concept of a ``view''. A view is in essence a record of
what a machine in a given interaction sees: it is an ordered list of everything
the machine reads in sequence. We will care quite a bit about views throughout
this thesis since views are a record of the ``public'' information of a proof:
when we begin to work with zero knowledge, we will be using the view of an
interaction in order to show that no information is leaked.

\begin{defn}\label{def:view-ip}\index{view}
  The \emph{view} of an interactive Turing machine $M$ in a joint computation on
  input string $x$ is the sequence $(x, r, m_{1}, \ldots, m_{n})$, where $x$ is the
  input string, $r$ is the sequence of random bits seen by $M$, and $m_{i}$ are
  the random messages received by $M$ from $N$. We will denote the view of $M$
  by $\View_{M}^{N}(x)$.
\end{defn}

\section{Single-prover systems}\label{sec:single-prover}

Now that we have a model for letting two machines talk to each other, we can
define the requirements for an interactive proof. We will begin with the
simplest form---that where there is only one prover and one verifier. To make sure
our proof system is useful, we need three restrictions on the machines: to
restrict the complexity of the verifier (lest it simply compute the problem
itself without communication), to require the verifier to generally accept
whenever the input is in the language, and to require the verifier to generally
reject whenever the input is not in the language.

\begin{defn}[{\cite[Def.\ 4.2.4]{Go01}}]\label{def:ips}\index{interactive proof!single-prover}
  An \emph{interactive proof system} is a pair of interactive machines $(P, V)$
  such that $V$ is polynomial-time and the following holds:
  \begin{itemize}
    \item \emph{Completeness}: For every $x \in L$,
          \[
            \mathbb{P}[\ang{P, V}(x) = 1] \ge \frac{2}{3}.
          \]
    \item \emph{Soundness}: For every $x \notin L$ and every interactive machine $B$,
          \[
            \mathbb{P}[\ang{B, V}(x) = 1] \le \frac{1}{3}.
          \]
  \end{itemize}
\end{defn}

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=0.5]
    \draw (0,4) rectangle (4,8) node[midway] {$\mathbb{P} \ge \frac{2}{3}$};
    \draw (4,4) rectangle (8,8) node[midway] {\shrug{}};
    \draw (0,0) rectangle (4,4) node[midway] {$\mathbb{P} \le \frac{1}{3}$};
    \draw (4,0) rectangle (8,4) node[midway] {$\mathbb{P} \le \frac{1}{3}$};
    \node[anchor=east] at (0,6) {$x \in L$};
    \node[anchor=east] at (0,2) {$x \notin L$};
    % Needed to make vertical alignment work out (sigh)
    \node[anchor=south] at (2,8) {Honest};
    \node[anchor=south] at (6,8) {Dishonest};
  \end{tikzpicture}
  \caption{Probability matrix for IP acceptance given prover and input
    string}\label{fig:ip-accept-grid}
\end{figure}

While we require our system to be correct at least $2/3$ of the time, our choice
of probability is actually somewhat arbitrary, so long as it is at least $50\%$.
This is because with a greater than $50\%$ chance of success, we can simply run
the checker multiple times and take the majority vote, which will allow us to
get the probability arbitrarily high. Since this iteration is for a fixed number
of times, it will only linearly scale the runtime and thus it does not affect
whether our algorithm is an interactive proof system.

For the soundness clause, note that we require the inequality to hold for
\emph{any} interactive machine $B$, and not just our chosen machine $P$. This is
important---it says that our verifier cannot be fooled reliably by a dishonest
machine, so long as $x$ is not in the language $L$. In practice, what this means
is that if the verifier has reason to believe that the machine it is interacting
with is not $P$, then it should always reject immediately, as we do not care
what happens with an arbitrary machine when $x \in L$. A consequence of this is
that if $V$ ever receives back improperly-formatted or nonsense input from its
prover, it will reject immediately. Similarly to what we do for ordinary Turing
machines parsing their input, we will not explicitly write out that $V$ should
reject if it receives a poorly-formatted response, as it serves little but to
provide clutter.

We also do not care how $V$ fares if $x \in L$ and $P$ is not the correct
verifier. This is because neither insisting the protocol fail or insisting the
protocol succeed will be a reasonable restriction. Since $x \in L$, an alternative
$P'$ could give messages arbitrarily close to the correct $P$ (and in some cases
even send identical messages, which would be impossible to distinguish), so we
cannot insist $L$ reject, even with high probability. However, if $V$ could
accept even in the face of an arbitrarily malicious prover, or a prover that
sends no useful information whatsoever, it would mean that $V$ would have some
means of computing the problem on its own; hence we would only ever be able to
accept languages in $\BPP$ (since $L$ is a $\BPP$ machine).

As with all of our interactive-proof variants, we will also define a complexity
class corresponding to the set of languages with the given proof. Once we have a
complexity class, we will be able to work with it in the same way we have been
all the ``standard'' classes like $\P$ or $\NSPACE$.

\begin{defn}[{\cite[Def.\ 4.2.5]{Go01}}]\label{def:ip}\index{IP@$\IP$}
  The class $\IP$ is the class of all languages that have an interactive proof
  system.
\end{defn}

Now that we have seen the formal definition of an interactive proof, let us
illustrate the formality with an example. To do so, consider the language of
non-isomorphic graphs:

\begin{defn}\label{def:gi}\index{GI@$\GI$}
  The language $\GI$ (for \emph{graph isomorphism}) is the set
  \[
    \GI = \{(G_{0}, G_{1}) \mid G_{0}, G_{1} \text{ graphs and } G_{0} \cong G_{1}\}.
  \]
\end{defn}

This language is interesting for reasons beyond the scope of this paper,
especially in that it is known to be in $\NP$ but is believed to be neither in
$\P$ nor $\NP$-complete.

\begin{defn}\label{def:gni}\index{GNI@$\GNI$}
  The language $\GNI$ (for \emph{graph non-isomorphism}) is the set
  \[
    \GNI = \{(G_{0}, G_{1}) \mid G_{0}, G_{1} \text{ graphs and } G_{0} \ncong G_{1}\}.
  \]
\end{defn}

Here, we will demonstrate that $\GNI$ has an interactive proof.

\begin{thm}\label{thm:gni-ip}
  The language $\GNI$ is in $\IP$.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{Two $n$-vertex graphs $G_{0}$ and $G_{1}$, and a security parameter $s$}
  \KwOut{Whether $G_{0} \ncong G_{1}$}
  \If{$\abs{V(G_{0})} \ne \abs{V(G_{1})}$ or $\abs{E(G_{0})} \ne \abs{E(G_{1})}$}{
    accept\;
  }
  \For{$i \in [s]$}{
    $V$: Pick a random $\sigma_{i} \in S^{n}$\;
    $V$: Pick a random $b_{i} \in \{0, 1\}$\;
    $V$: Compute $H_{i} \leftarrow \sigma_{i} \cdot G_{b_{i}}$\;
    $V$: Send $H_{i}$ to $P$\;
    $r_{i} \leftarrow$ \eIf{$H_{i} \cong G_{0}$}{
      $P$: Send $0$ to $S$\;
    }{
      $P$: Send $1$ to $S$\;
    }
    \If{$r_{i} \ne b_{i}$}{
      \Reject\;
    }
  }
  \Accept\;
  \caption{An interactive proof for the language $\GNI$}\label{alg:gni-ip}
\end{algorithm}

\begin{proof}
  We present an interactive protocol for $\GNI$ in \cref{alg:gni-ip}. In
  addition to the two graphs, we also give this algorithm one metaparameter $s$:
  the \emph{security parameter}\index{security parameter}. This parameter does
  not need to be dynamic; adjusting it only affects the number of rounds of the
  protocol and correspondingly the probability of outputting the correct value.

  First, we show $V$ runs in polynomial time. Picking a random permutation and
  single bit can be done in polynomial time, and computing the action of a
  permutation on a graph is also polynomial.

  Next, if $G_{0} \ncong G_{1}$ and $P$ is the honest prover, we show $V$ will accept
  with probability $\ge 2/3$. Since $G_{0} \ncong G_{1}$, it follows that
  $\sigma \cdot G_{0} \ncong \sigma' \cdot G_{1}$ for any permutations $\sigma$ and $\sigma'$. Hence, the honest
  prover will always answer with the correct $r_{i} = b_{i}$, and thus $V$ will
  always accept.

  If $G_{0} \cong G_{1}$, we show $V$ will reject with probability $\ge 2/3$,
  regardless of the prover. For any permutations $\sigma$ and $\sigma'$, we have that
  $\sigma \cdot G_{0} \cong \sigma' \cdot G_{1}$, by transitivity of isomorphisms. Further, we have
  that $S_{n}$ is exactly the class of isomorphisms on $n$-vertex labeled
  graphs, so for any isomorphic graph $G$, there exists exactly one $\sigma_{0}$ and
  $\sigma_{1}$ such that $\sigma_{0} \cdot G_{0} \cong G \cong \sigma_{1} \cdot G_{1}$. Hence, the odds of
  $b_{i}$ being $0$ are $1$ are equal for any given $G$. Thus, in each round the
  odds of $P$ guessing correctly are no more than $1/2$. Further, in each round
  the random picks are statistically independent; hence the overall odds of $P$
  fooling $V$ are no more than $2^{-s}$. For any $s > 1$, $2^{-s} < 1/3$; hence
  $V$ will reject with probability $\ge 2/3$.
\end{proof}

Once we have a complexity class, the question arises of how it relates to other
complexity classes. For $\IP$, Adi Shamir proved in 1992~\cite{Sha92} that a
language has a standard interactive protocol if and only if it is in $\PSPACE$.

\begin{thm}[{\cite{Sha92}}]\label{thm:ip-is-pspace}
  $\IP = \PSPACE$.
\end{thm}

\section{Multi-prover systems}\label{sec:multi-prover}

We have now seen quite a bit of single-prover interactive proofs. A natural
extension of the standard interactive proof format is to add more machines to
the interaction. Since our verifiers are trusted, increasing the number of
verifiers is not useful since any pair of verifiers could simply be simulated
with a single verifier working twice as hard (which would keep it polynomial).
However, increasing the number of provers to two turns out to give us more power
than we would get with a single prover.

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}[>=Stealth]
    \draw (0, 0) rectangle ++(1, 1) node [midway] {$V$};
    \draw (-2, 2) rectangle ++(1, 1) node [midway] {$P_{1}$};
    \draw (2, 2) rectangle ++(1, 1) node [midway] {$P_{1}$};
    \draw[<->] (0, 1) to (-1, 2);
    \draw[<->] (1, 1) to (2, 2);
    \draw[very thick,line cap=round] (0.5, 1.5) to (0.5, 3.5);
  \end{tikzpicture}
  \caption{A multi-prover interactive system}\label{fig:mip}
\end{figure}

\begin{defn}[{\cite[Def.\ 4.11.2]{Go01}}]\label{def:mps}\index{interactive proof!multi-prover}
  A \emph{multi-prover interactive proof system} is a triplet of interactive
  machines $(P_{1}, P_{2}, V)$ such that $P_{1}$ and $P_{2}$ cannot communicate,
  $V$ is probabilistic polynomial-time, and the following hold.
  \begin{itemize}
    \item \emph{Completeness}: For every $x \in L$,
          \[
            \mathbb{P}[\ang{P_{1}, P_{2}, V}(x) = 1] \ge \frac{2}{3}.
          \]
    \item \emph{Soundness}: For every $x \notin L$ and every pair of interactive
          machines $B_{1}$ and $B_{2}$,
          \[
            \mathbb{P}[\ang{B_{1}, B_{2}, V}(x) = 1] \le \frac{1}{3}.
          \]
  \end{itemize}
\end{defn}

The above definition should look rather similar to \cref{def:ips}; the only
difference is now we have two provers instead of just one. The fact that the two
provers cannot communicate is important: if they could, they would be able to
``strategize''; that is, agree on a joint plan to make sure that their responses
agree with each other. Since our provers are not required to be computationally
bounded, if they could communicate it would be no different than simply having
one prover. However, since the two provers cannot communicate, we gain some
information from times where they lie in \emph{different} ways: where each
prover individually could say something plausible, but in combination, the
provers' responses contradict.

\begin{defn}\label{def:mip}\index{MIP@$\MIP$}
  The class $\MIP$ is the class of languages that have a multi-prover
  interactive proof system.
\end{defn}

The first question about a class like $\MIP$ is how it relates to other
complexity classes we have already seen. First, if we have a single-prover
system we can always convert it into a multi-prover system by simply having the
verifier ignore $P_{2}$ completely; thus we get the following.

\begin{lemma}\label{lem:ip-in-mip}
  $\IP \subseteq \MIP$.
\end{lemma}

\begin{proof}
  Let $(P, V)$ be an interactive proof system. Consider the system
  $(P_{1}, P_{2}, V')$, where $P_{1} = P$, and $V'$ simulates $V$ and sends all
  its messages to $P_{1}$. If $x \in L$, then $\ang{P_{1}, P_{2}, V'}(x)$ will
  accept with exactly the same probability as $\ang{P, V}(x)$. Since $(P, V)$ is
  an interactive proof system, it follows that it will accept with probability
  at least $2/3$. If $x \notin L$, then $\ang{P_{1}^{*}, P_{2}^{*}, V}$ will reject
  with exactly the same probability as $\ang{P_{1}^{*}, V}$ regardless of the
  value of $P_{2}^{*}$, since the interaction is exactly the same and since we
  ignore the second prover completely. Again, since $(P, V)$ is an interactive
  proof system, it follows that it will reject with probability at least $2/3$
  in this case. Hence, $(P_{1}, P_{2}, V')$ is a MIP system.
\end{proof}

A groundbreaking result by Babai, Fortnow, and Lund~\cite{BFL90} is that $\MIP$
is exactly equal to $\NEXP$. Since it is known that $\NP \ne \NEXP$~\cite{Cook73},
this tells us that adding multiple provers gives an actual boost in
computational power over just having one.

\begin{thm}[{\cite{BFL90}}]\label{thm:mip-is-nexp}
  $\MIP = \NEXP$.
\end{thm}

Having seen how much more powerful systems become with two provers, one might
wonder what would happen if we were to add a third. Unfortunately, it turns out
that a third prover is no more powerful than just having two. We formalize this
below; because we do not get any benefit from three provers we will not work
with three-prover systems at all in this paper beyond this proof.

\begin{thm}[{\cite[Theorem 4]{BGKW88}}]\label{thm:mip-unchanged}
  If we redefine $\MIP$ to have $k$ provers instead of $2$, the class is unchanged.
\end{thm}

\begin{algorithm}[htbp]
  $\hat{V}$: Generate all random bits and send results to $\hat{P}_{1}$\;
  $\hat{P}_{1}$: Send transaction log between $(P_{1}, \ldots, P_{k}, V)$ with the
  chosen randomness\;
  \If{The simulated log is longer than the worst-case runtime of $V$}{
    \Reject\;
  }
  $\hat{V}$: Choose random $i \in [k]$\;
  $\hat{V}$ and $\hat{P}_{2}$: Simulate conversation between $V$ and $P_{i}$
  given coin tosses\;
  \eIf{the simulated conversation does not match the result of $\hat{P}_{1}$}{
    \Reject\;
  }{
    \Accept if and only if $V$ would accept with the given transcript\;
  }
  \caption{A 2-prover MIP simulating a $k$-prover MIP}\label{alg:2-prover-sim-k}
\end{algorithm}

\begin{proof}
  Let $(P_{1}, \ldots, P_{k}, V)$ be a $\MIP$ with $k$ provers. We show a simulator
  for $(P_{1}, \ldots, P_{k}, V)$ in \cref{alg:2-prover-sim-k}. Since $V$ runs in
  polynomial time and all $\hat{V}$ does is look at the simulated transaction
  log of $V$ (twice), it follows that $\hat{V}$ runs in polynomial time.

  Let $x \in L$. By definition, we know that
  \begin{equation}
    \mathbb{P}[\ang{P_{1}, \ldots, P_{k}, V}(x) = 1] \ge \frac{2}{3}.
  \end{equation}
  If $\hat{P}_{1}$ and $\hat{P}_{2}$ are honest, then the simulated
  conversations will match; hence $\hat{V}$ will reject if and only if $P$ would
  have. Thus, $\mathbb{P}[\ang{\hat{P}_{1}, \hat{P}_{2}, \hat{V}} = 1] \ge 2/3$.

  Let $x \notin L$, and let $\hat{P}_{1}^{*}$ and $\hat{P}_{2}^{*}$ be arbitrary
  verifiers. From the definition of MIP, for at least $2/3$ of the choices of
  randomness, the generated transcript would result in $V$ rejecting; hence
  $\hat{P}_{1}$ must deviate from it somewhere. Since $\hat{P}_{1}$ deviates
  from the protocol at least once, it follows there is at least a $1/k$ chance
  that the simulated conversation between $\hat{V}$ and $\hat{P}_{2}$ is
  different from what $\hat{V}$ received from $\hat{P}_{1}$. Hence, the
  probability of correctly rejecting here is at least $2/3k$.

  Note that if the two provers ever disagree, it must be that at least one of
  them is a cheating prover. Thus, if we run it at least $k^{2}$ times we will
  have at least one run where we catch the cheating with probability
  $2k^{2}/3k = 2/3$. Thus, we will correctly reject with probability at least
  $2/3$ regardless of the cheating prover. Hence, $(P_{1}, P_{2}, V)$ is a valid
  MIP system.
\end{proof}

Similarly to how we defined views for single-prover systems, a verifier in a
multi-prover system also has a view. This works similarly to the view of a
single-prover system, but with the main difference being that we need a bit to
signal which machine any given response came from.

\begin{defn}\label{def:view-mip}\index{view!MIP}
  The \emph{view} of the verifier in a multi-prover interactive system is the
  tuple
  \[
    (x, (b_{1}, m_{1}), \ldots, (b_{n}, m_{n})),
  \]
  where $x$ is the input, $m_{i}$ is the $i$th message received, and $b_{i}$ is
  either $0$ or $1$, depending on which machine the message came from.
\end{defn}

In some respects, we can think of the verifier's view as being the ``leaked''
information from an interaction. The framework behind this is the scenario
where we think of multiple computers communicating over some sort of public,
unencrypted link (for example, the internet).

\section{Zero-knowledge proofs}\label{sec:zero-knowledge}

Zero-knowledge proofs are a variant of interactive proofs that have certain
cryptographic requirements. What we care about is the idea that zero-knowledge
proofs transmit \emph{no knowledge} other than precisely the statement trying to
be proved. As an example, if the statement that you are trying to prove is ``I
have an instance of $X$'', the conceptually-easiest way to prove it would be to
produce the aforementioned instance. However, this would not be zero-knowledge
since it also transmits the knowledge of exactly what your instance of $X$ is.

The way we mathematically define zero-knowledge is a little tricky. The way we
demonstrate that the proof is zero-knowledge is by creating a simulator $S_{V}$
for each possible verifier $V$: a machine in $\P$ that \emph{by itself} can
reproduce the entire message log between $P$ and $V$ for any input. This
definition shows that no knowledge has been released because we are able to
reproduce all the public information of the proof with relatively little work.
If non-trivial knowledge were released by the proof, we would not be able to
recreate the interaction faithfully without access to the
prover.\footnote{Exception: if $L \in \P$ then we can trivially recreate the
  interaction no matter what, but that case is not particularly interesting for
  the purpose of zero-knowledge proofs.}

Having said that, it is not particularly obvious that there are any languages
that are outside of $\P$ with perfect zero-knowledge proofs.\footnote{To some
  extent, showing that there are languages \emph{truly} outside of $\P$ would
  require a proof that $\P \ne \NP$ (which is unfortunately beyond the scope of
  this paper), but there are lots of languages strongly believed to be outside
  of $\P$ with zero-knowledge proofs.} It turns out, however, that these
languages do in fact exist (and are reasonably common). Abstractly, the idea
behind why many of these work is that the verifier can perform a transformation
on some random value, such that undoing the transformation and reliably
recovering the original value is only possible with knowledge of the language.
However, a simulator would have access to the randomly-chosen value, and thus it
could construct a response immediately with no reference to the problem to be
solved.

\begin{defn}[{\cite[Def.\ 4.3.1]{Go01}}]\label{def:zero-knowledge}%
  \index{perfect zero-knowledge}\index{perfect simulator}%
  \index{zero-knowledge proof!perfect}
  A proof system $(P, V)$ for a language $L$ is \emph{perfect zero-knowledge} if
  for each probabilistic polynomial-time interactive machine $V^{*}$ there
  exists a probabilistic polynomial-time ordinary machine $M^{*}$ such that for
  every $x \in L$ we have the following conditions hold:
  \begin{enumerate}
    \item With probability at most $1/2$ , on input $x$, machine $M^{*}$ outputs
          a special symbol denoted $\bot$ (i.e.\ $\mathbb{P}[M^{*}(x) = \bot] \le 1/2$).
    \item Let $m^{*}(x)$ be the random variable such that
          \begin{equation}
            \mathbb{P}[m^{*}(x) = \alpha] = \mathbb{P}[M^{*}(x) = \alpha \mid M^{*}(x) \ne \bot]
          \end{equation}
          for all $\alpha$. That is, let $m^{*}(x)$ be the distribution of non-$\bot$
          values of $M^{*}$. Then $\ang{P, V^{*}}(x)$ and $m^{*}(x)$ are
          identically distributed for all $x \in L$.
  \end{enumerate}
  In this case, we say the machine $M^{*}$ is a \emph{perfect simulator} for the
  interaction of $V^{*}$ with $P$.
\end{defn}

Looking at this definition, one might wonder why we would want the ability for
$M^{*}$ to output $\bot$. This is a reasonable thing to wonder, because we do not
actually want this. However, we do not know of any non-trivial proof systems
that do not actually output $\bot$ at least some of the time, although~\cite{GT20}
has made significant inroads on this problem.\footnote{More specifically, they
  have shown that all of $\NP$ has a zero-knowledge proof without use of $\bot$,
  which includes nontrivial problems in the (generally expected) case that
  $\NP \ne \BPP$.} Thankfully, we can make $\mathbb{P}[\bot]$ arbitrarily small (bounded
above by $2^{-\poly(\abs{n})}$), but we cannot make it truly perfect. We can do
this by simply re-running the simulator every time we get a $\bot$ until we get a
valid answer.

Also note that while the definition of interactive proof systems focus on
cheating \emph{provers}, the definition of zero-knowledge focuses on cheating
\emph{verifiers}. This is because we can think of the two as being resilient to
different threat models. For interactive proofs, we care about verifier
efficiency: our goal is to show it is possible for some computer with unbounded
resources to easily convince a verifier of something, and to show that no other
equally-strong computer could lie.

For zero-knowledge, our main goal is to ensure that it is impossible for anybody
except for the honest verifier to extract any information from the honest
prover. The main way we do this is by ensuring that it is impossible for anybody
snooping on the transaction to gain any information (hence the simulator), but
we also want to ensure that the prover cannot be tricked into revealing
information by a dishonest verifier. We do not care about what happens with a
dishonest prover in this case because alternate provers could in theory reveal
anything---there is always a prover that just dumps any relevant information to
the interaction tape, for example.

As with other interactive proof systems, zero-knowledge proofs are
probabilistic; in particular this means they do \emph{not} function as proofs in
the mathematical sense.

\begin{defn}[{\cite[Def.\ 4.3.5]{Go01}}]\label{def:pzk}\index{PZK@$\PZK$}
  The class $\PZK$ is the class of all languages with a perfect zero-knowledge
  proof system.
\end{defn}

To demonstrate perfect zero-knowledge, we now show an example. In
\cref{sec:single-prover}, we demonstrated a non-zero-knowledge interactive proof
for the language $\GNI$ of non-isomorphic graphs. Here, we modify that algorithm
to not reveal anything beyond simply whether $G_{0}$ and $G_{1}$ are isomorphic.

\begin{thm}\label{thm:gi-pzk-ip}
  The language $\GI$ has a perfect zero-knowledge interactive proof.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{Two $n$-vertex graphs $G_{0}$ and $G_{1}$}
  \KwOut{Whether $G_{0} \cong G_{1}$}
  \For{$i \in [s]$}{
    $P$: pick a random $\sigma \in S^{n}$\;
    $P$: pick a random $b \in \{0, 1\}$\;
    $P$: send $\sigma \cdot G_{b}$ to $V$\;\nllabel{line:send-perm}
    $V$: pick a random $b' \in \{0, 1\}$\;
    $V$: send $b'$ to $P$\;\nllabel{line:send-b}
    $P$: compute $\sigma' \in S^{n}$ such that
    $\sigma' \cdot G_{b'} = \sigma \cdot G_{b}$\;\nllabel{line:sigma-prime}
    $P$: send $\sigma'$ to $V$\;
    \If{$\sigma' \cdot G_{b'} \ne \sigma \cdot G_{b}$}{\nllabel{line:confirm-sigma}
      \Reject\;
    }
  }
  accept\;
  \caption{A perfect zero-knowledge IP for $\GI$}\label{alg:gi-pzk-ip}
\end{algorithm}

\begin{algorithm}[htbp]
  $c \leftarrow 0$\;
  $L \leftarrow [\,]$\;
  \For{$i \in [2s]$}{
    \If{$c \ge s$}{
      \KwRet{$L$}\;
    }
    Choose random $\sigma \in S_{n}$\;
    Choose random $b \in \{0, 1\}$\;
    $H \leftarrow \sigma \cdot G_{b}$\;
    Add $H$ to $L$\;
    Simulate $V^{*}$ on input $(G_{0}, G_{1})$ and received message $H$ until it
    sends a message $\sigma'$\;\nllabel{line:sim-vstar-gi}
    \If{$b = b'$}{
      $c \leftarrow c + 1$\;
      Add $\sigma$ to $L$\;
    }
  }
  \KwRet{$\bot$}\;
  \caption{A simulator for \cref{alg:gi-pzk-ip}}\label{alg:gi-simulator}
\end{algorithm}

\begin{proof}
  We present the algorithm as \cref{alg:gi-pzk-ip}. Our proof will consist of
  two stages: first, we will prove that the algorithm is a functional
  interactive proof for $\GI$, and then we will show that it does not leak any
  knowledge.

  First, if $G_{0} \cong G_{1}$ and $P$ is honest, then regardless of what
  parameters we pick, there will always exist a $\sigma'$ we can compute in
  line~\ref{line:sigma-prime}. Hence, the condition in
  line~\ref{line:confirm-sigma} will never be true and hence we will always
  accept.

  If $G_{0} \ncong G_{1}$, then whenever $b' \ne b$ there exists no $\sigma'$ such that
  $\sigma' \cdot G_{b'} = \sigma \cdot G_{b}$. Hence, if $b' \ne b$ (which happens with probability
  $1/2$) then regardless of what $\sigma'$ is sent to $V$ the check in
  line~\ref{line:confirm-sigma} will fail and hence $V$ will reject. Since $b'$
  and $b$ are re-rolled in each round, the probability of them being equal in
  all $s$ rounds is $2^{-s}$. Hence, $V$ will accept with probability no more
  than $2^{-s}$.

  Now, we show zero-knowledge. To do this we show a simulator in
  \cref{alg:gi-simulator}. To summarize, it attempts to simulate a single round
  of the interaction repeatedly until it has $s$ successes; if it cannot do this
  in $2s$ tries, it outputs $\bot$.

  First, we show \cref{alg:gi-simulator} runs in polynomial time. We know
  $V^{*}$ runs in polynomial time by our hypothesis; hence
  line~\ref{line:sim-vstar-gi} runs in polynomial time. All the rest of the
  lines are either random choice of items, computing permutations, or simple
  arithmetic; all of these are also doable in polynomial time. Hence the
  interior of the loop runs in polynomial time, and since we iterate no more
  than $2s$ times, it follows that the whole algorithm is polynomial time.

  Note that an individual round of this simulator can only succeed when
  $b = b'$: if it could output a correct response $\sigma'$ when $b \ne b'$, then it
  would have $\sigma' \circ \sigma^{-1}$ as an isomorphism from $G_{0}$ to $G_{1}$, and thus
  \cref{alg:gi-simulator} would be a probabilistic polynomial-time determiner
  for $\GI$. Since we do not know whether or not $\GI \in \BPP$, we do not yet
  know how to construct any verifier that would do this.

  Since $V^{*}$ never receives $b$ and $b$ is randomly generated, if $G_{0}$ is
  isomorphic to $G_{1}$, for any value of $G$ it is just as likely that $b = 0$
  as it is that $b = 1$. More formally, for all $G$,
  \[
    \mathbb{P}[G \mid b = 0] = \mathbb{P}[G \mid b = 1].
  \]
  As such, for any $(G_{0}, G_{1}) \in \GI$, it is impossible for any $V^{*}$ to
  send $b' \ne b$ with probability more than $1/2$.

  Since, each individual round succeeds with probability at least $1/2$, the
  binomial theorem tells us that the probability of exactly $k$ successes in
  $2s$ tries is $\binom{2s}{k}/2^{2s}$. Hence, the probability of at least $s$
  successes in $2s$ tries is
  \begin{equation}\label{eqn:binom-sum-2s}
    \frac{1}{2^{2s}}\sum_{i=s}^{2s}\binom{2s}{i}.
  \end{equation}
  Since $\binom{2s}{k} = \binom{2s}{2s - k}$ and the sum of $\binom{2s}{k}$ over
  all $k$ is $2s$, it follows that \cref{eqn:binom-sum-2s} is equal to
  \begin{equation}
    \frac{\displaystyle \sum_{i=s}^{2s}\binom{2s}{i}}{\displaystyle \sum_{i=1}^{2s}\binom{2s}{i}}
    = \frac{\displaystyle s + \sum_{i=s+1}^{2s}\binom{2s}{i}}{2\displaystyle \sum_{i=s+1}^{2s}\binom{2s}{i}}
    \ge \frac{1}{2}.
  \end{equation}
  Hence, the algorithm will output $\bot$ with probability no more than $1/2$.

  Next, we show \cref{alg:gi-simulator} outputs an identically-distributed view
  to what $V^{*}$ sees for all $x \in \GI$, regardless of what $V^{*}$ is. We show
  that it outputs an identical view for a single round; since the simulator will
  simulate $s$ rounds it follows that the total result will be identical exactly
  when an individual round is.

  An individual round of \cref{alg:gi-pzk-ip} has a total of three messages
  sent: two from $P$ and one from $V$. The view of a verifier only consists of
  the messages from $P$, so after each round we should be adding two items to
  the log. Our honest prover sends two messages: first, a random graph
  $H \cong G_{b}$ and second, an isomorphism $\sigma$ that maps $G_{b'}$ to $H$.

  The simulator picks a random $b$ and $H$ in exactly the same way as $P$; thus
  $H$ will be identically distributed in the simulator as it is in the original
  algorithm. Next, remember that we only care about the views being identical in
  the case where $(G_{1}, G_{2}) \in \GI$, that is, where $G_{1} \cong G_{2}$. In this
  case, picking a random isomorphic copy of $G_{1}$ will give you an
  identically-distributed random variable to picking a random isomorphic copy of
  $G_{2}$. Similarly, $H \cong G_{1} \cong G_{2}$ by construction, so picking a random
  isomorphic copy of $H$ will also give you an identical distribution. As such,
  $\sigma$ is a random isomorphism and hence the transaction $(H, \sigma)$ is distributed
  identically to the originally-generated transcript.
\end{proof}

\subsection{Commitment schemes}\label{sec:commitment-scheme}

It should not come as too much of a surprise to learn that most intuitive proofs
for a given problem are not in fact zero-knowledge.\footnote{As an example, I
  would certainly hope that the proofs in this text have, in fact, imparted at
  least some knowledge on the reader.} As such, we will want the assistance of a
few techniques that, once understood, will allow us to build zero-knowledge
proofs more easily. The first of these is a \emph{bit-commitment scheme}.

Abstractly, a bit-commitment scheme allows a machine to ``commit to'' a given
single bit, with the intent of revealing it later on to a verifier. To do this,
we need two important things: first, that the bit is not revealed to the
verifier at commitment time, and second, that if the revealed bit is not equal
to the committed bit, the verifier will be able to know (that is, the committer
will not be able to change its choice once it has committed).

Before we can define a bit-commitment scheme formally, we do need a few
preliminaries. First, we define what it means for an interaction to look like a
commitment from the receiver's perspective; since the receiver needs to be
convinced of the commitment, we need a definition that only considers its
perspective.

\begin{defn}\label{def:possible-commit}\index{possible commitment}
  Let $\sigma \in \{0, 1\}$. A receiver's view of an interaction
  $(x, r, m_{1}, \ldots, m_{n})$ is a \emph{possible $\sigma$-commitment} if there exists
  a string $s$ such that $m_{i}$ describes the messages received by $R$ when $R$
  uses local coins $r$ and interacts with machine $S$ that uses local coins $s$
  and has input $(\sigma, 1^{n})$.
\end{defn}

The above definition does not preclude a series of messages looking like it
could be both a $0$-commitment and a $1$-commitment; consider the case of a
machine $S$ that completely ignores its input and sends the same series of
strings. In this case, the record of that interaction would be a possible
commitment for any input, since the input is ignored completely.

\begin{defn}\label{def:ambiguous-view}\index{ambiguous view}
  A receiver's view is \emph{ambiguous} if it is both a possible $0$-commitment
  and a possible $1$-commitment.
\end{defn}

Now, we can define a bit-commitment scheme. In brief, a bit-commitment scheme is
a scheme such that there are no ambiguous views and yet the total
publicly-released information is ambiguous.

\begin{defn}[{\cite[Def.\ 4.4.1]{Go01}}]\label{def:commitment-scheme}\index{commitment scheme}
  A \emph{bit-commitment scheme} is a pair of interactive probabilistic
  polynomial-time machines $(S, R)$, such that
  \begin{enumerate}
    \item Both machines receive an integer $n$ in unary,
    \item $S$ receives a single bit $v$,
    \item For any PPT machine $R'$, the output of $\ang{S(0), R'}(1^{n})$ and
          $\ang{S(1), R'}(1^{n})$ are computationally indistinguishable over all
          inputs $n$, and
    \item For almost all coin tosses of $R$, there exists no sequence of
          messages from $S$ such that the view of $R$ is ambiguous.
  \end{enumerate}
\end{defn}

The simplest examples of bit-commitment schemes involve one-way
functions\index{one-way function}. A \emph{one-way function} is a function that
is computable in polynomial time, but whose inverse is \emph{not} computable in
polynomial time. These functions are not known to exist: in particular their
existence implies $\P \ne \NP$, but they are still widely believed to exist.

\begin{algorithm}[htbp]
  \KwIn{A one-way function $G\colon \{0, 1\}^{*} \rightarrow \{0, 1\}^{*}$ such that
    $\abs{G(s)} = 3\abs{s}$ for all $s \in \{0, 1\}^{*}$}
  \tcc{Commit}
  $R$: Pick uniform $r \in \{0, 1\}^{3n}$ and send to $S$\;
  $S$: Let $v \in \{0, 1\}$ be the committed-to bit\;
  \eIf{$v = 0$}{
    $S$: Send $\alpha = G(s)$ to $R$\;
  }{
    $S$: Send $\alpha = G(s) \oplus r$ to $R$\;
  }
  \tcc{Reveal}
  $S$: Send $s$ to $R$\;
  \uIf{$G(s) = \alpha$}{
    $R$: \Accept $v = 0$\;
  }
  \uElseIf{$G(s) \oplus r = \alpha$}{
    $R$: \Accept $v = 1$\;
  }
  \uElse{
    $R$: \Reject\;
  }
  \caption{A bit-commitment scheme based on a one-way function
    $f$}\label{alg:bit-commit}
\end{algorithm}

Not only do the simplest examples of bit-commitment schemes involve one-way
functions, but Goldreich and Levin showed that bit-commitment schemes can only
exist if one-way functions do~\cite{GL89}. Referring back to our discussion of
Impagliazzo's five worlds in \cref{sec:five-worlds}, this means that
bit-commitment schemes exist only if we live in cryptomania. The importance of
bit-commitment schemes to zero-knowledge proofs in general is one of the major
reasons why this paper assumes the existence of one-way functions.

\section{Probabilistically-checkable proofs}\label{sec:pcp}

So far, all of our computational proofs have focused on the interaction between
two computers, but there exist non-interactive models as well.
Probabilistically-checkable proofs do not use interactive Turing machines, but
instead have access to a ``proof'' that their input is in the given language.
The nontriviality is that the number of bits of the proof we can access is
bounded---simply reading the entire proof will not suffice. For any string in the
language, we ensure there exists a correct proof, which our algorithm must
always recognize accurately. Further, for any string \emph{not} in the language,
the algorithm must reliably (but not necessarily always) reject.

In general, query-based machines (i.e.\ those where the \emph{number} of queries
is an important part of the complexity) come in two flavors: adaptive and
non-adaptive. Adaptive-query machines allow the machine to change what locations
it queries based on what it has already seen; non-adaptive machines do not allow
that. In general, adaptive queries are more powerful---it turns out that any
adaptive machine can be simulated by a non-adaptive machine using $2^{q}$
queries. Most interesting results about PCPs can be proven even with weaker
non-adaptive machines, so that is what we will focus on for the rest of this
paper.

\begin{defn}\label{def:adaptive}\index{adaptive}
  Let $M$ be a Turing machine with query access to some string
  $\pi \in \{0, 1\}^{*}$. The queries by $M$ are \emph{non-adaptive} if the
  locations queried depend only on the contents of the input tape and random
  generator. If the queries are dependent on previous query results, then $M$ is
  \emph{adaptive}.
\end{defn}

Now, it is time to define a probabilistically-checkable proof. Unlike an
interactive proof system, our formal definition only defines the verifier, and
not the entire system itself. This is because, in an interactive proof, both the
prover and verifier are themselves Turing machines, with all the attendant
parameters to be defined therein. On the other hand, with a PCP, only the
verifier is actually a Turing machine, and the proof itself is simply a
particular string satisfying some properties.

\begin{defn}[{\cite[Def.\ 18.1]{AB09}}]\label{def:prob-check}\index{probabilistically-checkable proof}
  Let $L \subseteq \{0, 1\}^{n}$ be a language and $q, r\colon \mathbb{N} \rightarrow \mathbb{N}$. A
  \emph{$(r(n), q(n))$-verifier} for $L$ is a polynomial-time probabilistic
  algorithm $V$ such that
  \begin{enumerate}
    \item When given an input string $x \in \{0, 1\}^{n}$ and random access to a
          string $\pi \in \{0, 1\}^{*}$, $V$ uses at most $r(n)$ random coins and
          makes at most $q(n)$ non-adaptive queries to locations of $\pi$ before
          either accepting or rejecting.
    \item If $x \in L$ then there exists a $\pi_{x} \in \{0, 1\}^{*}$ (which we call
          the \emph{proof} of $x$) such that $V$ will always accept when given
          input $x$ and random string $\pi_{x}$.
    \item If $x \notin L$ then $V$ will reject with probability $\ge 1/2$ for
          \emph{all} random strings $\pi$.
  \end{enumerate}
  We denote the output of $V$ on input $x$ and proof $\pi$ by $V^{\pi}(x)$.
\end{defn}

\begin{figure}
  \centering
  \begin{tikzpicture}[>=Stealth]
    \draw (0,0) rectangle ++(3,2);
    \foreach [count=\i] \c in {?,0,?,?,?,1,?,?,1,?,}{
      \pgfmathsetmacro{\xval}{\i*0.5};
      \draw (\xval,3) rectangle ++(0.5,0.5) node [midway] {\c};
    }
    % cdots doesn't appear to be centered in the box?
    \path (5.5,3) rectangle ++(0.5,0.5) node [midway] {$\cdots$};
    \foreach \x in {2, 6, 9}{
      \pgfmathsetmacro{\xval}{\x/2+0.25};
      \draw[->] (1.5,2) to (\xval,3);
    }
  \end{tikzpicture}
  \caption{A probabilistically-checkable proof verifier}\label{fig:pcp}
\end{figure}

The first piece of this definition to notice is that if $x \in L$ we require $L$
to accept unconditionally for $x$'s corresponding proof, despite the fact that
$L$ has access to randomness. We want this because we want to construct
verifiers that look for inconsistencies in the purported proof and reject if
they find them---a correct proof should not contain any inconsistency at all so we
should accept every time.

On the other hand, while an incorrect proof should still be noticeable
\emph{most} of the time (otherwise this would not be a particularly useful
definition), if we are only querying a limited number of bits from a proof it is
impossible to prevent scenarios where we only query bits that are identical to
the original proof. In this case, it would be impossible for any algorithm to
distinguish correct proofs from incorrect ones (outside of ignoring the proof
completely and just solving the problem itself) and as such we do not require
$V$ to always correctly reject.

So far, all of our proof-complexity classes have just had a single class for all
languages with the proof regardless of internal complexity, but for
probabilistically-checkable proofs we actually stratify the class further. This
is for multiple reasons: first, we can actually get astonishingly tight bounds
on the parameters for PCPs (as we will see in \cref{thm:pcp-theorem}), and
second, because these are ``access'' complexity (i.e.\ we measure the number of
preexisting bits actually read by the algorithm), they are actually independent
of computational model, so the need for polynomial equivalence is negated.

In addition, this means PCPs become susceptible to the alphabet the proof is
written in. Since individual bits carry more data when a larger alphabet is
used, a larger alphabet will necessarily require fewer queries to transmit the
same information. As such, while we will by default still work over the alphabet
$\{0, 1\}$, there are times where we will need to be a little more specific
about the alphabet we use.

\begin{defn}[{\cite[Def.\ 18.1]{AB09}}]\label{def:pcp}\index{PCP@$\PCP$}
  For any $q, r\colon \mathbb{N} \rightarrow \mathbb{N}$, the class $\PCP_{\Sigma}[q(n), r(n)]$ is the class of all
  languages with a $(cq(n), dr(n))$-verifier for some $c, d \in \mathbb{N}$, where the
  proof is written over the alphabet $\Sigma$. When $\Sigma = \{0, 1\}$, we sometimes omit
  it.
\end{defn}

A small note on notation: The text ``PCP'' can be used as both a complexity
class and an abbreviation: in this paper when it is written $\PCP$, we are
referring to the complexity class (and therefore the set of \emph{languages}
with a probabilistically-checkable proof); when it is written PCP, we are
referring to the proofs themselves.

Next, we give an example of a nontrivial probabilistically-checkable proof. As
with our previous examples, the language $\GNI$ provides a good example for us,
as it is a relatively simple language that is still not known to be in $\BPP$.

\begin{thm}\label{thm:gni-pcp}
  $\GNI \in \PCP[\poly(n), 1]$.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{Two $n$-vertex graphs $G_{1}$ and $G_{2}$}
  \KwOut{Whether $G_{1} \ncong G_{2}$}
  \tcc{Proof:}
  \For{$H$ a graph with $n$ nodes}{
    \eIf{$H \cong G_{0}$}{
      $\pi[H] \leftarrow 0$\;
    }{
      $\pi[H] \leftarrow 1$\;
    }
  }
  \KwRet{$\pi$}\;
  \tcc{Verifier:}
  Pick random $b \in \{0, 1\}$\;
  Pick random $\sigma \in S_{n}$\;
  Apply $\sigma$ to the vertices of $G_{b}$\;
  Accept if and only if $\pi[\sigma \cdot G_{b}] = b$\;\nllabel{line:gni-accept}
  \caption{A PCP for $\GNI$}\label{alg:gni-pcp}
\end{algorithm}

\begin{proof}
  We describe such a PCP in \cref{alg:gni-pcp}. Next, we show that this
  algorithm has the properties we seek.

  The verifier runs in polynomial time: both picking and computing an $n$-bit
  permutation are known to be in polynomial time with respect to $n$. Further,
  picking a random $n$-bit permutation is doable in $\poly(n)$ bits, and picking
  a random bit is doable in $1$ bit; hence $V$ uses $\poly(n)$ bits of
  randomness. Lastly, we only make a single query to $\pi$, in
  line~\ref{line:gni-accept}.

  If $G_{0} \ncong G_{1}$, we show $V$ always accepts when given $\pi$ as input. Since
  isomorphisms are transitive, we know there is no $H$ with both $H \cong G_{0}$ and
  $H \cong G_{1}$. Hence, for all $H$, if $H \cong G_{0}$ then $\pi[H] = 0$ and if
  $H \cong G_{1}$ then $\pi[H] = 1$. Hence, since $\sigma \cdot G_{b} \cong G_{b}$ regardless of
  our choice of $\sigma$ and $b$, we have that $\pi[\sigma \cdot G_{b}] = b$.

  Next, if $G_{0} \cong G_{1}$ then $V$ rejects with probability at least $1/2$,
  regardless of choice of $\pi$. Since $G_{0} \cong G_{1}$, it follows that if
  $H \cong G_{0}$ then $H \cong G_{1}$, and vice versa. Hence, for any graph
  $H = \sigma \cdot G_{0}$, there exists a $\sigma' \in S_{n}$ with $H = \sigma' \cdot G_{1}$. For any
  $n$-vertex graph $H$, this means that we are equally likely to query $\pi[H]$
  with $b = 0$ and $\sigma$ as we are with $b = 1$ and $\sigma'$. Since we know $\pi[H]$ can
  only be $0$ or $1$, it must be the case that $\pi[H]$ is incorrect at least half
  the time. Hence, $V$ will reject with probability at least $1/2$ for any proof
  $\pi$.
\end{proof}

It is reasonable to be surprised about the fact that we only need one query to
determine this problem to the constraints imposed by a PCP\@. This is our first
clue that PCPs are surprisingly powerful: in \cref{chap:pcp-theorem} and again
in \cref{chap:zk-pcp-theorem} we will explore the extremes of the power of PCPs.

\subsection{PCPs of proximity}\label{sec:pcpp}

Since a PCP will always only check a limited proportion of any given proof, for
any strings in the language, the verifier will still accept any proof that is
close to the official proof with very high probability. While this is
interesting in and of itself, it can also lead to the question of how PCPs
perform on \emph{values} that are close to strings in our language. This is the
notion behind PCPs of proximity: what if we weaken a PCP to only require it to
reject strings that are not sufficiently close to strings in the given language?

We discussed Hamming distance, a measure for proximity, in
\cref{def:hamming-dist} earlier. This is the definition we will be using when we
refer to the distance of strings. in a PCPP.

Unlike a PCP, which is defined for any language $L$, a PCPP is only defined for
\emph{pair languages}\index{pair language}, languages that consist entirely of
ordered pairs of two objects. This is because we need our language to consist of
pairs so that we can have a notion of distance between the elements. The good
news is this will not affect us too much---lots of the languages we care about are
pair languages already.

The main difference between a PCP and a PCPP is that we relax the rejection
condition to only require consistent rejection for pairs $(x, y)$ where there is
no close $y'$ where $(x, y')$ is in the accepted language. There are many pair
languages that we can think of as being made of function-value pairs $(f, y)$
with the property that there is some $x$ such that $f(x) = y$. When we have a
language like this, we have a more intuitive explanation of PCPPs: here, a PCPP
will accept any $(f, y)$ pair where $f(x)$ is $\delta$-close to $y$.

\begin{defn}[{\cite[Def.\ 2.2]{GOS25}}]\label{def:pcpp}\index{PCPP@$\PCPP$}
  For $\delta\colon \mathbb{N} \rightarrow [0, 1]$, a \emph{probabilistically-checkable proof of
    proximity} for a language $L$ consisting of ordered pairs $(x, y)$ with
  proximity parameter $\delta$ consists of a prover $P$ and verifier $V$ such that
  the following holds for all $(x, y)$:
  \begin{enumerate}
    \item When given an input string $x \in \{0, 1\}^{n}$ and random access to a
          string $\pi \in \{0, 1\}^{*}$, $V$ uses at most $r(n)$ random coins and
          makes at most $q(n)$ non-adaptive queries to locations of $\pi$ and $y$
          before either accepting or rejecting.
    \item If $(x, y) \in L$, there exists a proof $\pi_{(x, y)}$ such that $V$ will
          always accept when given input $x$ and oracle access to $y$ and
          $\pi_{(x, y)}$.
    \item If $y$ is $\delta$-far from the set $L(x) = \{y \mid (x, y) \in L\}$, then for
          every oracle $\pi^{*}$, $V$ will reject on input $x$ and oracles $y$ and
          $\pi^{*}$ with probability $\ge 1/2$.
  \end{enumerate}
\end{defn}

One important thing to note here about the definition of a PCPP is that $V$ does
\emph{not} get complete access to the input string $y$, only oracle
access.\footnote{In particular, this means that access to $y$ is subject to the
  limits imposed by our query bounds.} As such, unlike every other proof system
we have seen, it is not immediate that a PCPP can solve every problem in $\P$.
With every other proof system, we have gotten unfettered access to the entire
input string; thus if the language is in $\P$ our verifier could simply compute
the answer itself without actually engaging in any part of what makes the proof
system interesting. However, since a PCPP only gets oracle access to one of the
two elements of the pair, it must make at least some queries to $y$ in order to
determine what the entire input is. This point will be salient to remember in
\cref{sec:alg-circuit}, when we talk about a PCPP for a language in $\P$.

\subsection{Robustness}\label{sec:robust-verifiers}

One important property of PCPs is their ability to reliably reject when given a
proof of a false input. While often this requirement is enough to make PCPs
useful, one particular place where it falls apart is in the notion of PCP
composition. Here, we introduce the notion of \emph{robust} PCPs, which
strengthens the original requirements of a PCP in order to facilitate
composition. In this case, we strengthen the original soundness requirement of
PCPs to that of \emph{robust soundness}: instead of simply requiring that a PCP
reliably reject when given a proof of a false input, we also require that the
set of public messages is reliably far from any set of accepting messages.

\begin{defn}\label{def:acc(v)}\index{Acc@$\Acc$}
  Let $V$ be a non-adaptive PCP verifier that makes $q$ queries. The set of
  \emph{accepting views} of $V$ for some input $x$ and randomness $\mu$, denoted
  $\Acc(V(x; \mu))$, is the set of all elements $a \in \Sigma^{q}$ such that $V$ accepts
  when given input $x$, randomness $\mu$, and has its queries answered by the
  sequential elements of $a$.
\end{defn}

\begin{defn}[{\cite[Def.\ 2.6]{BGHSV06}}]\label{def:robust-verifer}\index{robust verifier}
  Let $s, \rho\colon \mathbb{N} \rightarrow [0, 1]$. A PCP verifier $V$ has
  \emph{robust-soundness error} $s$ with \emph{robustness parameter} $\rho$ if for
  all $x \notin L$, the bits read by $V$ are $\rho$-close to being accepted with
  probability strictly less than $s$. More formally,
  \begin{equation}
    \mathbb{P}[\Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu))) \le \rho] \le s.
  \end{equation}
\end{defn}

We will denote robust PCP classes with
\[
  \PCP\pcpr{q(n)}{r(n)}{s(n)}{\rho(n)}.
\]
Note the additional two parameters to our class compared to the original
definition of a $\PCP$ class.

Having done that, adding two additional parameters to the notion of a PCP is a
lot to take in, especially as it adds not one, but two new things to prove
whenever we demonstrate any theorem. Thankfully, these two are relatively
closely related, and so much of the time we will be able to roll them up into a
single statement, which has the added benefit of being much easier to prove. The
idea is that instead of putting a bound on the probability of our proofs being a
certain distance away from acceptable ones, we instead simply look at the
expected distance of proofs from acceptable ones.

\begin{defn}\label{def:exp-rob-pcp}\index{expected robustness!PCP@$\PCP$}
  Let $\rho: \mathbb{N} \rightarrow [0,1]$. A PCP for some language $L$ has \emph{expected
    robustness} $\rho$ if for all $x \notin L$, we have for every oracle $\pi^{*}$, the
  expected distance between any accepting view and the set of actually-seen
  elements is no more than $\rho(x)$. More formally,
  \begin{equation}
    \underset{\mu}{\mathbb{E}}\mleft[\Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu)))\mright] \ge \rho(x).
  \end{equation}
\end{defn}

So far, we have looked exclusively at PCPs, but these definitions are no less
valid for PCPs of proximity. The only change here is since our soundness
guarantee only applies to inputs that are $\delta$-far from valid ones, similarly we
will only require expected robustness on those same inputs.

\begin{defn}\label{def:exp-rob-pcpp}\index{expected robustness!PCPP@$\PCPP$}
  Let $\rho: \mathbb{N} \rightarrow [0,1]$. A PCPP for some pair language $L$ has \emph{expected
    robustness} $\rho$ if for all $(x, y)$ where $y$ is $\delta$-far from $L[x]$, we
  have for every oracle $\pi^{*}$, the expected distance between any accepting
  view and the set of actually-seen elements is no more than $\rho(x)$. More
  formally,
  \begin{equation}
    \underset{\mu}{\mathbb{E}}\mleft[\Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu)))\mright] \ge \rho(x).
  \end{equation}
\end{defn}

Now, it is important to define the relationship between expected robustness and
robust soundness. The idea is that expected robustness provides a
\emph{relationship} between robust-soundness and robustness. In general, for any
expected robustness $\rho$, the difference between robust-soundness error and
robustness parameter is $1 - \rho$. Within that constraint, we can pick any $\varepsilon$ to
increase one with respect to the other; the only real bound is that both
robust-soundness error and robustness parameter must be between $0$ and $1$.

\begin{lemma}[{\cite[Proposition 2.10]{BGHSV06}}]\label{lem:exp-rob-to-rs}
  If a PCPP has expected robustness $\rho$, then for all $\varepsilon \le \rho$, it has
  robust-soundness error $1 - \varepsilon$ with robustness parameter $\rho - \varepsilon$.
\end{lemma}

The original paper did not present an explicit proof of this; hence we present
our own proof here.

\begin{proof}
  We prove this by contradiction. Let $V$ be a PCPP verifier with expected
  robustness $\rho(n)$, and let $\varepsilon(n)$ be between $0$ and $\rho(n)$. Let $x \notin L$ such
  that $V$ does not have robust-soundness error $1 - \varepsilon$ with robustness
  parameter $\rho - \varepsilon$. By definition, this means
  \begin{equation}
    \underset{\mu}{\mathbb{P}}[\Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu))) \le \rho - \varepsilon] > 1 - \varepsilon.
  \end{equation}
  Pulling out the formal definition of probability, we get
  \begin{equation}\label{eqn:expand-prob-mu}
    \frac{\abs{\{\mu \in \{0, 1\}^{\abs{\mu}} \mid \Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu))) \le \rho - \varepsilon\}}}{2^{\abs{\mu}}} > 1 - \varepsilon.
  \end{equation}
  Define $S$ to be the set in the numerator of \cref{eqn:expand-prob-mu}, and
  let $S^{C}$ be its complement. Based on the definition of $S$, we get
  \begin{equation}\label{eqn:sum-s}
    \sum_{\mu \in S}\Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu))) \le \abs{S}(\rho - \varepsilon).
  \end{equation}
  Similarly, since Hamming distance is bounded above by $1$, we get
  \begin{equation}\label{eqn:sum-s-c}
    \sum_{\mu \notin S}\Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu))) \le \abs{S^{C}}.
  \end{equation}
  Combining \cref{eqn:sum-s,eqn:sum-s-c}, we get
  \begin{equation}
    \sum_{\mu \in \{0, 1\}^{\abs{\mu}}}\Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu))) \le \abs{S}(\rho - \varepsilon) + \abs{S^{C}}.
  \end{equation}

  From \cref{eqn:expand-prob-mu}, we get $\abs{S} > 2^{\abs{\mu}}(1 - \varepsilon)$. Hence,
  it follows that $\abs{S^{C}} < 2^{\abs{\mu}}\varepsilon$. Thus,
  \begin{equation}
    \sum_{\mu \in 2^{\abs{\mu}}}\Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu))) < \abs{S}(\rho - \varepsilon) + 2^{\abs{\mu}}\varepsilon.
  \end{equation}
  Further, $\abs{S} \le 2^{\abs{\mu}}$ (since $S \subseteq \{0, 1\}^{\abs{\mu}}$), so we have
  \begin{equation}
    \sum_{\mu \in 2^{\abs{\mu}}}\Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu))) < 2^{\abs{\mu}}(\rho - \varepsilon) + 2^{\abs{\mu}}\varepsilon = 2^{\abs{\mu}}\rho.
  \end{equation}
  By the definition of expected value, the above equation implies
  \begin{equation}
    \underset{\mu}{\mathbb{E}}[\Delta(\pi^{*}|_{Q(x)}, \Acc(V(x; \mu))] < \rho,
  \end{equation}
  a contradiction.
\end{proof}

This lemma will allow us to prove statements in terms of expected robustness
instead of normal robustness. In general, we will find expected robustness much
easier to prove; hence why we will be using it more often.

\section{Zero-knowledge probabilistically-checkable proofs}\label{sec:pzkpcp}

A zero-knowledge probabilistically-checkable proof is a combination of the ideas
of zero-knowledge proofs (as seen in \cref{sec:zero-knowledge}) and
probabilistically-checkable proofs (as seen in \cref{sec:pcp}). Since we can
model a PCP as an interaction between between a verifier and a proof (instead of
a prover), we can model this interaction as being zero-knowledge as well.

Unlike interactive proofs, we cannot achieve \emph{arbitrary} zero-knowledge
guarantees: since the proof is non-interactive, we have no recourse against an
attacker who simply reads the entire proof end-to-end. As such, we introduce a
\emph{query bound}\index{query bound}: we limit our verifier to a certain number
of queries, under which we retain the standard zero-knowledge restrictions.

\begin{defn}[{\cite[Def.\ 8.6]{GOS24}}]\label{def:pzkpcp}\index{perfect-zero knowledge PCP}
  A probabilistically-checkable proof system is \emph{zero-knowledge} with query
  bound $q$ if for any verifier $V'$ such that $V'$ makes no more than $O(q(n))$
  adaptive queries on an input of length $n$, there exists a probabilistic
  polynomial-time simulator $S$ such that on input $x$, $S$ can simulate every
  interaction of $V'$ with the associated proof of $x$.\footnote{To clarify, $S$
    does \emph{not} have access to the proof of $x$, just $x$ itself.}
\end{defn}

The first thing to notice here is that for \emph{validity} of PCPs, we
parameterize over the proof (i.e.\ the one verifier should remain valid for all
proofs $\pi$), for \emph{zero-knowledge} we parameterize over the verifier $V$.
This is because for zero-knowledge proofs we care about not revealing any
information from the proof, no matter how clever we are about asking questions
with the verifier. In that way, outside of the ``happy path'' (where $x \in L$ and
the given string is the proof of $x$), the two notions are somewhat orthogonal:
a $\PCP$ cares about how we react when $x \notin L$ but $V$ is trusted, while
zero-knowledge cares about what happens when $V$ is not trusted, but the proof
is.

\begin{defn}\label{def:pzkpcp-class}\index{PZK-PCP@$\PZKPCP$}
  The class $\PZKPCP$ is the class of all languages that have a perfect
  zero-knowledge probabilistically-checkable proof.
\end{defn}

\section{Interactive probabilistically-checkable proofs}\label{sec:ipcp}

Interactive probabilistically-checkable proofs are a combination of the concepts
of an interactive protocol and a probabilistically-checkable proof. The broad
idea is our proof proceeds in two phases: first, the prover sends a purported
proof to the verifier, after which they engage in an interactive protocol,
during which the verifier can access the proof as an oracle.

\begin{defn}[{\cite[\defaultS 1.1]{KR08}}]\label{def:ipcp}\index{interactive PCP}
  Let $L$ be a language, let $p, q, l\colon \mathbb{N} \rightarrow \mathbb{N}$, and let $c, s \in [0, 1]$. An
  \emph{interactive probabilistically-checkable proof} for $L$ is an interactive
  protocol as follows:

  \begin{algorithm}[H]
    \KwIn{To both $P$ and $V$: a string $x$ of length $n$}
    \KwIn{To $P$ alone: A string $w$}
    \KwOut{Whether $x \in L$}
    $P$: Send an oracle $R$ to $V$\;
    $V^{R}$: Engage in an interactive protocol with $P$\;
    \caption{The IPCP protocol}\label{alg:ipcp-protocol}
  \end{algorithm}
\end{defn}

Next, we need to define a few properties of IPCPs. In general we will care about
IPCPs with specific bounds on these properties; later on we will spend some time
working on optimizing bounds on some of these properties at the expense of
others.

The first two complexities we will look at come from the interactive-proof
portion of the IPCP\@. As a reminder, two important parameters we care about
with regard to interactive proofs are the number of communication rounds (i.e.,
the number of times the switch tape flips) and the total amount of information
sent between the two machines. Since these are supposed to relate to just the IP
portion of the protocol, we need to modify the definitions slightly to exclude
the information exchanged during the PCP phase.

\begin{defn}\label{def:ipcp-round-complexity}\index{round complexity!IPCP}
  The \emph{round complexity} of an IPCP is the number of rounds in the second
  portion of the protocol.
\end{defn}

\begin{defn}\label{def:ipcp-comm-complexity}\index{communication complexity!IPCP}
  The \emph{communication complexity} of an IPCP is the total number of bits
  exchanged between $P$ and $V$ \emph{except} for the message that contains $R$.
\end{defn}

The third complexity parameter we care about comes from the PCP portion of the
IPCP\@.

\begin{defn}
  The \emph{query complexity} of an IPCP is the total number of queries that $V$
  makes to the PCP oracle $R$.
\end{defn}

As with all of our other proof-system definitions, with a new type of machine
comes a corresponding class.

\begin{defn}\label{def:ipcp-class}\index{IPCP@$\IPCP$}
  The class $\IPCP$ is the class of all languages with an interactive $\PCP$.
\end{defn}

Next, we give a few reasonable class inclusions regarding $\IPCP$.

\begin{thm}
  $\PCP \subseteq \IPCP$ and $\IP \subseteq \IPCP$.
\end{thm}

\begin{proof}
  Both of these come from the definition of an interactive PCP: a regular PCP is
  simply the first half of the IPCP protocol where the interactive portion is
  useless, and an interactive proof is simply the second half of the protocol
  where the oracle is useless.
\end{proof}

The tuple-notation we used when talking about the class $\PCP$ (see
\cref{def:pcp}) is rather hard to read when we have this many parameters, and as
such we will us the following clearer notation when talking about the various
bounds on $\IPCP$ algorithms:
\begin{equation*}
  L \in \IPCP\ipcp{r}{\ell}{c}{q}{\varepsilon}
\end{equation*}
to mean the language $L$ is a member of $\IPCP$ with the listed restrictions.

\begin{defn}\label{def:low-deg-ipcp}\index{low-degree IPCP@low-degree $\IPCP$}
  Let $\mathbb{F}$ be a field, and $d, m \in \mathbb{N}$. A \emph{low-degree IPCP} is an
  IPCP instance with the following two properties:
  \begin{enumerate}
    \item The oracle sent by the honest prover $P$ is an $m$-variable
          $\mathbb{F}$-polynomial $Q$ with multidegree no more than $d$ (i.e.\
          $Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$)
    \item Soundness is only required to hold against provers that send oracles
          that are polynomials in $\mathbb{F}[X_{1, \ldots, m}^{\le d}]$.
  \end{enumerate}
\end{defn}

Similarly to what we did with normal $\IPCP$ oracles, we will use the following
notation to talk about low-degree $\IPCP$ instances:
\begin{equation*}
  L \in \IPCP\ldipcp{r}{\ell}{c}{q}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{\varepsilon}.
\end{equation*}
We combine all the information about the degree of the oracle into one line
because we have an efficient notation for multidegree-bounded polynomials, and
so that we do not wind up with an exorbitant number of lines in our
notation.\footnote{Having said that, there are still a lot of lines in this
  notation, but this is the best we can do.}

\begin{defn}\label{def:ipcp-view}\index{view!IPCP}
  The \emph{view} of an IPCP $(P, V)$ on input $x$ is the random variable
  \[
    (x, r, s_{1}, \ldots, s_{n}, t_{1}, \ldots, t_{m})
  \]
  where $r$ is the random bits used by $V$, $s_{i}$ are the messages from $P$,
  and $t_{i}$ are the answers to $V$'s queries to the oracle sent by $P$.
\end{defn}

We denote the view of $P$ and $V$ on input $x$ by $\View_{V}^{P}(x)$.

\section{Zero-knowledge IPCPs}

At this point, the astute reader may have noticed a trend: after each new
interactive-proof variant we introduce, we then describe how to make it
zero-knowledge. We will continue this trend by showing how an interactive PCP
can be made zero-knowledge.

\begin{defn}[{\cite[\defaultS 5.2]{CFGS22}}]\label{def:zk-ipcp}\index{interactive PCP!zero-knowledge}
  An interactive PCP is \emph{perfect zero-knowledge} with query bound $b$ when
  there exists a polynomial-time simulator algorithm $S$ such that for every
  interactive Turing machine $\tilde{V}$ that makes no more than $b$ queries,
  $S^{\tilde{V}}(x)$ and $\View\ang{P(x), \tilde{V}(x)}$ are identically
  distributed.
\end{defn}

\begin{defn}\label{def:pzkipcp}\index{PZK-IPCP@$\PZKIPCP$}
  The class $\PZKIPCP$ is the class of all languages with a perfect
  zero-knowledge IPCP\@.
\end{defn}

As with our other notations, we will write
\[
  L \in \PZKIPCP\pzkipcp{r}{\ell}{c}{q}{b}{\varepsilon}
\]
to show $L$ has a perfect zero-knowledge IPCP with the listed restrictions.

\chapter{The PCP theorem}\label{chap:pcp-theorem}

The PCP theorem is one of the most important and stunning results involving
probabilistically-checkable proofs. In brief, it tells us how $\NP$ slots in to
the hierarchy of $\PCP$ classes (based on query and randomness complexity). The
reason it is so stunning is specifically how low on the hierarchy it falls: as
we will see, a PCP for any problem in $\NP$ only needs to read $3$ bits off of
its proof and it will be able to solve the problem with high probability. Our
goal with this chapter is to provide a proof of this theorem, with the secondary
goal of designing the proof in such a way that it is an easy jumping-off point
for the zero-knowledge variant we will be proving in \cref{chap:zk-pcp-theorem}.
We start by providing a statement of the PCP theorem.

\begin{thm}[{PCP theorem,~\cite{AS98}}]\label{thm:pcp-theorem}\index{PCP theorem}
  Any problem in $\NP$ has a probabilistically-checkable proof of constant query
  complexity and using a maximum of $O(\log n)$ random bits, and vice versa.
  Equivalently, $\NP = \PCP[\log n, 1]$.
\end{thm}

\section{Algebraic circuits}\label{sec:alg-circuit}

Before we can get started on the main proof of the PCP theorem, we need to
define a little more background. For much of this proof, we will be working in
an algebraic landscape, and thus we would like an algebraically-focused
$\NP$-complete problem, as opposed to $\SAT$, which is based on boolean
formulae. We will find such a problem in the domain of algebraic circuits.

\begin{defn}[{\cite[\defaultS 14.1]{AB09}}]\label{def:alg-circuit}\index{algebraic circuit}
  An \emph{algebraic circuit} is a directed acyclic graph such that
  \begin{enumerate}
    \item each leaf (called an \emph{input node}) takes values in some field
          $\mathbb{F}$,
    \item each internal node (called a \emph{gate}) is labeled with either $+$
          or $\cdot$ (the two field operations),
    \item there is one output node, and
    \item each gate has in-degree no more than $2$.
  \end{enumerate}
  Optionally, there may be input nodes labeled $1$ and $-1$ as well.
\end{defn}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}[>=stealth,every node/.style={minimum size=0.75cm},scale=2]
      \node [diamond,draw] (R) at  (0,0) {$+$};
      \node [rectangle,draw] (A1) at  (-0.5,1) {$\times$};
      \node [diamond,draw] (A2) at  (0.5,1) {$+$};
      \node [diamond,draw] (B1) at  (-1,2) {$+$};
      \node [rectangle,draw] (B2) at  (0,2) {$\times$};
      \node [diamond,draw] (B3) at  (1,2) {$+$};
      \node [rectangle,draw] (C1) at  (-0.5,3) {$\times$};
      \node [diamond,draw] (C2) at  (0.5,3) {$+$};
      \node [circle,draw] (X1) at  (-1.5,4) {$x_{1}$};
      \node [circle,draw] (X2) at  (-0.5,4) {$x_{2}$};
      \node [circle,draw] (X3) at  (0.5,4) {$x_{3}$};
      \node [circle,draw] (X4) at  (1.5,4) {$x_{4}$};

      \draw[->] (X1) to (B1);
      \draw[->] (X2) to (C1);
      \draw[->] (X2) to (C2);
      \draw[->] (X3) to (C1);
      \draw[->] (X3) to [bend left=15] (B3);
      \draw[->] (X3) to (C2);
      \draw[->] (X4) to (B3);
      \draw[->] (C1) to (B2);
      \draw[->] (C2) to [bend left] (A1);
      \draw[->] (C2) to (B1);
      \draw[->] (C2) to (B2);
      \draw[->] (B3) to (A2);
      \draw[->] (B1) to (A1);
      \draw[->] (B2) to (A2);
      \draw[->] (A1) to (R);
      \draw[->] (A2) to (R);
    \end{tikzpicture}
  \end{center}
  \caption{An algebraic circuit}\label{fig:algebraic-circuit}
\end{figure}

We give an example of an algebraic circuit in \cref{fig:algebraic-circuit}.
Notice in particular that unlike the in-degree, the out-degree is unbounded (the
rightmost node in the second row has out-degree three, for example). In many
ways, we will see a parallel between how we think about algebraic circuits and
how we think about boolean formulae. We will see this parallel both with the
terminology we are about to define, and later on when we show how algebraic
circuits can be used to define another $\NP$-complete problem.

\begin{defn}\label{def:alg-assignment}\index{assignment!algebraic circuit}
  An \emph{assignment} to an algebraic circuit is a labeling of its input nodes.
  The \emph{result} of the assignment is the value in the output node, where the
  value of any $+$ gate is $a+b$ and any $\cdot$ gate is $a \times b$, where $a$ and $b$
  are the values of the two in-nodes.
\end{defn}

Not all assignments are created equal for our purposes. In parallel with our
work on Boolean circuits, we would like to assign some sort of truthiness value
to our assignments, which will give us the ability to turn this into a decision
problem. As such, we define a ``truthy'' circuit to be one that evaluates to
$1$, and everything else is not.

\begin{defn}\label{def:alg-circuit-sat}\index{satisfying assignment!algebraic circuit}
  An algebraic circuit has a \emph{satisfying assignment} when there exists a
  labeling of its input nodes such that the value computed by the output node is
  $1$.
\end{defn}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}[>=stealth,every node/.style={minimum size=0.75cm},scale=2]
      \node [diamond,draw] (R) at (0,0) {$1$};
      \node [rectangle,draw] (A1) at (-0.5,1) {$5$};
      \node [diamond,draw] (A2) at (0.5,1) {$4$};
      \node [diamond,draw] (B1) at (-1,2) {$3$};
      \node [rectangle,draw] (B2) at (0,2) {$6$};
      \node [diamond,draw] (B3) at (1,2) {$4$};
      \node [rectangle,draw] (C1) at (-0.5,3) {$2$};
      \node [diamond,draw] (C2) at (0.5,3) {$3$};
      \node [circle,draw]  (X1) at (-1.5,4) {$3$};
      \node [circle,draw]  (X2) at (-0.5,4) {$2$};
      \node [circle,draw]  (X3) at (0.5,4) {$1$};
      \node [circle,draw]  (X4) at (1.5,4) {$3$};

      \draw[->] (X1) to (B1);
      \draw[->] (X2) to (C1);
      \draw[->] (X2) to (C2);
      \draw[->] (X3) to (C1);
      \draw[->] (X3) to [bend left=15] (B3);
      \draw[->] (X3) to (C2);
      \draw[->] (X4) to (B3);
      \draw[->] (C1) to (B2);
      \draw[->] (C2) to [bend left] (A1);
      \draw[->] (C2) to (B1);
      \draw[->] (C2) to (B2);
      \draw[->] (B3) to (A2);
      \draw[->] (B1) to (A1);
      \draw[->] (B2) to (A2);
      \draw[->] (A1) to (R);
      \draw[->] (A2) to (R);
    \end{tikzpicture}
  \end{center}
  \caption{A satisfying assignment to \cref{fig:algebraic-circuit} over $\mathbb{Z}/7\mathbb{Z}$}\label{fig:sat-assignment}
\end{figure}

We give a satisfying assignment to \cref{fig:algebraic-circuit} as an example in
\cref{fig:sat-assignment}. Notice that we have kept the shapes of the nodes from
the original in order to communicate the underlying operation.

Satisfying assignments will form the root of our circuit classes: the problem we
care about computing is whether or not a circuit has a satisfying assignment.
We find this a useful formulation because it works well with arbitrary algebraic
formulas: since we will be working in a primarily algebraic setting for these
proofs, having an example of a $\NP$-complete problem that is itself algebraic
will make for relatively easy transformations.

Earlier, we talked about the language $\SAT$ consisting of all boolean formulae
that have a satisfying assignment. In the same way that $\SAT$ relates to
boolean formulae, the language we introduce, $\CktSAT$, corresponds to algebraic
circuits.

\begin{defn}\label{def:cktsat}\index{CktSAT@$\CktSAT$}
  The language $\CktSAT$ is the language of all algebraic circuits with a
  satisfying assignment.
\end{defn}

\begin{thm}\label{thm:cktsat-np-complete}
  $\CktSAT$ is $\NP$-complete.
\end{thm}

\begin{proof}
  First, we need that $\CktSAT \in \NP$. Evaluating a circuit can be done in
  polynomial time relative to its size, so a certificate that is simply the
  values of each input node in the satisfying assignment will suffice.

  Next, we need that there is a reduction from every problem in $\NP$ to
  $\CktSAT$. We will use the fact that we already know normal $\SAT$ is
  $\NP$-complete for this. First, consider the \emph{boolean circuit problem}:
  similar to algebraic circuits but where the operators are boolean formulae
  instead of the field operators. We can construct a boolean circuit from a
  boolean formula by replacing every variable with an input node and every
  boolean operator with a gate corresponding to its formula.

  From there, we can transform a boolean circuit into an algebraic circuit over
  the field $\mathbb{Z}/2\mathbb{Z}$. Multiplication in $\mathbb{Z}/2\mathbb{Z}$ is exactly an AND gate, so that is
  simply a one-to-one replacement. OR gates are slightly trickier, however. We
  would like the replacement to be as simple as multiplication: mapping $x \vee y$
  to $x + y$. However, $T \vee T = T$ but $1 + 1 = 0$ in $\mathbb{Z}/2\mathbb{Z}$. Instead, we map
  $x \vee y$ to $(x + y) + (x \times y)$, which gives the answer we seek. Lastly, we map
  NOT gates to $x + 1$.

  We have shown $\CktSAT \in \NP$ and we have shown a polynomial-time reduction
  from $\SAT$ to $\CktSAT$; hence $\CktSAT$ is $\NP$-complete.
\end{proof}

Corresponding to $\CktSAT$ is a the pair language consisting of each element
$x \in \CktSAT$ paired with its corresponding certificate. While often we do not
think about this language explicitly, in this case the language is in fact
reasonably interesting on its own. This is because a valid certificate for a
circuit in $\CktSAT$ is the explicit values of a satisfying assignment,
something which we will be working with later on. Further, we would like to
define this because we now have a Turing machine variant that only works with
pair languages, and so we cannot have it solve $\CktSAT$ directly.

\begin{defn}\label{def:cktval}\index{CktVal@$\CktVal$}
  The language $\CktVal$ is the language of all pairs $(C, w)$, where $C$ is an
  algebraic circuit and $w$ is a satisfying assignment (i.e., $C(w) = 1$).
\end{defn}

\begin{thm}\label{thm:cktval-in-p}
  $\CktVal$ is in $\P$.
\end{thm}

\begin{proof}
  We are given a circuit and an assignment; computing the resultant value of a
  circuit is possible in polynomial time relative to its length. Hence, we can
  simply compute $C(w)$ directly and check if the answer is $1$.
\end{proof}

\begin{lemma}[{\cite[Prop.\ 2.4]{BGHSV06}}]\label{thm:cktval-cktsat}
  If $\CktVal$ has a PCPP, then $\CktSAT$ has a PCP with identical parameters.
\end{lemma}

\begin{proof}
  Let $C \in \CktSAT$. Then, let $w$ be a satisfying assignment to $C$, and let
  $\pi'$ be a PCPP that $(C, w) \in \CktVal$. Consider the oracle $\pi = (\pi', w)$:
  this proof can be verified using the PCPP verifier, forwarding all its queries
  to the new proof.
\end{proof}

\begin{thm}[{\cite[Theorem 3.3]{BGHSV06}}]\label{thm:ckt-val-pcpp}
  For any $\varepsilon > 0$, $\CktVal$ has a PCPP $(P, V)$ where $P$ is deterministic and
  polynomial-time, such that
  \[
    \CktVal \in \PCPP\pcpp{\log(n) + O(\log^{\varepsilon}(n))}{O(1/\varepsilon)}{\Theta(\varepsilon)}{1/2}.
  \]
\end{thm}

While important, the proof of this is somewhat involved and unfortunately
outside the scope of this project. Hence, we will defer the proof of this
particular theorem to the original paper~\cite{BGHSV06}.

\section{The composition theorem}\label{sec:comp-theorem}

We now introduce a theorem that will allow us to compose robust PCPs with robust
PCPPs to make a new (non-robust) PCP with improved bounds.

% NOTE: I believe this replaces 2.1 (outer/inner verifiers) from ALMSS98
\begin{thm}[{\cite[Theorem 2.7]{BGHSV06}}]\label{thm:composition}
  Let
  \begin{align*}
    r_{\out}, r_{\oin}, d_{\out}, d_{\oin}, q_{\oin}&\colon \mathbb{N} \rightarrow \mathbb{N} \\
    \varepsilon_{\out}, \varepsilon_{\oin}, \rho_{\out}, \delta_{\oin}&\colon \mathbb{N} \rightarrow [0, 1]
  \end{align*}
  be functions such that the following holds:
  \begin{enumerate}
    \item The language $L$ has a robust PCP verifier $V_{\out}$ with randomness
          complexity $r_{\out}$, decision complexity $d_{\out}$,
          robust-soundness error $1 - \varepsilon_{\out}$, and robustness parameter
          $\rho_{\out}$.
    \item $\CktVal$ has a PCPP verifier $V_{\oin}$ with randomness complexity
          $r_{\oin}$, query complexity $q_{\oin}$, decision complexity
          $d_{\oin}$, proximity parameter $\delta_{\oin}$, and soundness error
          $1 - \varepsilon_{\oin}$.
    \item For every $n \in \mathbb{N}$, $\delta_{\oin}(d_{\out}(n)) \le \rho_{\out}(n)$.
  \end{enumerate}
  Then $L$ has a standard PCP $V_{\comp}$ with
  \begin{enumerate}[label=\alph*.]
    \item randomness complexity $r_{\out}(n) + r_{\oin}(d_{\out}(n))$,
    \item query complexity $q_{\oin}(d_{\out}(n))$,
    \item decision complexity $d_{\oin}(d_{\out}(n))$, and
    \item soundness error $1 - \varepsilon_{\out}(n)\varepsilon_{\oin}(d_{\out}(n))$.
  \end{enumerate}
\end{thm}

\begin{algorithm}[htbp]
  Pick a random $R \in \{0, 1\}^{r_{\out}}$\;
  Simulate $V_{\out}$ on $x$ and random coins $R$\;
  Let $I_{\out} = (i_{1}, \ldots, i_{q_{\out}})$ be the query responses of
  $V_{\out}$\;
  Let $D_{\out}$ be the decision of $V_{\out}$\;
  Run $V_{\oin}$ on input $D_{\out}$ and random coin tosses\;
  Let $I_{\oin} = ((b_{1}, j_{1}), \ldots (b_{q_{\oin}}, j_{q_{\oin}}))$ be the query
  responses and $D_{\oin}$ be the result of the previous line's simulation\;
  \For{$\ell \in [q_{\oin}]$}{
    \eIf{$b_{\ell} = 0$}{
      $k_{\ell} \leftarrow (\out, i_{j_{\ell}})$\;
    }{
      $k_{\ell} \leftarrow (R, i_{j_{\ell}})$\;
    }
  }
  \KwRet{$(k_{1}, \ldots, k_{q_{\oin}})$ and $D_{\oin}$}\;
  \caption{A composed PCP~\cite[Theorem 2.7]{BGHSV06}}\label{alg:composed-pcp}
\end{algorithm}

We present the algorithm here, in \cref{alg:composed-pcp}, but leave the full
proof of completeness and soundness to the original construction in
\cite{BGHSV06}.

\begin{cor}[{\cite[Corollary 3.11]{GOS25}}]\label{cor:constant-query}
  \begin{equation}\label{eqn:constant-query}
    \PCP\pcpr{r}{q}{s}{\Omega(1)} \subseteq
    \PCP\pcpr{r + \log(n)}{1}{s}{\Omega(1)}.
  \end{equation}
\end{cor}

\begin{proof}
  Let
  \begin{equation}
    L \in \PCP\pcpr{r}{q}{s}{\Omega(1)}.
  \end{equation}
  Then as per \cref{thm:ckt-val-pcpp}, we have
  \begin{equation}
    \CktVal \in \PCPP\pcpp{\log(n) + O(\log^{\varepsilon}(n))}{O(1/\varepsilon)}{k\varepsilon}{1/2},
  \end{equation}
  for some $\varepsilon > 0$ and constant $k$. Define $\varepsilon = \rho/k$. Then, as per
  \cref{thm:composition}, we have $L$ as desired.
\end{proof}


\section{Alphabet reduction}\label{sec:alph-reduction}

One major difference between PCPs and many other types of computational model is
that the size of the alphabet proofs are written can have a non-trivial impact.
This is because when we talk about query complexity, we are no longer simply
looking at classes with the coarseness of e.g.\ $\P$, but are instead looking at
much finer complexities.

Despite this, we would like to be able to construct some PCPs over non-binary
languages. Non-binary PCPs can be more intuitive in certain circumstances, and
the complexity bounds can be more apparent. As such, we would like a theorem for
how changing the language we work over affects our parameters.

\begin{thm}[{\cite[Lemma 2.13]{BGHSV06}}]%
  \label{thm:alph-reduction}\index{alphabet reduction}
  Let $L$ be a language with a PCP over the language $\{0, 1\}^{a}$ such
  that
  \[
    L \in \PCP_{\{0, 1\}^{a}}\pcpr{q}{r}{s}{\rho}.
  \]
  Then $L$ has a PCP over the language $\{0, 1\}$ such that
  \[
    L \in \PCP_{\{0, 1\}}\pcpr{O(aq)}{r}{s}{\Omega(\rho)}.
  \]
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{A PCP $(P, V)$ over the language $\{0, 1\}^{a}$ for $L$, along with
    input $x$}
  \KwIn{A error-correcting code $\ECC: \{0, 1\}^{a} \rightarrow \{0, 1\}^{b}$, where
    $b = O(a)$}
  \KwOut{A PCP over the language $\{0, 1\}$ for $L$}
  \tcc{Proof}
  Let $\pi$ be the original accepting proof of $(P, V)$\;
  Define the proof oracle $\tau$ by $\tau(\gamma) = \ECC(\pi(\gamma))$\;
  \KwRet{$(\pi, \tau)$}\;
  \tcc{Verifier}
  Compute the set of queries $Q$ of $V$ on input $x$\;
  Simulate $V$ on input $x$ and proof $\pi$\;\nllabel{line:sim-boolean-p}
  \If{$V$ rejects}{
    \Reject\;
  }
  \For{$\gamma \in Q$}{
    Compute $\ECC(\pi_{a}(\gamma))$\;
    Query $\tau(\gamma)$\;
    \If{$\ECC(\pi_{a}(\gamma)) \ne \tau(\gamma)$}{\nllabel{line:check-ecc-tau}
      \Reject\;
    }
  }
  \Accept\;
  \caption{A boolean reduction of a PCP~\cite[Construction 3.6]{GOS25}}\label{alg:boolean-reduction}
\end{algorithm}

\begin{proof}
  We show that \cref{alg:boolean-reduction} is a boolean reduction algorithm.

  By definition, we know if $x \in L$, then $V$ will always accept when given
  proof $\pi$; thus line~\ref{line:sim-boolean-p} will always pass. Further, since
  $\tau(\gamma)$ is defined as being $\ECC(\pi(\gamma))$, the check in
  line~\ref{line:check-ecc-tau} will always pass. Hence, when given an $x \in L$
  and the correct proof, \cref{alg:boolean-reduction} will always pass.

  If $x \notin L$, then the nature of our error-correcting code is that so long as
  our input proof has at least expected distance $\rho$ from any correct view, our
  new input will as well. The reason we need the error-correcting code here is
  that when we move to a smaller alphabet, we can now flip a single bit within
  each character and it will only count for $1/a$ of the distance that it did
  before, since in the larger alphabet it throws off the whole character, while
  here it only throws off the single bit we flipped.
\end{proof}

This is relatively good: it tells us that query complexity is not made much
worse by changing around our language. It is also worth pointing out that one
can always scale \emph{up} the size of their language for free: simply
zero-padding the extra bits will leave you with a functioning PCP for a larger
language. This is particularly useful since later on we will be working with
languages whose base is not $\{0, 1\}$ but some finite field $\mathbb{F}$, and
this way we can scale them to some $\{0, 1\}^{a}$ where $2^{a} \ge \abs{F}$
without worry.

\section{Robust total-degree test}\label{sec:robust-degree}

An important algorithm to have in our back pocket for the rest of this chapter
will be a robust way for a PCP verifier to check if a given polynomial is in
fact a low-total-degree polynomial or not. This will allow us to test for
membership in $\RM[\mathbb{F}, m, d]$ relatively easily.

\begin{thm}[{\cite[Prop.\ 5.7]{Par21}}]\label{thm:robust-low-deg}
  Let $\delta > 0$, and $k, m, d \in \mathbb{N}^{+}$. Let $\mathbb{F}$ be a finite field with
  $\abs{\mathbb{F}} > 25k$. Then there exists a test that,
  \begin{enumerate}
    \item has oracle access to a function $F\colon \mathbb{F}^{m} \rightarrow \mathbb{F}^{k}$,
    \item makes $\abs{F}$ queries to $F$,
    \item runs in time $\poly(\abs{\mathbb{F}}, m, d, k)$,
    \item accepts with probability $1$ if $F \in \RM^{k}[\mathbb{F}, m, d]$, and
    \item if $\Delta(F, \RM^{k}[\mathbb{F}, m, d]) > \varepsilon$, then the expected distance
          between the tester's view of $F$ and any accepting view is in $\Omega(\varepsilon)$.
  \end{enumerate}
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{A function $F\colon \mathbb{F}^{m} \rightarrow \mathbb{F}^{k}$}
  \KwOut{Whether $F \in \RM[\mathbb{F}, m, d]$}
  Sample a uniformly random line $L \in \mathbb{F}^{m}$\;
  \For{$\ell \in L$}{
    $x_{i} \leftarrow$ query $F$ at $\ell$\;
  }
  \eIf{$F|_{\{x_{i} \mid i \in \abs{L}\}}$ agrees with a polynomial
    $p \in \mathbb{F}^{\le d}[X_{1, \ldots, m}]$}{
    \Accept\;
  }{
    \Reject\;
  }
  \caption{A robust low-degree test~\cite[Prop.\ 5.7]{Par21}}\label{alg:robust-low-deg}
\end{algorithm}

The algorithm in the previous proof is described in \cref{alg:robust-low-deg}.
The complexity of the proof (in particular, the proof of robustness) is complex
enough that we will leave it in full generality to~\cite{Par21}. However, we
will note that by definition, if $F \in \RM[\mathbb{F}, m, d]$, then by definition
the restriction of $F$ to a subset $L$ will always agree with a polynomial $p$;
in particular it will agree with $p = F$.

\section{PCPPs for polynomial summation}\label{sec:pcpp-poly-sum}

\begin{defn}[{\cite[Def.\ 4.1]{GOS25}}]\label{def:sum-lang}\index{Sum@$\Sum$}
  The language $\Sum$ is the set of all ordered pairs
  $((\mathbb{F}, 1^{m}, 1^{d}, H, \gamma), F)$, where
  \begin{itemize}
    \item $\mathbb{F}$ is a finite field,
    \item $m, d \in \mathbb{N}$,
    \item $H \subseteq \mathbb{F}$,
    \item $\gamma \in F$, and
    \item $F \in \mathbb{F}^{\le d}[X_{1, \ldots, m}]$ with
          \[
            \sum_{b \in H^{m}}F(b) = \gamma.
          \]
  \end{itemize}
\end{defn}

While the general language $\Sum$ can be very useful, it can be a little
unwieldy, especially since it includes polynomials over all finite fields (so
determining the language can be a little strange). As such, we define a subset
of $\Sum$ where we restrict the possible values of everything except $F$ to a
fixed constant. This is much easier to work with, and it will still be very
useful to us later on.

\begin{defn}\label{def:sum-params}
  For a fixed finite field $\mathbb{F}$, $m, d \in \mathbb{N}$, $H \subseteq \mathbb{F}$, and
  $\gamma \in \mathbb{F}$, the language $\Sum[\mathbb{F}, m, d, H, \gamma]$ is the subset of
  $\Sum$ where the co-named parameters in \cref{def:sum-lang} are equal to their
  fixed values.
\end{defn}

For simplicity, we will often say $F \in \Sum[\mathbb{F}, m, d, H, \gamma]$ in cases
where we mean $((\mathbb{F}, m, d, H, \gamma), F) \in \Sum[\mathbb{F}, m, d, H, \gamma]$ to
reduce redundancy, since every parameter save $F$ is fixed in this definition.
Now that we have defined $\Sum$, we demonstrate a PCPP for it.

\begin{algorithm}[htbp]
  \Proof{
    \For{$i \in \{1, \ldots, m - 1\}$}{
      $g_{i}(X) \leftarrow \sum_{b \in H^{m-i}}F(X, b)$\;
    }
    Define $\pi\colon \mathbb{F}^{m-1} \rightarrow \mathbb{F}^{m-1}$ by
    \[
      \pi(c_{1}, \ldots, c_{m-2}, \alpha) = (g_{1}(\alpha), g_{2}(c_{1}, \alpha), \ldots, g_{m-1}(c_{1}, \ldots, c_{m-2}, \alpha))
    \]
    for each $(c_{1}, \ldots, c_{m-2}, \alpha) \in \mathbb{F}^{m-1}$\;
    \KwRet{$\pi$}\;
  }
  \Verifier{
    Sample $c \in \mathbb{F}^{m-1}$ at random\;
    \For{$\alpha \in \mathbb{F}$}{
      Query $\pi(c_{1}, \ldots, c_{m-2}, \alpha)$\;\nllabel{line:query-pi}
      Query $F(c_{1}, \ldots, c_{m-1}, \alpha)$\;\nllabel{line:query-f}
    }
    \For{$i \in \{1, \ldots, m-1\}$}{
      \If{$g_{i} \notin \mathbb{F}[X_{1, \ldots, i}^{\le d}]$}{
        \Reject\;
      }
    }
    \If{$\sum_{b \in H}g_{1}(b) \ne \gamma$}{\nllabel{line:check-g_1}
      \Reject\;
    }
    \For{$i \in \{1, \ldots, m - 2\}$}{
      Check
      \begin{algomathdisplay}\nllabel{line:check-g_i}
        \sum_{b \in H}g_{i+1}(c_{1}, \ldots, c_{i}, b) = g_{i}(c_{1}, \ldots, c_{i})
      \end{algomathdisplay}
    }
    Check
    \begin{algomathdisplay}\nllabel{line:check-g_last}
      \sum_{b \in H}F(c, b) = g_{m-1}(c)
    \end{algomathdisplay}
    Run \cref{alg:robust-low-deg} on $F$, with proximity parameter
    $\delta_{R} = \min(\delta, 1/5)$\;\nllabel{line:check-f-degree}
    \Accept{} if and only if the prior test passes\;
  }
  \caption{A robust PCPP for $\Sum$~\cite[Construction 4.3]{GOS25}}\label{alg:sum-pcpp}
\end{algorithm}

Finally, it is time to introduce a verifier for $\Sum$.

\begin{thm}[{\cite[Lemma 4.2]{GOS25}}]\label{thm:pcpp-sum}
  Let $\delta > 0$, $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$,
  $\gamma \in \mathbb{F}$, and $m, d \in \mathbb{N}$ such that $\frac{md}{\abs{\mathbb{F}}} < \delta$
  and $d > \abs{H} + 1$. Then there exists a PCP of proximity for
  $\Sum[\mathbb{F}, m, d, H, \gamma]$ over the alphabet $\mathbb{F}^{m+1}$ with
  proximity parameter $\delta$ and robustness parameter $\rho = \Omega(\delta)$. Further, the
  verifier makes $O(\abs{\mathbb{F}})$ queries to $F$ and $\pi$ and the proof
  length is $O(\abs{\mathbb{F}}^{m})$. Alternately,
  \[
    \Sum[\mathbb{F}, m, d, H, \gamma] \in
    \PCPP_{\mathbb{F}^{m+1}}\pcppr{\poly(n)}{\abs{\mathbb{F}}}{\delta}{\varepsilon}{s}{\Omega(\delta)}.
  \]
\end{thm}

\begin{proof}
  We construct such an algorithm as \cref{alg:sum-pcpp}. To show this is a PCPP,
  we will first show that the verifier will always accept when given a valid
  input and proof; following that we will show it will correctly reject for all
  inputs not near any polynomial in the language.

  Let $F \in \Sum[\mathbb{F}, m, d, H, \gamma]$ and let $\pi$ be the honest proof. In
  this case, our definition of $g_{i}$ is as
  \begin{equation}
    g_{i}(x_{1}, \ldots, x_{i}) = \sum_{b \in H^{m-i}}F(x_{1}, \ldots, x_{i}, b_{1}, \ldots, b_{m-i}).
  \end{equation}
  In particular, this means
  \begin{align*}
    \sum_{b \in H}g_{1}(b) &= \sum_{b \in H}\sum_{c \in H^{m-1}}F(b, c_{1}, \ldots, c_{m-1}) \\
                      &= \sum_{b \in H^{m}}F(b_{1}, \ldots, b_{m}) \\
                      &= \gamma.
  \end{align*}
  Hence, the check in line~\ref{line:check-g_1} will always pass.

  Further, we have that
  \begin{align*}
    g_{i}(x_{1}, \ldots, x_{i}) &= \sum_{b \in H^{m-i}}F(x_{1}, \ldots, x_{i}, b_{1}, \ldots, b_{m-i}) \\
                           &= \sum_{a \in H}\sum_{b \in H^{m-i-1}}F(x_{1}, \ldots, x_{i}, a, b_{1}, \ldots, b_{m-i-1}) \\
                           &= \sum_{a \in H}g_{i+1}(x_{1}, \ldots, x_{i}, a).
  \end{align*}
  Hence, the check in line~\ref{line:check-g_i} will always pass.

  The check in line~\ref{line:check-g_last} follows immediately from the
  definition of $g_{m-1}$, and thus it will pass. Since $F$ is in our language,
  it follows that it has degree at most $d$ and therefore the check in
  line~\ref{line:check-f-degree} will pass. Since all the checks pass, it
  follows that \cref{alg:sum-pcpp} will always accept when given an $F$ in the
  language and an honest proof.

  Next, let $F$ be $\delta$-far from $\Sum[\mathbb{F}, m, d, H, \gamma]$. We break this
  into two cases: those where $F$ is $\delta_{\RM}$-far from $\RM[\mathbb{F}, m, d]$
  and those where $F$ is $\delta_{\RM}$-close to $\RM[\mathbb{F}, m, d]$.

  Before that, note that the simulation of the low-degree test in
  line~\ref{line:check-f-degree} makes $O(\mathbb{F})$ queries (as per
  \cref{thm:robust-low-deg}), and the rest of the algorithm makes
  $2\abs{\mathbb{F}}$ queries, all in lines~\ref{line:query-pi}
  and~\ref{line:query-f}. Both of these are independent of the length of the
  input, meaning the proportion of the queries made by each of the two halves of
  the verifier are constant.

  If $F$ is $\delta_{\RM}$-far from $\RM[\mathbb{F}, m, d]$, then
  \cref{alg:robust-low-deg} will always have its view at least $\Omega(\delta)$ from any
  accepting view as per \cref{thm:robust-low-deg}. Since the low-degree test is
  a constant fraction of the total number of queries, this means that the
  expected distance between the low-degree verifier's view and any accepting
  view is also $\Omega(\delta)$. Thus, $V$ has expected robustness $\Omega(\delta)$ in this case.

  If $F$ is $\delta_{\RM}$-close to $\RM[\mathbb{F}, m, d]$, then we need to
  introduce a new lemma.

  \begin{lemma}[{\cite[Lemma 4.4]{GOS25}}]\label{lem:sum-ev}
    Let $\tilde{F}\colon \mathbb{F}^{m} \rightarrow \mathbb{F}$ be $\delta_{\Sigma}$-far from
    $\Sum[\mathbb{F}, m, d, H, \gamma]$, but $\delta_{\RM}$-close to
    $\RM[\mathbb{F}, m, d]$ for some
    $\frac{md}{\abs{\mathbb{F}}} < \delta_{\RM} \le \delta_{\Sigma}$ and $d \ge \abs{H} + 1$. Then,
    for all proofs $\pi^{*}$,
    \begin{multline} % NOTE: This is the view of the verifier in alg:sum-pcpp
      \underset{c \leftarrow \mathbb{F}^{m-1}}{\mathbb{E}}\mleft[
      \Delta\mleft(
      (\pi^{*}(c_{1}, \ldots, c_{m-2}, \alpha)_{\alpha \in \mathbb{F}}, \tilde{F}(c_{1}, \ldots, c_{m-1}, \alpha)_{\alpha \in \mathbb{F}}),
      \Acc(V)
      \mright)
      \mright] \\
      \ge \frac{\min(\delta_{\RM}, 1 - 4\delta_{\RM})}{2}.
    \end{multline}
  \end{lemma}

  We leave this lemma unproven, as the proof is mostly nasty algebra. In brief,
  it gives us a specific bound on the distance between the accepting views of
  $V$ and the set of polynomials given by the verifier.

  By \cref{lem:sum-ev}, we know that the view of the verifier is at least
  $\min(\delta_{\RM}, 1 - 4\delta_{\RM})/2$ from any accepting view. Since we defined
  $\delta_{\RM} = \min(\delta, 1/5)$, this means the distance is at least
  $\min(\delta/2, 1/10)$, which is in $\Omega(\delta)$. Since a constant proportion of
  \cref{alg:sum-pcpp}'s queries occur outside of line~\ref{line:check-f-degree},
  this means the total distance between the view of $V$ and any accepting view
  is still $\Omega(\delta)$. Thus, $V$ has expected robustness $\Omega(\delta)$ in this case as
  well.

  Since $V$ has expected robustness $\Omega(\delta)$ regardless of whether or not $F$ is
  $\delta_{\RM}$-close to $\RM[\mathbb{F}, m, d]$, this means that $V$ has expected
  robustness $\Omega(\delta)$ overall. Hence, \cref{alg:sum-pcpp} is a robust PCPP for
  the language $\Sum$.
\end{proof}

\section{Properties of $\OSAT$}

To begin defining a zero-knowledge $\PCP$ for $\NP$ and $\NEXP$, we must first
define a few convenience polynomials.

\begin{defn}\label{def:g_a}
  Let $B$ be an instance of $\OSAT$ and let $\hat{B}$ be the low-degree
  extension of $1 - B$. Let $\hat{A}\colon \mathbb{F}^{m_{2}} \rightarrow \mathbb{F}$. We
  define $g_{\hat{A}}\colon \mathbb{F}^{m_{1} + 3m_{2} + 3} \rightarrow \mathbb{F}$ to be
  \begin{multline}\label{eqn:g_a}
    g_{\hat{A}}(z, b_{1}, b_{2}, b_{3}, a_{1}, a_{2}, a_{3}) \\ =
    \hat{B}(\gamma_{1}(z), \gamma_{2}(b_{1}), \gamma_{2}(b_{2}), \gamma_{2}(b_{3}), a_{1}, a_{2}, a_{3})
    \prod_{i=1}^{3}(\hat{A}(b_{i}) + a_{i} - 1).
  \end{multline}
\end{defn}

\begin{defn}\label{def:h_c}
  Let $B$ be an instance of $\OSAT$ and let $\hat{B}$ be the low-degree
  extension of $1 - B$. Let $\hat{C}\colon \mathbb{F}^{m_{2} + k} \rightarrow \mathbb{F}$.
  We define
  $h_{\hat{C}}\colon \mathbb{F}^{m_{1} + 3m_{2} + 3 + 3k} \rightarrow \mathbb{F}$ to be
  \begin{multline}\label{eqn:h_c}
    h_{\hat{c}}(z, b_{1}, b_{2}, b_{3}, a_{1}, a_{2}, a_{3}, c_{1}, c_{2}, c_{3}) \\ =
    \hat{B}(\gamma_{1}(z), \gamma_{2}(b_{1}), \gamma_{2}(b_{2}), \gamma_{2}(b_{3}), a_{1}, a_{2}, a_{3})
    \prod_{i=1}^{3}(\hat{C}(b_{i}, c_{i}) + (a_{i} - 1)\delta_{0^{k}}(c_{i})).
  \end{multline}
\end{defn}

As a reminder, $\delta$ is the low-multidegree polynomial defined in
\cref{eqn:delta-poly} that for all $x, y \in H^{n}$, $\delta_{x}(y) = [x = y]$.

From the above lemma, we get two equivalent statements to whether $B \in \OSAT$.
These equivalent statements will play an important role in our proof of
\cref{thm:nexp-zk-pcp}.

From here, we will prove a nice relationship between these two polynomials.
This correspondence will come in handy when proving the next lemma, which will
eventually give us some nice equivalent statements to $\OSAT$.

\begin{lemma}\label{lem:g_a-vs-h_c}
  If $\hat{A}(X) = \sum_{c \in H^{k}}\hat{C}(X, c)$, then
  \begin{equation}\label{eqn:g_a-vs-h_c}
    \sum_{c_{1}, c_{2}, c_{3} \in H^{k}}h_{\hat{C}}(z, b_{1}, b_{2}, b_{3}, a_{1}, a_{2}, a_{3}, c_{1}, c_{2}, c_{3})
    = g_{\hat{A}}(z, b_{1}, b_{2}, b_{3}, a_{1}, a_{2}, a_{3}).
  \end{equation}
\end{lemma}

We will use this lemma to prove that two important conditions are equivalent to
$B$ being an element of $\OSAT$. Our upcoming PZK-PCP for $\OSAT$ will leverage
these conditions heavily in order to prove its correctness and soundness.

\begin{lemma}[{\cite[Claim 6.5]{GOS25}}]\label{lem:three-equiv}
  Let $B\colon \{0, 1\}^{m} \rightarrow \{0, 1\}$. Then the following statements are
  equivalent:
  \begin{enumerate}
    \item\label{item:b-in-osat} $B \in \OSAT$;
    \item\label{item:exists-a} there exists a polynomial
          $\hat{A}\colon \mathbb{F}^{m} \rightarrow \mathbb{F}$ such that for all
          $z \in H^{m_{1}}$, $b_{1}, b_{2}, b_{3} \in H^{m_{2}}$, and
          $a_{1}, a_{2}, a_{3} \in \{0, 1\}$,
          \begin{equation}
            g_{\hat{A}}(z, b_{1}, b_{2}, b_{3}, a_{1}, a_{2}, a_{3}) = 0;
          \end{equation}
    \item\label{item:exists-c} there exists a polynomial
          $\hat{C}\colon \mathbb{F}^{m_{2}+k} \rightarrow \mathbb{F}$ such that for all
          $z \in H^{m_{1}}$, $b_{1}, b_{2}, b_{3} \in H^{m_{2}}$, and
          $a_{1}, a_{2}, a_{3} \in \{0, 1\}$,
          \begin{equation}
            \sum_{c_{1}, c_{2}, c_{3} \in H^{k}}h_{\hat{C}}(z, b_{1}, b_{2}, b_{3}, a_{1}, a_{2}, a_{3}, c_{1}, c_{2}, c_{3}) = 0.
          \end{equation}
  \end{enumerate}
\end{lemma}

\begin{proof}
  \Cref{lem:g_a-vs-h_c} tells us that \cref{item:exists-a,item:exists-c} are
  equivalent. Hence, all we need to show is that
  \cref{item:b-in-osat,item:exists-a} are equivalent.

  ($\Rightarrow$) Let $B \in \OSAT$, and let $\hat{A}$ be the low-degree extension of a satisfying
  assignment $A$. Further fix
  $z, b_{1}, b_{2}, b_{3}, a_{1}, a_{2}, a_{3} \in \{0, 1\}$. In the case where
  $B(z, b_{1}, b_{2}, b_{3}, a_{1}, a_{2}, a_{3}) = 1$, it follows
  $\hat{B} = 1 - B = 0$, so we are done. In the other case, Otherwise, we know
  $A$ is a satisfying assignment; thus for some $i \in \{1, 2, 3\}$
  $A(b_{i}) \ne a_{i}$, since otherwise $B$ must by definition evaluate to $1$.
  Hence, by the definition of $\hat{A}$, $\hat{A}(b_{i}) = 1 - a_{i}$. Hence,
  the product on the right-hand side of \cref{eqn:g_a} evaluates to $0$ and thus
  $g_{\hat{A}}$ is zero for all parameters.

  ($\Leftarrow$) Conversely, let $\hat{A}$ be a polynomial such that $g_{\hat{A}}$ is
  zero on all inputs. Define $A$ to be the function
  \begin{align*}
    A: \{0, 1\}^{s}  &\rightarrow \{0, 1\} \\
    b &\mapsto [\hat{A}(b) = 1].
  \end{align*}
  Let $z \in \{0, 1\}^{r}$ and $b_{1}, b_{2}, b_{3} \in \{0, 1\}^{s}$ be arbitrary.
  Further, let $a_{1}, a_{2}, a_{3} \in \{0, 1\}$ such that
  $B(z, b_{1}, b_{2}, b_{3}, a_{1}, a_{2}, a_{3}) = 0$. If no such $a_{i}$
  exist, then we know whatever the output of $A$, $B$ will evaluate to $1$;
  hence we are done. Otherwise, it must be that there exists some
  $i \in \{1, 2, 3\}$ such that $\hat{A}(b_{i}) = 1 - a_{i}$ and thus
  $A(b_{i}) = \hat{A}(b_{i})$ by definition. Thus, it follows that
  $A(b_{i}) \ne a_{i}$. By contradiction, therefore, it must be the case that if
  $A(b_{i}) = a_{i}$ for all $i$, then $B$ evaluates to $1$, and thus $A \in \OSAT$.
\end{proof}

\section{A PCP for $\NP$}

Finally, it is time to construct our PCP for all of $\NP$. We will start by
constructing a PCP that does not have particularly nice parameters, and then use
several of the theorems we have shown earlier in this chapter to show that there
exists a PCP with the parameters we would like. We do this so that our
constructed algorithm is reasonably intuitive and explainable.

\begin{thm}[{\cite[Theorem 6.3]{GOS25}}]\label{thm:nexp-pcp}
  \[
    \NEXP \subseteq
    \PCP_{\Sigma(n)}\pcpr{\poly(n)}{\poly(n)}{s}{\Omega(1)}.
  \]
  where $\Sigma(n)$ is any alphabet with $\abs{\Sigma(n)} \in \poly(n, q)$.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{A 3-CNF $B\colon \{0, 1\}^{r+3s+3} \rightarrow \{0, 1\}$}
  \KwOut{Whether $B$ is implicitly satisfiable}
  \Proof{
    Let $A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ be a satisfying assignment for $B$\;
    Choose $\hat{C} \in \mathbb{F}[X_{m_{2} + k}^{\le 2(\abs{H} - 1)}]$ randomly such
    that $\sum_{c \in H^{k}}\hat{C}(b, c) = A(\gamma_{2}, b)$ for all
    $b \in H^{m_{2}}$\;\nllabel{line:sample-c}
    \For{$\tau \in \mathbb{F}^{m_{1}+3m_{2}+3}$}{
      Let $\pi_{\tau}$ be a PZK-PCPP for the claim
      \begin{algomathdisplay}\nllabel{line:pcpp-claim}
        \sum_{\substack{z \in H^{m_{1}} \\ b_{1}, b_{2}, b_{3} \in H^{m_{2}}}}
        \sum_{\substack{a \in \{0, 1\}^{3} \\ c_{1}, c_{2}, c_{3} \in H^{k}}}
        \delta_{(z,b,a)}(\tau)h_{\hat{C}}(\tau, c_{1}, c_{2}, c_{3}) = 0
      \end{algomathdisplay}
    }
    \KwRet{$(\pi_{C}, (\pi_{\tau})_{\tau \in \mathbb{F}^{m_{1}+3m_{2}+3}})$}\;
  }
  \Verifier{
    $q_{a}, q_{b} \leftarrow 0$\;
    \Repeat{$2q_{a} > q_{b}$ and $2q_{b} > q_{a}$}{
      \Repeat{$2q_{a} > q_{b}$}{
        Run \cref{alg:robust-low-deg} on the proof, with $\varepsilon = 1/100$ and
        $\delta = 1/2$\;\nllabel{line:low-deg-test}
        Add the total number of queries in the last line to $q_{a}$\;
        \If{the above rejects}{
          \Reject\;
        }
      }
      \Repeat{$2q_{b} > q_{a}$}{
        Choose
        $\tau \in \mathbb{F}^{m_{1}} \times (\mathbb{F}^{m_{2}})^{3} \times \mathbb{F}^{3}$ at random\;
        Simulate \cref{alg:sum-pcpp} with proof $\pi_{\tau}$\;\nllabel{line:sumcheck-pi-tau}
        Add the total number of queries in the last line to $q_{b}$\;
        \For{$i \in \{1, 2, 3\}$}{
          Query $\hat{C}$ at $(\nu_{i}, \eta_{i})$\;
          Add $1$ to $q_{b}$\;
        }
        Compute $h_{\hat{C}}(\tau, \eta_{1}, \eta_{2}, \eta_{3})$\;
        \If{the claim in line~\ref{line:sumcheck-pi-tau} is false}{\nllabel{line:sumcheck-claim}
          \Reject\;
        }
      }
    }
    \Accept\;
  }
  \caption{A $\PZKPCP$ for $\OSAT$~\cite[Construction 6.4]{GOS25}}\label{alg:pcp-osat}
\end{algorithm}

\begin{proof}
  We construct a PZK-PCP for $\OSAT$, a $\NEXP$-complete language, in
  \cref{alg:pcp-osat}. First, note that the combined effect of the three
  ``repeat'' loops is to ensure that the number of queries taken up by each of
  the two inner portions is no more than $1/3$ of the total number of queries.
  This will be useful to us later on when we try to prove soundness.

  We know \cref{alg:robust-low-deg} runs in polynomial time, as does the
  verifier for \cref{alg:sum-pcpp}. Computing the value of a known polynomial is
  also polynomial time, as is querying values; hence the entire algorithm runs
  in polynomial time.

  Next, we show \cref{alg:pcp-osat} will accept with probability at least $2/3$
  when given a $B \in \OSAT$ and the honest proof. As per \cref{lem:three-equiv},
  the claim in line~\ref{line:pcpp-claim} is true. Hence,
  line~\ref{line:low-deg-test} will always accept. From there,
  \cref{alg:sum-pcpp} will always succeed, as per its own soundness
  guarantee. Further, the claim it outputs will always be true, so
  line~\ref{line:sumcheck-claim}'s test will always succeed. Hence,
  \cref{alg:pcp-osat} will always accept given an honest prover and a
  $B \in \OSAT$.

  Next, we show \cref{alg:pcp-osat} will reject with probability at least $2/3$
  when given a $B \notin \OSAT$. We aim to show this in two cases. The first is where
  $\pi_{C}$ is $\varepsilon$-close to $\RM[\mathbb{F}, m_{2}+k, m_{2}+d]$, in which case we
  will show line~\ref{line:low-deg-test} will reliably fail. The second is where
  $\pi_{C}$ is $\varepsilon$-far, in which case we will show line~\ref{line:sumcheck-pi-tau}
  will reliably fail.

  Define $\varepsilon = 1/100$. If $\pi_{C}$ is $\varepsilon$-close to
  $\RM[\mathbb{F}, m_{2}+k, (m_{2}+k)d]$, then let
  $\hat{C} \in \RM[\mathbb{F}, m_{2}+k, (m_{2}+k)d]$ be the closest element to
  $\pi_{C}$. By the definition of distance and since $\pi_{C}$ is $\varepsilon$-close, this
  means $\Delta(\hat{C}, \pi_{C}) \le \varepsilon$. Next, note that a random evaluation of $h_{f}$
  depends on three random evaluations of $f$; as such it must be that
  $\Delta(h_{\hat{C}}, h_{\pi_{C}}) \le 3\varepsilon$.

  By \cref{lem:three-equiv}, there exists $z \in H^{m_{1}}$,
  $b_{1}, b_{2}, b_{3} \in H^{m_{2}}$, and $a_{1}, a_{2}, a_{3} \in \{0, 1\}$ such
  that
  \begin{equation}
    \sum_{\substack{z \in H^{m_{1}} \\ b_{1}, b_{2}, b_{3} \in H^{m_{2}}}}\sum_{a_{1}, a_{2}, a_{3} \in \{0, 1\}}
    \delta_{(z,b,a)}(X)\sum_{c_{1}, c_{2}, c_{3} \in H^{k}}h_{\hat{C}}(X, c_{1}, c_{2}, c_{3})
  \end{equation}
  is a nonzero polynomial in
  $\mathbb{F}[X_{1, \ldots, m_{1}+m_{2}+m_{3}}^{O(d_{B}\abs{H})}]$. Hence, the claim
  our verifier makes in line~\ref{line:sumcheck-pi-tau} is false with
  probability $1 - O((m_{1}+m_{2}+m_{3})d_{b}\abs{H}/\abs{\mathbb{F}})$.

  If $\pi_{C}$ is $\varepsilon$-far from $\RM[\mathbb{F}, m_{2}+k, (m_{2}+k)d]$, then the
  verifier in line~\ref{line:sumcheck-pi-tau}'s view is $\Omega(\varepsilon)$-far from accepting, as per
  \cref{thm:robust-low-deg}.

  What we have now shown is that regardless of the distance of $\pi_{C}$ from
  $\RM[\mathbb{F}, m_{2}+k, m_{2}+d]$, at least one of the two portions of our
  view is $\Omega(\varepsilon)$-far from an accepting portion of the view. Since we know that
  these portions take up at least $1/3$ of the view,\footnote{We note that the
    fraction $1/3$ is actually arbitrary here, so long is it is a constant
    nonzero fraction; $1/3$ is just a relatively easy number to deal with and it
    will make convergence faster.} it means that the total distance of the
  verifier's view from an accepting view is at least $\Omega(\varepsilon/3) = \Omega(\varepsilon)$. Hence,
  \cref{alg:pcp-osat} has expected robustness $\Omega(\varepsilon)$.
\end{proof}

\begin{cor}\label{cor:np-pcp}
  \begin{equation}\label{eqn:np-subseteq-pzkpcp}
    \NP \subseteq \PCP_{\Sigma(n)}\pcpr{\log(n)}{\poly(\log(n))}{s}{\Omega(1)}.
  \end{equation}
\end{cor}

\begin{proof}
  For this, we leverage our work from \cref{thm:nexp-pcp}. In that, we used the
  fact that $\OSAT$ is a $\NEXP$-complete problem. However, the Cook-Levin
  variant we proved in \cref{thm:cook-levin-general} is even more general than
  that: in particular it showed that log-length $\OSAT$ is in fact
  $\NP$-complete. Hence, if we adjust the length of our input to be $\log(n)$,
  the inclusion here follows.
\end{proof}

At last, it is time for our proof of the main PCP theorem.

\begin{thm}[{\cref{thm:pcp-theorem}, restated}]
  $\NP \subseteq \PCP[\log(n), 1]$.
\end{thm}

\begin{proof}
  To show this, we will take the PCP for $\NP$ that we showed in
  \cref{cor:np-pcp}, which has relatively weak bounds and an arbitrary
  alphabet, and transform it into one with the exact parameters we seek. We do
  this first through an alphabet reduction (as per \cref{thm:alph-reduction})
  and then through proof-composing it with the algorithm for $\CktVal$ with
  \cref{cor:constant-query}, we can get a constant query-complexity PCP\@. Since
  several of these class inclusions involve modifying a small number of
  parameters in a relatively complex class, we will be highlighting the changed
  parameters in each inclusion statement.

  Let $q^{*}(n) \le 2^{\poly(n)}$ be arbitrary. This will be the query complexity
  of our final PCP after we do all the class inclusions. As per
  \cref{cor:np-pcp}, we know that
  \begin{equation}\label{eqn:np-subseteq-pcp}
    \NP \subseteq \PCP_{\Sigma(n)}\pcpr{\log(n)}{\poly(\log(n))}{s}{\Omega(1)}.
  \end{equation}
  where $\abs{\Sigma(n)} \in \poly(n)$. \Cref{cor:np-pcp} only guarantees this
  inclusion for alphabets of $\{0, 1\}^{a}$, however a PCP over $\Sigma(n)$ is
  equivalent to a PCP over $\{0, 1\}^{\log_{2}(\abs{\Sigma(n)})}$ by a simple
  relabeling of alphabet items, so this inclusion still holds.

  Next, we perform an alphabet reduction: by \cref{thm:alph-reduction},
  \begin{equation}\label{eqn:alph-red-np}
    \begin{aligned}
      \PCP_{\color{pumpkin}{\Sigma(n)}}&\pcpr{\log(n)}{\poly(\log(n))}{s}{\Omega(1)} \\
      \subseteq \PCP_{\color{pumpkin}{\{0,1\}}}&\pcpr{\log(n)}{\poly(\log(n))}{s}{\Omega(1)}.
    \end{aligned}
  \end{equation}

  Next, we perform proof composition. By \cref{cor:constant-query}, we have that
  \begin{equation}\label{eqn:qstar-arb-np}
    \begin{aligned}
      \PCP_{\{0,1\}}\pcpr{\log(n)}{\color{pumpkin}{\poly(\log(n)))}}{\color{plum}{s}}{\color{plum}{\Omega(1)}}
      \subseteq \PCP_{\{0,1\}}[\log(n), 1].
    \end{aligned}
  \end{equation}
  By combining the inclusions in
  \crefrange{eqn:np-subseteq-pcp}{eqn:qstar-arb-np}, we get that
  $\NP \subseteq \PCP[\log(n), 1]$, as desired.
\end{proof}

This has shown that there \emph{exists} a constant-query PCP for any language in
$\NP$, but it would be nice of us to know what that query count is. First, note
that there does exist a finite upper bound on the number of constant queries:
since we can always perform a polynomial-time reduction and retain a functioning
PCP, the necessary number of queries for any $\NP$-complete problem is
sufficient to prove any problem in $\NP$. Johan H\r{a}stad showed that we can
solve the $\SAT$ problem with only three queries, so it follows that any
language in $\NP$ can be solved by querying only three bits.

\begin{thm}[{\cite{Has97}}]\label{thm:pcp-max-queries}
  Any language in $\NP$ has a $\PCP$ that queries a maximum of $3$ bits of the
  proof and uses $O(\log n)$ random bits.
\end{thm}

% \chapter{Quantum computation}\label{chap:quantum}

% Quantum computation is a model of computation that uses non-classical physical
% interactions. Quantum computers are interesting from a complexity-theoretic
% perspective for two main reasons: first, it has a built-in way to generate
% randomness, and second, it seems to be notably faster than classical computers.
% The exact level of speedup that quantum computers give is still the subject of
% much research, but we will be working with quantum computers for a slightly
% different reason.

% For us, quantum computers will be interesting because of their ability to share
% entangled bits. Briefly, entangled bits are a pair of states where measuring one
% of the states affects the measurement of the other.

% \section{Quantum computers}\label{sec:quant-comp}

% \begin{defn}\label{def:qubit}\index{qubit}
%   A \emph{qubit} is a unit vector in $\mathbb{C}^{2}$.
% \end{defn}

% \begin{defn}\label{def:operator}\index{operator}
%   A \emph{operator} is a linear function $A\colon V \rightarrow V$ such that
%   $v \cdot A \cdot v^{T} \ge 0$ for all $v \in V$.
% \end{defn}

% \begin{defn}\label{def:quantum-tm}\index{Turing machine!quantum}
%   A \emph{quantum Turing machine} is
% \end{defn}

% \subsection{Measurement}

% \begin{defn}\label{def:projective-measure}\index{projective measurement}
%   A \emph{projective measurement} is
% \end{defn}

% \begin{defn}\label{def:povm}\index{positive operator-valued measure}
%   A \emph{positive operator-valued measure} is
% \end{defn}

% \section{Quantum complexity classes}

% \begin{defn}\label{def:bqp}\index{BQP@$\BQP$}
%   The class $\BQP$ is the class of all languages $L$ such that there exists a
%   quantum Turing machine $Q$ such that
%   \begin{enumerate}
%     \item For all $x \in L$, $\mathbb{P}[Q(x) = 1] \ge 2/3$, and
%     \item for all $x \notin L$, $\mathbb{P}[Q(x) = 1] \le 1/3$.
%   \end{enumerate}
% \end{defn}

% \section{Quantum interactive proofs}\label{sec:quant-interactive}

% \begin{defn}\label{def:mip-star}\index{MIP*@$\MIP*$}
%   The class $\MIP*$ is the class of all languages with a $\MIP$ instance where
%   the two provers are quantum computers sharing a single entangled bit.
% \end{defn}

% This statement

% \begin{thm}[{\cite{JNVWY21}}]\label{thm:mip-star-is-re}
%   $\MIP* = \RE$.
% \end{thm}

% \section{Gentle measurement}\label{sec:gentle-measurement}

% \begin{lemma}[{\cite{ON07}}]\label{lem:gentle-measurement}
%   Let $\rho$ be a density operator on a Hilbert space $H$, and let $A, B \in L(H)$
%   such that $A^{\dag}A, B^{\dag}B \le \Id$. Then,
%   \begin{equation}
%     \norm{A \rho A^{\dag} - B \rho B^{\dag}} \le 2\sqrt{\tr((A - B)\rho(A - B)^{\dag})}.
%   \end{equation}
% \end{lemma}

% \section{Quantum low-multidegree test}\label{sec:quantum-multidegree-test}

% \begin{thm}[{\cite{JNVWY20}}]\label{thm:quantum-low-degree}
%   There is a universal constant $C > 0$ such that the following holds. Let
%   $(\tilde{P}_{1}, \tilde{P}_{2}, \ket{\Psi})$ be a projective strategy that passes
%   the $(\mathbb{F}, d, m)$-low-multidegree test with probability at least
%   $1 - \varepsilon$. For $i \in \{1, 2\}$, $\alpha \in \mathbb{F}^{m}$, denote by
%   $\{A_{i,\alpha}^{z}\}_{z \in \mathbb{F}}$ the measurement applied by $\tilde{P}_{i}$
%   upon receiving question $\alpha$. Then there exist projective measurements
%   $\{L_{1}^{Q}\}_{Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]}$ and
%   $\{L_{2}^{Q}\}_{Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]}$ such that for
%   $v \in \poly(m, d)(d/\abs{\mathbb{F}} + c)^{C}$, the following holds:
%   \begin{enumerate}
%     \item Consistency with
%           $\{A_{1, \alpha}^{z}\}_{z \in \mathbb{F}, \alpha \in \mathbb{F}^{m}}$ and
%           $\{A_{1, \alpha}^{z}\}_{z \in \mathbb{F}, \alpha \in \mathbb{F}^{m}}$:
%           \begin{align}
%             \mathbb{E}_{\alpha \in \mathbb{F}^{m}}\sum_{Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]}\sum_{z \in \mathbb{F} \setminus \{Q(\alpha)\}}
%             \braopket{\Psi}{A_{1,\alpha}^{z} \otimes L_{2}^{Q}}{\Psi} &\le v, \\
%             \mathbb{E}_{\alpha \in \mathbb{F}^{m}}\sum_{Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]}\sum_{z \in \mathbb{F} \setminus \{Q(\alpha)\}}
%             \braopket{\Psi}{L_{1}^{Q} \otimes A_{2,\alpha}^{z}}{\Psi} &\le v.
%           \end{align}
%     \item Consistency of $\{L_{1}^{Q}\}_{Q}$ and $\{L_{2}^{Q}\}_{Q}$:
%           \begin{equation}
%             \sum_{Q \ne Q' \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]}\braopket{\Psi}{L_{1}^{Q} \otimes L_{2}^{Q}}{\Psi} \le v.
%           \end{equation}
%   \end{enumerate}
% \end{thm}

% \begin{proof}
%   % TODO
% \end{proof}

% \chapter{Lifting $\IPCP$ to $\MIP*$}\label{chap:ipcp-to-mip}

% The main goal of the next two chapters is to prove that any language in $\NEXP$
% has a multi-prover interactive proof where the two provers can share an
% entangled bit and further, that this can be done in a way that preserves zero
% knowledge. The first major step in this is described in this chapter: to show
% that any interactive probabilistically-checkable proof can be replaced by a
% multi-prover interactive proof, and that if the IPCP is zero-knowledge then so
% is the constructed MIP*.

% \TODO{Another paragraph on consequences of this and potentially high-level
%   overview of the proof}

% \section{Reducing query complexity}\label{sec:reduce-queries}

% The first step in transforming IPCPs into MIP* instances is to reduce the query
% complexity. As it turns out, any IPCP is equivalent to an IPCP that makes one
% query, subject to a minor relaxation of the other constraints. Not only does
% this work on normal IPCPs, but when used on a zero-knowledge IPCP, the resulting
% IPCP is also zero-knowledge. We will be using this transformation in order to
% make it easier in the next step to lift from $\IPCP$ to $\MIP*$.

% \begin{thm}[{\cite[Prop.\ 9.2]{CFGS22}}]\label{thm:ipcp-one-query}
%   There exists a transformation $T$ such that, for every $m, d \in \mathbb{N}$ and finite
%   field $\mathbb{F}$, if
%   \begin{equation*}
%     (P, V) \in \IPCP\ldipcp{r}{\ell}{c}{q}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{\varepsilon},
%   \end{equation*}
%   then $T(P, V)$ recognizes the same language as $(P, V)$ and
%   \begin{equation*}
%     T(P, V) \in \IPCP\ldipcp{r+1}{\ell}{c+\poly(m,d,q)}{1}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{\varepsilon+\frac{mdq}{\abs{\mathbb{F}}-q}}.
%   \end{equation*}
% \end{thm}

% \begin{algorithm}[htbp]
%   \KwIn{An honest oracle $R \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ and string $s$}
%   \KwOut{Whether or not $s \in L_{(P, V)}$}
%   Let $S \subseteq \mathbb{F}$ with $\abs{S} < q$\;
%   $V'$: choose random $r \in \mathbb{F}^{m}$\;
%   $V'$: choose random $t \in \mathbb{F} \setminus S$\;
%   $V'$: Simulate $V$ on $s$ answering every query with
%   $0$\;\nllabel{line:zero-sim}
%   Let $A$ be the set of queries of $V$\;
%   $V'$: compute polynomial $\gamma\colon \mathbb{F} \rightarrow \mathbb{F}^{m}$ of degree $q$
%   such that $\{\gamma(s)\}_{s \in S} = A$ and $\gamma(t) = r$\;
%   $V'$: Send $\gamma$ to $P'$\;
%   $P'$: Reply with the coefficients of the polynomial $\rho = R \circ \gamma$\;
%   $V'$: query $R$ at $r$ and receive answer $a$\;
%   \If{$a \ne \rho(t)$}{\nllabel{line:check-rho}
%     \Reject\;
%   }
%   $V'$: Simulate $V(x)$, answering all queries $a \in A$ with
%   $\rho(A)$\;\nllabel{line:proper-sim}
%   \KwRet{the result of $V(x)$}\;
%   \caption{A single-query, zero-knowledge transformation of an
%     IPCP~\cite[Construction 4]{CFGS22}}\label{alg:single-query}
% \end{algorithm}

% \begin{proof}
%   We describe the result of $T$ in \cref{alg:single-query}. First, note that
%   this algorithm is not $T$ directly, but the result of $T(P, V)$. So, what we
%   require is to show that $L_{T(P, V)} = L_{(P, V)}$, and to briefly demonstrate
%   why $V'$ runs in polynomial time.

%   To show $V'$ runs in polynomial time, note that the random choices are in
%   polynomial time (as a reminder, random choice is polynomial relative to the
%   length), simulating $V$ is polynomial time through the definition of $V$, and
%   computing polynomials can be done in polynomial time as well.

%   When $P'$ is honest, then this will simulate $V$ answering all its queries via
%   $\rho$ (which gives the correct answer to every value in $A$); hence $V$ will
%   gave the correct answer by definition.

%   When $P'$ is dishonest, then we know $\rho$ is a univariate polynomial with
%   degree at most $dq$ (since $\rho = R\gamma$). Further note that our random choice of
%   both $t$ and $\gamma$ means that $\gamma(t)$ is uniformly distributed over
%   $\mathbb{F}^{m}$. Since $\rho$ is a univariate polynomial with degree at most
%   $mdq$, then it can agree with $R \circ \gamma$ at most $mdq$ distinct points without
%   being equal. Hence, since we query it at a random point in $\mathbb{F} \setminus S$,
%   if the check in line~\ref{line:check-rho} passes with at probability bigger
%   than $\frac{mdq}{\abs{\mathbb{F}} - q}$, it must mean that $\rho = R\gamma$ on more
%   than $mdq$ points. Thus, $\rho = R \circ \gamma$ and so we will get the correct answer for
%   all of our queries on $A$. If it passes with a lower probability, then we are
%   within our soundness error overall and so the result does not matter.
% \end{proof}

% Next, we show that \cref{alg:single-query} also preserves zero knowledge. This
% is important for us because zero knowledge is \emph{actually} what we care
% about. Like with most other bounds, the zero-knowledge query bound of the
% one-query algorithm is lower than the query bound of the original algorithm.
% This is because with the one-query transformation, more information is
% necessarily released by the prover; thus a malicious verifier needs fewer
% queries to extract nontrivial information out of the proof.

% \begin{thm}[{\cite[Prop.\ 9.2]{CFGS22}}]\label{thm:one-query-pzk}
%   If $(P, V)$ is perfect zero-knowledge with query bound $b$, then $T(P, V)$
%   (where $T$ is the transformation from \cref{thm:ipcp-one-query}) is perfect
%   zero-knowledge with query bound $b - (mdq + 1)$.
% \end{thm}

% \begin{proof}
%   We again look at \cref{alg:single-query}. First, notice that if we make a
%   query $r \in \gamma(\mathbb{F})$, since $\rho = R\gamma$ we could replace our call to $R$
%   with a call to $\rho(\gamma^{-1}(r))$, and this would no longer count towards our
%   query bound. Further, from what we know of the degree of $\rho$, we know that in
%   the worst case, we would only need $mdq + 1$ queries to $R$ to effectively
%   replicate $\rho$. Hence, if we reduce our query bound by $mdq + 1$, it follows
%   that we can still effectively simulate the interaction of $P'$ and $V'$, since
%   we know $P$ and $V$ have a perfect simulator.
% \end{proof}

% \section{A lifting algorithm}\label{sec:lift-ipcp-mip}

% Next, we construct an algorithm that can transform a single-query IPCP oracle
% into a MIP* program. Like before, we only give the result of the algorithm on a
% given input $(P', V')$, as opposed to the transformation algorithm itself.
% Interestingly, the transformed MIP* requires only one more round than the
% non-transformed IPCP\@. This is thanks to the algorithm we showed earlier in
% \cref{sec:reduce-queries}; that way we need only deal with a single oracle query
% besides the interactive-proof portion of the simulation.

% \begin{thm}[{\cite[Lemma 9.1]{CFGS22}}]\label{thm:lift-ipcp-mip}
%   Let $L$ be a language, let $m, d, q \in \mathbb{N}$, and let $\mathbb{F}$ be a finite
%   field of size $\poly(m, d, q)$ sufficiently large. Then, there exists a
%   transformation
%   \begin{equation*}
%     T: \IPCP\ldipcp{r}{\ell}{c}{q}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{\varepsilon}
%     \rightarrow \MIP*\mipstar{2}{r+1}{c}{1 - \frac{1}{\poly(m, d)}}.
%   \end{equation*}
%   such that $(P', V')$ and $T(P', V')$ recognize the same language.

%   Further, if the IPCP $(P', V')$ is zero-knowledge with query bound
%   $b \ge 2(q+1)md + 3$, then the $\MIP*$ $(P_{1}, P_{2}, V)$ is zero-knowledge.
% \end{thm}

% \begin{algorithm}[htbp]
%   $V$: Choose a random $r \in \{0, 1\}$\;
%   \eIf{$r = 0$}{
%     $V$: Perform the low multidegree test from \cref{thm:quantum-low-degree}\;
%   }{
%     \tcp{IPCP emulation}
%     $P_{1}$ and $V$ emulate the interaction of the IPCP $(P'', V'') = T(P', V')$
%     (see \cref{thm:ipcp-one-query})\;
%     The above emulation generates a uniform $\beta \in \mathbb{F}^{m}$, and a
%     $c \in \mathbb{F}$ such that with probability $1 - \varepsilon$, $x \in L$ if and only if
%     $\mathcal{O}(\beta) = c$\;
%     $V$: send $\beta$ to $P_{2}$\;
%     $P_{2}$: reply with $z = \mathcal{O}(\beta)$\;
%     \eIf{$c = z$}{
%       \Accept\;
%     }{
%       \Reject\;
%     }
%   }
%   \caption{Construction of a $\MIP*$ from an IPCP~\cite[Construction 2]{CFGS22}}\label{alg:mip-from-ipcp}
% \end{algorithm}

% In order to prove this theorem, we break it down in to two sections: the first
% deals with the correctness of the algorithm, and the second demonstrates that it
% is zero-knowledge.

% \subsection{Correctness of \cref{alg:mip-from-ipcp}}\label{sec:mip-ipcp-correct}

% \begin{lemma}\label{lem:mip-ipcp-correct}
%   For any IPCP $(P', V')$, \cref{alg:mip-from-ipcp} is a multi-prover
%   interactive proof for the same language as $(P', V')$.
% \end{lemma}

% \begin{proof}
%   First, we show \cref{alg:mip-from-ipcp} runs in polynomial time. If $r = 0$,
%   then \cref{thm:quantum-low-degree} tells us that the low-multidegree test runs
%   in polynomial time. If $r = 1$, then we know $V''$ runs in polynomial time by
%   \cref{thm:ipcp-one-query}; as does sending $\beta$ and checking if $c = z$. Hence,
%   the entire algorithm runs in polynomial time.

%   Next, we show if $x \in L$, then $(P_{1}, P_{2}, V)$ will accept with
%   probability at lest $2/3$. If $r = 0$ then \TODO{}. If $r = 1$, then we know
%   that the IPCP emulation will always succeed (from the definition of an IPCP).
%   Further, since $P_{2}$ is honest it will respond with $z = R(\beta) = c$ and thus
%   the algorithm will accept.

%   We now show that for any $x \notin L$ and any provers $\hat{P}_{1}$ and
%   $\hat{P}_{2}$, \cref{alg:mip-from-ipcp} will reject with probability at least
%   $2/3$. We will show this by contradiction: assume
%   $(\tilde{P}_{1}, \tilde{P}_{2})$ are provers that succeed with probability at
%   least $1 - \delta/2$, for some small $\delta = 1/\poly(md)$.

%   In the case where $r = 0$, since $V$ (by assumption) accepts with probability
%   $1 - \delta/2$ overall, and since $r = 0$ exactly half of the time, it follows that
%   the low-degree test must accept with probability at least $1 - \delta$. When
%   performing this strategy, $P_{1}$ and $P_{2}$ make a measurement of their
%   entangled state $\ket{\psi}$; define
%   $\{A_{1,\alpha}^{z}\}_{z \in \mathbb{F},\alpha \in \mathbb{F}^{m}}$ and
%   $\{A_{2,\alpha}^{Z}\}_{z \in \mathbb{F},\alpha \in \mathbb{F}^{m}}$ to be the measurements
%   made by $\tilde{P}_{1}$ and $\tilde{P}_{2}$, respectively, when performing
%   this test.

%   \TODO{From \cref{thm:quantum-low-degree}, we construct low-degree measurement
%     from earlier measurements; this will fool the low-degree test with high
%     probability and since it is within $v$ of the right answer this will result
%     in contradiction of the IPCP's soundness}

%   With these low-multidegree measurements, our plan is to construct a
%   $\tilde{P}_{2'}$ such that $\tilde{P}_{2'}$ uses only low-multidegree
%   measurements and further that $\ang{\tilde{P}_{1}, \tilde{P}_{2'}, V}(x)$ will
%   still accept with probability reasonably close to
%   $\ang{\tilde{P}_{1}, \tilde{P}_{2}, V}(x)$; this will allow us to construct a
%   malicious verifier for the IPCP that contradicts its soundness guarantees.

%   \TODO{Setup, define low-degree measurements}

%   \begin{align}
%     \sigma_{B} &= \tr_{\mathcal{B}}\mleft[
%               \underset{\alpha \in \mathbb{F}^{m}}{\mathbb{E}}\sum_{z \in \mathbb{F}}
%               \ket{\alpha}\bra{\alpha}^{\mathcal{A}} \otimes (A_{2,\alpha}^{z} \otimes \Id_{\mathcal{C}})\sigma(A_{2,\alpha}^{z} \otimes \Id_{\mathcal{C}}) \otimes \ket{z}\bra{z}^{\mathcal{D}}
%             \mright]\label{eqn:sigma-b} \\
%     \sigma_{T} &= \tr_{\mathcal{B}}\mleft[
%               \underset{\alpha \in \mathbb{F}^{m}}{\mathbb{E}}\sum_{z \in \mathbb{F}}
%               \ket{\alpha}\bra{\alpha}^{\mathcal{A}} \otimes (L_{\alpha}^{z} \otimes \Id_{\mathcal{C}})\sigma(L_{\alpha}^{z} \otimes \Id_{\mathcal{C}}) \otimes \ket{z}\bra{z}^{\mathcal{D}}
%             \mright]\label{eqn:sigma-t}
%   \end{align}

%   \TODO{}

%   \TODO{Does all this algebraic shenaniganry belong in an appendix?}
%   By the definition of $\sigma_{B}$ and $\sigma_{T}$, we have
%   \begin{equation}\label{eqn:prob-to-sigma}
%     \abs*{\mathbb{P}[\ang{\tilde{P}_{1}, \tilde{P}_{2}, V} = 1] - \mathbb{P}[\ang{\tilde{P}_{1}, \tilde{P}_{2'}, V} = 1]}
%     \le \frac{1}{2}\norm{\sigma_{B} - \sigma_{T}}_{1}.
%   \end{equation}
%   Substituting in the values from \cref{eqn:sigma-b,eqn:sigma-t}, we get
%   \FIXME{This equation is way too long}
%   \begin{multline}\label{eqn:sigma-add-eqns}
%     \frac{1}{2}\norm{\sigma_{B} - \sigma_{T}} = \\
%     \frac{1}{2}\norm*{
%       \tr_{\mathcal{B}}\underset{\alpha \in \mathbb{F}^{m}}{\mathbb{E}}\sum_{z \in \mathbb{F}}
%       \ket{\alpha}\bra{\alpha}^{\mathcal{A}} \otimes \mleft(
%         (A_{2,\alpha}^{z} \otimes \Id_{\mathcal{C}})\sigma(A_{2,\alpha}^{z} \otimes \Id_{\mathcal{C}}) -
%         (L_{\alpha}^{z} \otimes \Id_{\mathcal{C}})\sigma(L_{\alpha}^{z} \otimes \Id_{\mathcal{C}})
%       \mright) \otimes \ket{z}\bra{z}^{\mathcal{D}}
%     }_{1}.
%   \end{multline}
%   Because \TODO{Why?}, \cref{eqn:sigma-add-eqns} simplifies to
%   \begin{equation}
%     \frac{1}{2}\norm{\sigma_{B} - \sigma_{T}} \le
%     \frac{1}{2}\underset{\alpha \in \mathbb{F}^{m}}{\mathbb{E}}\norm*{\sum_{z \in \mathbb{F}}
%         (A_{2,\alpha}^{z} \otimes \Id_{\mathcal{C}})\sigma(A_{2,\alpha}^{z} \otimes \Id_{\mathcal{C}}) -
%         (L_{\alpha}^{z} \otimes \Id_{\mathcal{C}})\sigma(L_{\alpha}^{z} \otimes \Id_{\mathcal{C}})
%         \otimes \ket{z}\bra{z}
%     }_{1}.
%   \end{equation}
%   We can apply \cref{lem:gentle-measurement} to \cref{eqn:sigma-add-eqns} to get
%   us
%   \begin{equation}
%     \frac{1}{2}\norm{\sigma_{B} - \sigma_{T}} \le
%     \underset{\alpha \in \mathbb{F}^{m}}{\mathbb{E}}\sqrt{
%       \sum_{z \in \mathbb{F}}\tr\mleft((A_{2,\alpha}^{z} - L_{\alpha}^{z})\rho(A_{2,\alpha}^{z} - L_{\alpha}^{z})^{\dagger}\mright)
%     }.
%   \end{equation}
%   The square root function is concave down; hence we can apply Jensen's
%   inequality (\cref{thm:jensen-inequality}), giving us
%   \begin{equation}
%     \underset{\alpha \in \mathbb{F}^{m}}{\mathbb{E}}\sqrt{
%       \sum_{z \in \mathbb{F}}\tr\mleft((A_{2,\alpha}^{z} - L_{\alpha}^{z})\rho(A_{2,\alpha}^{z} - L_{\alpha}^{z})^{\dagger}\mright)
%     } \le
%     \underset{\alpha \in \mathbb{F}^{m}}{\mathbb{E}}\sqrt{
%       \sum_{z \in \mathbb{F}}\tr\mleft((A_{2,\alpha}^{z} - L_{\alpha}^{z})^{2}\mright)
%     }.
%   \end{equation}
%   Finally, \TODO{Reference CFGS22's equation (3) and claim 1} informs us
%   \begin{equation}\label{eqn:less-than-v}
%     \underset{\alpha \in \mathbb{F}^{m}}{\mathbb{E}}\sqrt{
%       \sum_{z \in \mathbb{F}}\tr\mleft((A_{2,\alpha}^{z} - L_{\alpha}^{z})^{2}\mright)
%     } \le v.
%   \end{equation}
%   Combining \crefrange{eqn:prob-to-sigma}{eqn:less-than-v} tells us that
%   \begin{equation}
%     \abs*{\mathbb{P}[\ang{\tilde{P}_{1}, \tilde{P}_{2}, V} = 1] - \mathbb{P}[\ang{\tilde{P}_{1}, \tilde{P}_{2'}, V} = 1]} \le v.
%   \end{equation}
%   In other words, the difference in the probability of acceptance when we use
%   our low-degree $\tilde{P}_{2'}$ instead of $\tilde{P}_{2}$ is no more than
%   $v$. Since we already said we are choosing $\delta$ small enough such that
%   $v < \frac{1}{2}$, this means that \TODO{Construct $\tilde{P}''$}
% \end{proof}

% \subsection{\Cref{alg:mip-from-ipcp} preserves
%   zero-knowledge}\label{sec:mip-ipcp-pzk}

% \begin{algorithm}[htbp]
%   Simulate $\tilde{V}$ and respond to its queries as follows\;
%   \uIf{$\tilde{V}$ simulates an IPCP interaction with $P_{1}$}{
%     Run the IPCP simulator for $(P'', \tilde{V})$\;
%   }
%   \uElseIf{$\tilde{V}$ sends a point $\alpha \in \mathbb{F}^{m}$ to either $P_{i}$}{
%     Query $\mathcal{O}$ at $\alpha$\;
%     Respond to $\tilde{V}$ with $\mathcal{O}(\alpha)$\;
%   }
%   \uElseIf{$\tilde{V}$ sends a line $\ell$ to either $P_{i}$}{
%     \For{$i \in [md + 1]$}{
%       Pick a random $p_{i} \in \ell$\;
%       Query $\mathcal{O}$ at $p_{i}$\;
%     }
%     Use the $p_{i}$ to interpolate $R \circ \ell$\;
%     Respond to $\tilde{V}$ with $R \circ \ell$\;
%   }
%   \Else{
%     \Reject\;
%   }
%   \KwRet{The view of $\tilde{V}$}\;
%   \caption{An IPCP verifier $\tilde{V}'$ corresponding to any malicious MIP*
%     verifier $\tilde{V}$~\cite[\defaultS
%     9.4]{CFGS22}}\label{alg:ipcp-corresp-ver}
% \end{algorithm}

% \begin{lemma}\label{lem:sim-mip-works}
%   There exists a simulator for \cref{alg:mip-from-ipcp}.
% \end{lemma}

% \begin{proof}
%   Instead of constructing a simulator directly, we instead construct an IPCP
%   verifier $\tilde{V}'$ that corresponds to any given malicious MIP* verifier
%   $\tilde{V}$ in \cref{alg:ipcp-corresp-ver}. Because we assume the underlying
%   IPCP can be simulated, it follows that if $\tilde{V}'$ outputs the same view
%   as $\tilde{V}$, then we can run the IPCP simulator on $\tilde{V}'$ and the
%   output of this will be a proper simulation for $\tilde{V}$.

%   We prove that $\tilde{V}'$ returns an identical view to $\tilde{V}$ for each
%   of the cases in \cref{alg:ipcp-corresp-ver}. In the case where $\tilde{V}$
%   simulates an IPCP interaction with $P_{1}$, we simply run the IPCP simulator
%   for $(P'', \tilde{V})$, which exists because $(P'', V'')$ is a perfect
%   zero-knowledge IPCP\@. By the definition of a simulator, this runs in
%   polynomial time and outputs an identically-distributed view to
%   $(P'', \tilde{V})$.

%   In the case where $\tilde{V}$ sends a point $\alpha \in \mathbb{F}^{m}$, then in
%   \cref{alg:mip-from-ipcp}, we know that $P_{2}$ will respond with $z = \mathcal{O}(\beta)$.
%   Hence, since we still have access to the oracle $\mathcal{O}$ in the IPCP model, we can
%   query the oracle directly, giving us the response from $P_{2}$.

%   In the case where $\tilde{V}$ sends a line $\ell$, \TODO{}.

%   If $\tilde{V}$ sends anything outside the above cases, it is immediately
%   obvious to the honest provers that this verifier is malicious and thus the
%   provers will abort and refuse to answer further.

%   Since \cref{alg:ipcp-corresp-ver} can reconstruct the view of the verifier in
%   \cref{alg:mip-from-ipcp}, its simulator can therefore reconstruct the view of
%   \cref{alg:mip-from-ipcp} as well. Hence, \cref{alg:mip-from-ipcp} is perfect
%   zero-knowledge.
% \end{proof}

\chapter{Low-degree zero-knowledge IPCPs}\label{chap:ipcp-zero-knowledge}

Now that we can construct a zero-knowledge $\MIP*$ instance from a
zero-knowledge IPCP, all that remains is to show that $\NEXP \subseteq \PZKIPCP$. From
there, we will be able to leverage~\cite[Lemma 9.1]{CFGS22} to demonstrate
$\NEXP \subseteq \PZKMIP*$.

\section{AQC of polynomial summation}\label{sec:aqc-poly-sum}

We first need to define the \emph{polynomial summation problem}. We will want a
lower bound on the algebraic query complexity of this problem, similar to the
examples we saw in \cref{sec:alg-query-complexity}.

\begin{defn}\label{def:poly-sum}\index{polynomial summation problem}
  The \emph{polynomial summation problem} is the following:
  \begin{quote}
    Let $\mathbb{F}$ be a field with $G \subseteq \mathbb{F}$. Let $m, k, d, d' \in \mathbb{N}$ and
    let $Z \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d'}]$.
    What is the value of the polynomial
    \[
      R(X) = \sum_{\beta \in G^{k}}Z(X, \beta) \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]?
    \]
  \end{quote}
\end{defn}

We will need a lower bound on the algebraic query complexity of this problem
(where $Z$ functions as the oracle) later on in order to help demonstrate
zero-knowledge. We will do this with one additional restriction---we will also
need $d'$ to be sufficiently large relative to $G$, but this will not hamper us
in practice. In brief, the lower bound will tell us that so long as we limit
our total queries, we will not receive \emph{any} information about $R(X)$.

\begin{lemma}[{\cite[Lemma 12.1]{CFGS22}}]\label{lem:sum-c-z}
  Let $\mathbb{F}$ be a field, $m, k, d, d' \in \mathbb{N}$, and $G, K, L$ be finite
  subsets of $\mathbb{F}$ such that $K \subseteq L$, $d' \ge \abs{G} - 2$, and
  $\abs{K} = d + 1$. If $S \subseteq \mathbb{F}^{m+k}$ is such that there exist matrices
  $C \in M_{L^{m},\ell}(\mathbb{F})$ and $D \in M_{S, \ell}(\mathbb{F})$ such that for
  all $Z \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d'}]$ and all
  $i \in \{1, \ldots, \ell\}$
  \begin{equation}\label{eqn:polynomial-sum}
    \sum_{\alpha \in L^{m}}C_{\alpha,i}\sum_{y \in G^{k}}Z(\alpha, y) = \sum_{q \in S}D_{q,i}Z(q),
  \end{equation}
  then $\abs{S} \ge \rk(BC)(\min(d' - \abs{G} + 2, \abs{G}))^{k}$, where
  $B \in M_{K^{m}, L^{m}}(\mathbb{F})$ is such that the column of $B$ indexed by
  $\alpha$ represents $Z(\alpha)$ in the basis $\{Z(\beta) \mid \beta \in K^{m}\}$.
\end{lemma}

\begin{proof}
  First, if $d' = \abs{G} - 2$, then $d' - \abs{G} + 2 = 0$; hence our bound
  simplifies to
  \begin{align*}
    \abs{S} &\ge \rk(BC)\min(0, \abs{G})^{k} \\
            &\ge 0\rk(BC) \\
            &\ge 0,
  \end{align*}
  which is true regardless of $S$.

  Otherwise, we can rewrite the left-hand side of \cref{eqn:polynomial-sum} as
  follows:
  \begin{equation}\label{eqn:poly-sum-constant}
    \sum_{\alpha \in L^{m}}C_{\alpha,i}\sum_{y \in G^{k}}Z(\alpha,y) = \sum_{\alpha \in L^{m}}C_{m,i}\sum_{\beta \in K^{m}}b_{\beta,\alpha}\sum_{y \in G^{k}}Z(\beta,y).
  \end{equation}
  Then, define $B \in M_{K^{m},L^{m}}(\mathbb{F})$ to be the matrix whose
  $(i,j)$-entry is $\beta_{i,j}$, and define $C' = BC \in M_{K^{m},\ell}(\mathbb{F})$.
  From that, \cref{eqn:poly-sum-constant} simplifies to
  \begin{equation}\label{eqn:poly-sum-prime}
    \sum_{\alpha \in L^{m}}C_{m,i}\sum_{\beta \in K^{m}}b_{\beta,\alpha}\sum_{y \in G^{k}}Z(\beta,y) = \sum_{\beta \in K^{m}}C'_{\beta,i}\sum_{y \in G^{k}}Z(\beta,y).
  \end{equation}
  Next, define $H \subseteq G$ such that $\abs{H} = \min(d' - \abs{G} + 2, \abs{G})$.
  Further, let
  \[
    P_{0} = \{p \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le \abs{H} - 1}]
      \mid p(q) = 0 \text{ for all } q \in S\}.
  \]
  We can write $P_{0}$ as the kernel of the linear function
  $F_{S}\colon \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le \abs{H} - 1}] \rightarrow \mathbb{F}^{S}$
  defined by $(F_{S}(p))_{s} = p(s)$. By the rank-nullity theorem, this means
  \[
    \dim(P_{0}) \ge (d+1)^{m}\abs{H}^{m} - \abs{S}.
  \]

  Let $A_{0} \in M_{n,(K^{m} \times H^{k})}(\mathbb{F})$ (for some arbitrary $n$) be a
  matrix whose rows form a basis for
  \[
    \hat{P}_{0} = \{(p(\alpha,y))_{\alpha \in K^{m},y \in H^{k}} \mid p \in P_{0}\}.
  \]
  The dimension of $\hat{P}_{0}$ is exactly the dimension of $P_{0}$; since two
  distinct polynomials of degree $n$ can agree at no more than $n$ points,
  polynomials in $P$ have maximum degree $d^{m}(\abs{H}-1)^{k}$, and each vector
  in $\hat{P}_{0}$ has $\abs{K^{m} \times H^{k}} = (d+1)^{m}\abs{H}^{k}$ points, it
  follows that the map from $p$ to its corresponding vector in $\hat{P}_{0}$ is
  a bijection and thus dimension is preserved.

  For any $y_{0} \in H^{k}$, let $A_{y_{0}} \in M_{n,K^{m}}(\mathbb{F})$ be the
  submatrix of $A_{0}$ consisting only of rows where $y = y_{0}$.
  Since $A_{0}$'s columns form a basis, it has full rank. Further, the set of
  all $A_{y_{0}}$s span the row space of $A_{0}$; hence
  \[
    n = \rk(A_{0}) \le \sum_{y_{0} \in H^{k}}\rk(A_{y_{0}}).
  \]
  Hence, there exists a $y_{0}$ such that
  \[
    \dim(A_{y_{0}}) \ge \frac{\rk(A_{m})}{\abs{H^{k}}}
    \ge \frac{(d+1)^{m}\abs{H^{k}} - \abs{S}}{\abs{H^{k}}} = (d+1)^{m} - \frac{\abs{S}}{\abs{H^{k}}}.
  \]

  Let $q \in \mathbb{F}[Y_{1, \ldots, k}^{\ge \abs{G} - 1}]$ be the polynomial such that
  $q(y_{0}) = 1$ and $q(y) = 0$ for all $y \in G^{k} \setminus \{y_{0}\}$ (as per
  \cref{thm:low-deg-ext-exists}). Then, for all $i \in \{1, \ldots, n\}$ and
  $j \in \{1, \ldots, k\}$ it holds that
  \begin{equation}
    (A_{y_{0}}C')_{ij} = \sum_{\beta \in K^{m}}C'_{\beta,j}p_{i}(\beta, y_{0})
  \end{equation}
  where $p_{i}(\alpha, \beta)$ is the element of $A_{0}$ in row $i$ and column $(\alpha,\beta)$.
  This comes from the definition of matrix multiplication and $A_{y_{0}}$. Next,
  the definition of $q$ gives us
  \begin{equation}
    \sum_{\beta \in K^{m}}C'_{\beta,j}p_{i}(\beta, y_{0}) = \sum_{\beta \in K^{m}}C'_{\beta,j}\sum_{y \in G^{k}}q(y)p_{i}(\beta,y).
  \end{equation}
  Now, since $p_{i}$ is a row of $A_{0}$ and the rows of $A_{0}$ form a basis
  for the space of all outputs of polynomials in $P_{0}$, we can simply treat
  $p_{i}$ as a polynomial in $P_{0}$.

  Note that $qp_{i} \in \mathbb{F}[X_{1, \ldots, k}^{\ge d}, Y_{1, \ldots, k}^{\ge d'}]$ (from
  the definitions of $q$ and $p_{i}$). Hence, \cref{eqn:poly-sum-prime} tells us
  that
  \begin{equation}\label{eqn:sum-qpi}
    \sum_{\beta \in K^{m}}C'_{\beta,j}\sum_{y \in G^{k}}q(y)p_{i}(\beta,y) = \sum_{s \in S}D_{s,i}(qp_{i})(s).
  \end{equation}
  Since $s \in Q$, the definition of $P_{0}$ tells us that $p_{i}(s) = 0$; hence
  the entire sum in \cref{eqn:sum-qpi} is equal to zero and thus
  $A_{y_{0}}C' = 0$.

  Lastly, we can apply Sylvester's rank inequality:
  \begin{align*}
    \rk(A_{y_{0}}) + \rk(C') - (d+1)^{m} &\le \rk(0) \\
    (d+1)^{m} - \rk(C') &\ge \rk(A_{y_{0}}) \\
    (d+1)^{m} - \rk(C') &\ge (d+1)^{m} - \abs{S}/\abs{H}^{k} \\
    \abs{S} &\ge \rk(C')\abs{H}^{k}.
  \end{align*}
  From the definition of $H$, this means
  \[
    \abs{S} \ge \rk(C')(\min(d' - \abs{G} + 2, \abs{G}))^{k},
  \]
  as desired.
\end{proof}

\begin{cor}[{\cite[Corollary 12.2]{CFGS22}}]\label{cor:sum-c-z}
  Let $\mathbb{F}$ be a finite field, $G \subseteq \mathbb{F}$, and $d, d' \in \mathbb{N}$ with
  $d' \ge 2(\abs{G} - 1)$. If $S \subseteq \mathbb{F}^{m+k}$ is such that there exist
  $(c_{\alpha})_{\alpha \in \mathbb{F}^{m}}$ and $(d_{\beta})_{\beta \in \mathbb{F}^{m+k}}$ such that
  \begin{enumerate}
    \item for all $Z \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d}]$ it
          holds that
          \begin{equation}\label{eqn:sum-c-z-d-z}
            \sum_{\alpha \in \mathbb{F}^{m}}c_{\alpha}\sum_{y \in G^{k}}Z(\alpha, y) = \sum_{q \in S}d_{q}Z(q),
          \end{equation}
          and
    \item there exists $Z' \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d}]$
          such that
          \begin{equation}\label{eqn:sum-c-z-zero}
            \sum_{\alpha \in \mathbb{F}^{m}}c_{\alpha}\sum_{y \in G^{k}}Z'(\alpha, y) = 0,
          \end{equation}
  \end{enumerate}
  then $\abs{S} \ge \abs{G}^{k}$.
\end{cor}

From here, we get that the algebraic query complexity of polynomial summation is
at least $\abs{G}^{k}$. That is, if we query $Z$ no more than $\abs{G}^{k}$
times, then we we will receive \emph{no information} about the polynomial
$R(X) = \sum_{\beta \in G^{k}}Z(X, \beta)$.

\begin{cor}[{\cite[Corollary 12.3]{CFGS22}}]\label{cor:indep-ensemble}
  Let $\mathbb{F}$ be a finite field, $G \subseteq \mathbb{F}$, and $d, d' \in \mathbb{N}$ with
  $d' \ge 2(\abs{G} - 1)$. Let $Q$ be a subset of $\mathbb{F}^{m+k}$ with
  $\abs{Q} \le \abs{G}^{k}$, and let $Z$ be uniformly random in
  $\mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d'}]$. Then, the random
  variables $(\sum_{y \in G^{k}}Z(\alpha, y))_{\alpha \in \mathbb{F}^{m}}$ and $(Z(q))_{q \in Q}$
  are independent.
\end{cor}

\begin{proof}
  We will leverage \cref{thm:lin-indep-stat-indep} that we proved earlier. Now,
  consider the vector space
  \begin{equation}
    V = \mleft\{
      \mleft(
        (Z(\gamma))_{\gamma \in \mathbb{F}^{m+k}}, \mleft(\sum_{y \in G^{k}}Z(\alpha, y)\mright)_{\alpha \in \mathbb{F}^{m}}
      \mright)
      \middlemid
      Z \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le d'}]
    \mright\}
  \end{equation}
  This is a vector space over $\mathbb{F}$ with basis
  $\{e_{i} \mid i \in \mathbb{F}^{m+k} \sqcup \mathbb{F}^{m}\}$.

  Consider the subsets indexed by $\mathbb{F}^{m}$ and $Q \subseteq \mathbb{F}^{m+k}$.
  As per \cref{thm:lin-indep-stat-indep}, we want to show there does not exist a
  $c \in V|_{\mathbb{F}^{m}}$ and $d \in V|_{Q}$ such that for all $w \in V$,
  $c \cdot w|_{\mathbb{F}^{m}} = d \cdot w|_{Q}$ and for some $w \in V$,
  $c \cdot w|_{\mathbb{F}^{m}} = 0$. Expanding these equations out given our
  definition of $V$, we get exactly \cref{eqn:sum-c-z-d-z,eqn:sum-c-z-zero}.
  However, \cref{cor:sum-c-z} tells us that this can only be true if
  $\abs{Q} > \abs{G}^{k}$. But we assumed $\abs{Q} \le \abs{G}^{k}$, so there
  cannot exist any such $c$ or $d$. Hence, these two random variables are
  statistically independent.
\end{proof}

% TODO: Additional theorem that links this to algebraic query complexity somehow

% \section{Algebraic commitment schemes}\label{sec:alg-commit-scheme}

% We gave an explanation of bit-commitment schemes in
% \cref{sec:commitment-scheme}, but there are many circumstances in which
% commitment to a single bit is insufficient. In some cases, we will want to
% commit to an entire polynomial. This is the setting in which \emph{algebraic
%   commitment schemes} exist.

% \begin{defn}\label{def:alg-comm-scheme}\index{commitment scheme!algebraic}
%   An \emph{algebraic commitment scheme} is a commitment scheme such that instead
%   of $S$ receiving a single bit it receives a polynomial
%   $Q\colon \mathbb{F}^{m} \rightarrow \mathbb{F}$.
% \end{defn}

% \TODO{Mention the role of algebraic query complexity
%   (\cref{sec:alg-query-complexity}) in commitment schemes}

% \begin{algorithm}[htbp]
%   \KwIn{A polynomial $Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d_{Q}}]$}
%   Let $K \subseteq \mathbb{F}$ with $\abs{K} = d + 1$\;
%   \For{$\alpha \in K^{m}$}{
%     sample a random $B^{\alpha} \in \mathbb{F}^{n}$ such that
%     $\sum_{i=1}^{m}B^{\alpha}_{i} = Q(\alpha)$\;
%   }
%   Define $B\colon K^{m} \times G^{k} \rightarrow \mathbb{F}$ by $B(\alpha, x) = B^{\alpha}(x)$\;
%   Define $\hat{B}\colon \mathbb{F}^{m} \times \mathbb{F}^{k} \rightarrow \mathbb{F}$ to be a
%   low-degree extension of
%   $B$\tcc*{Note: $P \in \mathbb{F}[X_{1, \ldots, m}^{\le d_{Q}}, Y_{1, \ldots, m}^{\le d}]$}
%   \KwRet{$\hat{B}$}\;
%   \caption{An algebraic commitment scheme~\cite[\defaultS
%     12]{CFGS22}}\label{alg:alg-commit-scheme}
% \end{algorithm}

% We would next like to give an example of an algebraic commitment scheme. \TODO{}

% \begin{thm}[{\cite[\defaultS 12]{CFGS22}}]\label{thm:alg-commit-scheme}
%   \Cref{alg:alg-commit-scheme} is a valid algebraic commitment scheme.
% \end{thm}

% \begin{proof}
%   To prove this, we first show that it is binding, and then that it is hiding.
%   We know that the sumcheck protocol we discussed in \cref{thm:sumcheck-ip}
%   \TODO{}
% \end{proof}

\section{The sumcheck problem}

Central to our upcoming work will be a nice answer to the \emph{sumcheck
  problem}, a computational problem about verifying whether or not the sum of a
polynomial over some subset of a field is equal to a provided value. This will
prove useful to us in arithmetizing our problems: if we can turn our boolean
formulae into a question in the sumcheck format, then we can delegate to the
protocol to complete our proof.

\begin{defn}[{\cite{LFKN92}}]\index{sumcheck problem}\label{def:sumcheck}
  The \emph{sumcheck problem} is the following problem:
  \begin{quote}
    Let $H$ be a subset of a finite field $\mathbb{F}$, let
    $F \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ a polynomial over $\mathbb{F}$, and let
    $a \in \mathbb{F}$. Does $\sum_{x \in H^{m}}F(x) = a$?
  \end{quote}
  For this question, we give $H$ and $a$ to both the prover and verifier, but
  only the prover gets access to $F$ as an algebraic oracle.
\end{defn}

\subsection{A non-zero-knowledge sumcheck protocol}

We begin with an interactive protocol for sumcheck that does not have any
zero-knowledge characteristics. While this protocol itself does not preserve
anything, it will form an essential building block for the zero-knowledge
protocols we will construct later.

\begin{algorithm}[htbp]
  \KwIn{A polynomial $F \in \mathbb{F}[X_{1, \ldots, n}^{\le d}]$, subset $H \subseteq \mathbb{F}$, and number $a$}
  \KwOut{Whether $\sum_{x \in H^{m}}F(x) = a$}
  $P$: send the polynomial $F_{1}(x_{1}) = \sum_{X \in \{0, 1\}^{n-1}}F(x_{1}, X)$\;
  $V$: check if $a = \sum_{x \in H}F_{1}(x)$\;\nllabel{line:init-sumcheck}
  $V$: choose random $r_{1} \in \mathbb{F}$ and send to $P$\;
  \For{$i$ from $2$ to $n$}{
    $P$: send the polynomial
    \[
      F_{i}(x_{i}) = \sum_{X \in H^{m-i}}F(r_{1}, \ldots, r_{i-1}, x_{i}, X)
    \]
    to $V$\;\nllabel{line:equation-sent}
    $V$: check $F_{i-1}(r_{i-1}) = \sum_{x \in H}F_{i}(x)$\;\nllabel{line:seq-sumcheck}
    \If{$i \ne n$}{
      $V$: choose random $r_{i} \in \mathbb{F}$ and send to $P$\;
    }
  }
  $V$: choose random $r_{n} \in \mathbb{F}$\;
  $V$: check $F_{n}(r_{n}) = F(r_{1}, \ldots, r_{n})$\;\nllabel{line:final-check}
  \caption{The standard sumcheck protocol~\cite[Thm.\ 1]{LFKN92}}\label{alg:sumcheck-std}
\end{algorithm}

\begin{thm}\label{thm:sumcheck-ip}
  The sumcheck problem is in $\IP$.
\end{thm}

\begin{proof}
  We show this by implementing an interactive protocol for sumcheck in
  \cref{alg:sumcheck-std}. We need to start by showing that this algorithm will
  correctly answer the sumcheck question. First, if $\sum_{x \in H^{m}}F(x) = a$ and
  $P$ is honest, then $V$s check in line~\ref{line:init-sumcheck} will succeed
  if and only if the question is correct. Again assuming an honest $P$, from the
  definition of each $F_{i}$,
  \[
    F_{i-1}(r_{i-1}) = \sum_{X \in H^{m-i+1}}F(r_{1}, \ldots, r_{i-2}, r_{i-1}, X)
  \]
  and
  \[
    \sum_{x \in H}\sum_{x \in H^{m-i}}F(r_{1}, \ldots, r_{i-1}, x, X) = \sum_{X \in H^{m-i+1}}F(r_{1}, \ldots, r_{i-1}, X).
  \]
  Hence, line~\ref{line:seq-sumcheck} will always succeed with an honest $P$.
  Finally, if $P$ has been honest in the last iteration,
  line~\ref{line:final-check} will always succeed since the sum in the equation
  in line~\ref{line:equation-sent} is now over a single object.

  If $P$ is dishonest, then we show soundness by induction on $n$. If $n = 1$
  then there is only one message sent. Two distinct $n$-variable polynomials of
  degree $d$ can be equal at most $d^{n}$ points, as such in the $n = 1$ case
  the probability of incorrectly passing the check in
  line~\ref{line:final-check} is at most $d/\abs{\mathbb{F}}$.

  Next, assume the $(n - 1)$-variable case has soundness error at most
  $(n - 1)d/\abs{\mathbb{F}}$. Let
  \[
    G_{1}(x_{1}) = \sum_{X \in H^{n-1}}F(x_{1}, X),
  \]
  i.e.\ the ``correct'' value of $F_{1}$ (were $P$ not to lie). If
  $F_{1} \ne G_{1}$, then as we saw before, $F_{1}(r_{1}) \ne G_{1}(r_{1})$ with
  probability $1 - d/\abs{\mathbb{F}}$. If this is the case, then in the rest of
  the loop, the system is attempting to prove the claim
  \[
    F_{1}(r_{1}) = \sum_{X \in H^{n-1}}F(r_{1}, X),
  \]
  which we know to be false. $F(r_{1}, \cdot)$ is an $(n - 1)$-variable polynomial
  of multidegree $d$; by induction this has soundness error
  $(r - 1)d/\abs{\mathbb{F}}$. Thus, $V$ will reject with probability
  \[
    1 - \mathbb{P}[F_{1}(r_{1}) \ne G_{1}(r_{1})] - \mathbb{P}[V \text{ rejects in round } j > 1 \mid F_{1}(r_{1}) \ne G_{1}(r_{1})]
  \]
  which, by substituting in our definitions, gives us probability at least
  \[
    1 - \frac{d}{\abs{\mathbb{F}}} - \frac{d(n - 1)}{\mathbb{F}} = 1 - \frac{dn}{\abs{\mathbb{F}}}.
  \]

  The only other step is to show that $V$ runs in polynomial time. Since each
  loop iteration only requires checking a sum over $H$, we only need to compute
  the various $F_{i}$s (which are themselves only polynomially larger than the
  original $F$) a total of $nH$ times; computing $F_{i}$ is also in polynomial
  time. Thus, $V$ overall runs in polynomial time.
\end{proof}

\subsection{Making the sumcheck protocol zero-knowledge}

\begin{thm}[{\cite[Theorem 13.3]{CFGS22}}]\index{zero-knowledge proof!for sumcheck}\label{thm:zk-sumcheck}
  There exists a zero-knowledge variant of \cref{alg:sumcheck-std}.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{An instance $(H, a)$ to both $P$ and $V$}
  \KwIn{A polynomial $F \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ as an oracle to $P$}
  \KwOut{Whether $\sum_{x \in H^{m}}F(x) = a$}
  $P$: draw random
  $Z \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, m}^{\le 2\lambda}]$\;\nllabel{line:p-pick-z}
  $P$: draw random $A \in \mathbb{F}[Y_{1, \ldots, k}^{\le 2\lambda}]$\;
  $P$: send the polynomial
  \[
    O(W, X, Y) = W \cdot Z(X, Y) + (1 - W) \cdot A(Y)
  \]
  to $V$\;
  \tcp{Note that $Z(x) = O(1, x)$ and $A(x) = O(0, 0, x)$, so $V$ can use both
    $Z$ and $A$ later}
  $P$: send $z = \sum_{\alpha \in H^{m}}\sum_{\beta \in G^{k}}Z(\alpha, \beta)$ to $V$\;\nllabel{line:send-z}
  $V$: draw random $\rho_{1} \in \mathbb{F}^{\times}$\;
  $V$: send $\rho_{1}$ to $P$\;
  Run the standard sumcheck IP (\cref{alg:sumcheck-std}) on the statement
  $\sum_{\alpha \in H^{m}}Q(\alpha) = \rho_{1}a + z$, where
  \[
    Q(X_{1}, \ldots, X_{m}) = \rho_{1}F(X_{1}, \ldots, X_{m}) + \sum_{\beta \in G^{k}}Z(X_{1}, \ldots, X_{m}, \beta).
  \]
  We have $P$ play the prover and $V$ the verifier, with the following
  modification: For $i = 1, \ldots, m$, in the $i$th round, $V$ samples its random
  element $r_{i}$ from the set $I$ instead of from all of $\mathbb{F}$; if $P$
  ever receives $r_{i} \in \mathbb{F} \setminus I$, it immediately aborts. In particular,
  in the $m$th (i.e., the final) round, $P$ sends a polynomial
  \[
    g_{m}(X_{m}) = \rho_{1}F(c_{1}, \ldots, c_{m-1}, X_{m}) + \sum_{\beta \in G^{k}}Z(c_{1}, \ldots, c_{m-1}, X_{m}, \beta)
  \]
  for some $c_{1}, \ldots, c_{m-1} \in I$\;\nllabel{line:sumcheck-1}
  $V$: send $c_{m} \in I$ to $P$\;
  $P$: send $w = \sum_{\beta \in G^{k}}Z(c, \beta)$ to $V$, where $c = (c_{1}, \ldots, c_{m})$\;
  % NOTE: This inlined alg:sumcheck-wzk so we aren't referring to that anymore
  % Both: engage in the weak-ZK sumcheck protocol (\cref{alg:sumcheck-wzk}) with
  % respect to the claim $\sum_{\beta \in G^{k}}Z(c, \beta) = w$, using $A$ as the masking
  % polynomial. If the verifier in that protocol rejects, so does $V$\;
  $z' \leftarrow \sum_{\alpha \in H^{m}}A(\alpha)$\;
  $P$: send $z'$ to $V$\;
  $V$: draw random $\rho_{2} \in \mathbb{F}$\;
  $V$: send $\rho_{2}$ to $P$\;
  $Q'(x) \leftarrow \rho_{2}Z(c, x) + A(x)$\;
  Both: run \cref{alg:sumcheck-std} on the statement
  $\sum_{\alpha \in H^{m}}Q'(\alpha) = \rho_{2}w + z'$\;\nllabel{line:sumcheck-2}
  $V$: output the claim $F(c) = \frac{g_{m}(c_{m}) - w}{\rho_{1}}$\;\nllabel{line:return-claim}
  \caption{Strong zero-knowledge sumcheck~\cite[Construction 3]{CFGS22}}\label{alg:zk-sumcheck}
\end{algorithm}

To avoid an overly long-winded proof, we will split the above theorem into two
lemmata. The first will show that \cref{alg:zk-sumcheck} is correct, and the
second will show that it is in fact zero-knowledge.

\begin{lemma}\label{lem:sumcheck-is-correct}
  \Cref{alg:zk-sumcheck} is a MIP* algorithm for sumcheck.
\end{lemma}

\begin{proof}
  If $P$ is honest in \cref{alg:zk-sumcheck}, then both sumcheck protocols will
  pass if and only if $(F, H, a)$ is valid. From the various definitions in the
  program, we have
  \begin{align*}
    \sum_{\alpha \in H^{m}}Q(a) &= \rho_{1}a + z \\
    \sum_{\alpha \in H^{m}}\mleft(\rho_{1}F(\alpha) + \sum_{\beta \in G^{k}}Z(\alpha, \beta)\mright) &= \rho_{1}a + \sum_{\alpha \in H^{m}}\sum_{\beta \in G^{k}}Z(\alpha, \beta) \\
    \sum_{\alpha \in H^{m}}\rho_{1}F(a) + \sum_{a \in H^{m}}\sum_{\beta \in G^{k}}Z(\alpha, \beta) &= \rho_{1}a + \sum_{\alpha \in H^{m}}\sum_{\beta \in G^{k}}Z(\alpha, \beta) \\
    \rho_{1}\sum_{a \in H^{m}}F(a) &= \rho_{1}a \\
    \sum_{a \in H^{m}}F(a) &= a.
  \end{align*}
  Since $\rho_{1} \ne 0$, all of these transformations are biconditionally true;
  hence the sumcheck protocol in line~\ref{line:sumcheck-1} will pass if and
  only if $(F, H, a)$ is valid. The modification does not affect this
  correctness since all it does is limit the set of elements we can randomly
  sample from---since we know this works for all $r_{i} \in \mathbb{F}$, it will
  also be true for all $r_{i} \in I$.

  For the second sumcheck (in line~\ref{line:sumcheck-2}), we have
  \begin{align*}
    \sum_{\alpha \in H^{m}}Q'(\alpha) &= \rho w + z \\
    \sum_{\alpha \in H^{m}}\mleft(\rho Z(c, \alpha) + A(\alpha)\mright) &= \rho w + \sum_{\alpha \in H^{m}}A(\alpha) \\
    \sum_{\alpha \in H^{m}}\rho F(\alpha) + \sum_{\alpha \in H^{m}}A(\alpha) &= \rho w + \sum_{\alpha \in H^{m}}A(\alpha) \\
    \rho\sum_{\alpha \in H^{m}}F(\alpha) &= \rho w \\
    \sum_{\alpha \in H^{m}}F(\alpha) &= w.
  \end{align*}
  As before, these transformations are all biconditionally true; hence an honest
  $P$ will always cause the sumcheck in line~\ref{line:sumcheck-2} to succeed if
  and only if $(F, H, a)$ is valid.

  \begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[>=Stealth]
      \node (A) at (0, 0) {$(Z, a) \notin \text{sumcheck}$};
      \node[draw,rectangle,align=center] (B) at (0, -2) {RNG \\ $\rho_{1}$};
      \node[draw,rectangle,align=center] (C) at (0, -4) {Sumcheck IP \\ \cref{alg:sumcheck-std}};
      \node[draw,rectangle,align=center] (D) at (0, -6) {$w$ from $\tilde{P}$};
      \node[draw,rectangle,align=center] (E) at (1, -8) {Sumcheck IP \\ \cref{alg:sumcheck-std}};
      \node[draw,circle] (T1) at (3, -2) {T};
      \node[draw,circle] (T2) at (-4, -4) {T};
      \node[draw,circle] (T3) at (0, -10) {T};
      \node[draw,circle] (F1) at (4, -4) {F};
      \node[draw,circle] (F2) at (-2, -8) {F};
      \node[draw,circle] (F3) at (2, -10) {F};
      \draw[->] (A) to (B);
      \draw[->] (B) to (C);
      \draw[->] (B) to
        node[above,align=center] {\small{sumcheck} \\ \small{correct}}
        node[below] {$\frac{1}{\abs{\mathbb{F}} - 1}$}
      (T1);
      \draw[->] (C) to (D);
      \draw[->] (C) to (F1);
      \draw[->] (C) to
        node[above,align=center] {\small{true claim}}
        node[below] {$\frac{md}{\abs{I}}$}
      (T2);
      \draw[->] (D) to (E);
      \draw[->] (D) to (F2);
      \draw[->] (E) to
        node[left] {$\frac{kd+1}{\abs{\mathbb{F}}}$}
      (T3);
      \draw[->] (E) to (F3);
    \end{tikzpicture}
    \caption{The decision tree for \cref{alg:zk-sumcheck} given a false
      input}\label{fig:sumcheck-tree}
  \end{figure}

  If $P$ is dishonest, things get trickier. First, note that we have described
  the relevant portions of the path through \cref{alg:zk-sumcheck} as a decision
  tree in \cref{fig:sumcheck-tree}.

  Fix some $F \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ such that
  $\sum_{\alpha \in H^{m}}F(\alpha) \ne a$, and fix some $Z \in \mathbb{F}[X_{1, \ldots, m+k}^{\le d}]$.
  Define
  \begin{align*}
    \hat{a} &= \sum_{\alpha \in H^{m}}F(\alpha) \\
    \hat{z} &= \sum_{\alpha \in H^{m}}\sum_{\beta \in G^{k}}Z(\alpha, \beta).
  \end{align*}
  We know $\hat{a} \ne a$, and we do not know whether $\hat{z} = z$ (it depends on
  whether or not $P$ was honest in line~\ref{line:send-z}). The sumcheck
  question we ask in line~\ref{line:sumcheck-1} is true if and only if
  $\rho_{1}\hat{a} + \hat{z} = \rho_{1}a + z$. Through some algebra, this simplifies
  to being true if and only if
  \begin{equation}
    \rho_{1} = \frac{z - \hat{z}}{\hat{a} - a}.
  \end{equation}
  We know $\rho_{1}$ is a random element from $\mathbb{F}^{\times}$; hence if
  $z \ne \hat{z}$,
  \begin{align}
    \underset{\rho_{1} \in \mathbb{F}^{\times}}{\mathbb{P}}[\rho_{1}\hat{a} + \hat{z} = \rho_{1}a + z] &= \frac{1}{\abs{\mathbb{F}} - 1} \\
    \underset{\rho_{1} \in \mathbb{F}^{\times}}{\mathbb{P}}\mleft[\sum_{\alpha \in H^{m}}Q(\alpha) = \rho_{1}a + z\mright] &= \frac{1}{\abs{\mathbb{F}} - 1}
  \end{align}
  and if $z = \hat{z}$ the above probabilities are 0.

  Hence, by the soundness guarantee of \cref{alg:sumcheck-std}, if the presented
  sumcheck equation is incorrect then the subroutine will output a correct
  equation with probability at most $\frac{md}{\abs{I}}$.

  Next, we split into two cases: whether or not $P$ sends
  $w = \sum_{\beta \in G^{k}}Z(c, \beta)$. In the case where $P$ sends
  $w \ne \sum_{\beta \in G^{k}}Z(c, \beta)$, then as in the earlier sumcheck, we have that for
  a fixed $A$ and $Z$, $\sum_{\alpha \in H^{m}}Q'(\alpha) = \rho_{2}w + z'$ with probability at
  most $1/\abs{\mathbb{F}}$ (In this case we \emph{do} allow $\rho_{2}$ to be 0,
  since we will not be dividing by it in any later step). As we showed in
  \cref{thm:sumcheck-ip}, in the case when the sumcheck statement is false
  \cref{alg:sumcheck-std} will incorrectly accept with probability
  $kd/\abs{\mathbb{F}}$. Hence, line~\ref{line:sumcheck-2} will not reject with
  probability $\frac{kd + 1}{\abs{\mathbb{F}}}$.

  In the case where $P$ sends $w = \sum_{\beta \in G^{k}}Z(c, \beta)$, then it must be that
  $F(c) \ne \frac{g_{m}(c_{m})-w}{\rho_{1}}$, since if the verifier did not reject
  then we have
  \begin{align*}
    \rho_{1}F(c) + \sum_{\beta \in G^{k}}Z(c, \beta) &\ne g_{m}(c_{m}) \\
    \rho_{1}F(c) + w &\ne g_{m}(c_{m}) \\
    F(c) &\ne \frac{g_{m}(c_{m})-w}{\rho_{1}}
  \end{align*}
  regardless of $\rho_{1}$.

  Since $P$ is assumed to be malicious, we have to assume (in order to acquire
  an upper bound on when the input is incorrectly accepted) that it will
  therefore always send $w \ne \sum_{\beta \in G^{k}}Z(c, \beta)$, since that has a lower
  probability of being rejected.

  In total, the probability that we accept given incorrect input is, based on
  the decision tree from \cref{fig:sumcheck-tree}, bounded above by
  \begin{equation}
    \frac{1}{\abs{\mathbb{F}} - 1} + \mleft(1 - \frac{1}{\abs{\mathbb{F}} - 1}\mright)\mleft(
      \frac{md}{\abs{I}} + \mleft(1 - \frac{md}{\abs{I}}\mright)\mleft(
        \frac{kd + 1}{\abs{\mathbb{F}}}
      \mright)
    \mright).
  \end{equation}
  Since we are looking for an upper bound, we can simplify both
  $(1 - \frac{1}{\abs{\mathbb{F}} - 1})$ and $(1 - \frac{md}{\abs{I}})$ to 1,
  giving us that the probability is bounded above by
  \begin{equation}
    \frac{1}{\abs{\mathbb{F}} - 1} + \frac{md}{\abs{I}} + \frac{kd + 1}{\abs{\mathbb{F}}}.
  \end{equation}
  We can further increase the upper bound by decreasing the denominator of the
  third term by 1, allowing us to simplify to
  \begin{equation}
    \frac{md}{\abs{I}} + \frac{kd + 2}{\abs{\mathbb{F}}},
  \end{equation}
  which is the claimed soundness bound.
\end{proof}

\begin{lemma}\label{lem:sumcheck-is-pzk}
  \Cref{alg:zk-sumcheck} is zero-knowledge save for a single query to $F$.
\end{lemma}

\begin{algorithm}[htbp]
  Pick random
  $Z_{s} \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le 2\lambda}]$\;\nllabel{line:sim-pick-z}
  Run the weak-ZK sumcheck simulator\; % FIXME: Which algorithm is this?
  Begin simulating $\tilde{V}$, answering its oracle queries with $Z_{s}$ and
  the simulated $A$\;
  Send $z_{s} = \sum_{\alpha \in H^{m}}\sum_{\beta \in G^{k}}Z_{s}(\alpha, \beta)$\;
  Receive $\tilde{\rho}$ from the simulated $\tilde{V}$\;
  Draw $Q_{s} \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ such that
  $\sum_{\alpha \in H^{m}}Q_{s}(\alpha) = \tilde{\rho}\alpha + z_{s}$\;
  Engage in the sumcheck protocol (\cref{alg:sumcheck-std}) on the claim
  $\sum_{\alpha \in H^{m}}Q_{s}(\alpha) = \tilde{\rho}\alpha + z_{s}$\;
  \If{$\tilde{V}$ sends $c_{i} \notin I$ as a challenge in the above protocol}{
    \KwRet{$\bot$}\;
  }
  Let $c \in I^{m}$ be the point chosen by $\tilde{V}$\;
  Query $F(c)$\;
  $w_{s} \leftarrow Q_{s}(c) - \tilde{\rho}F(c)$\;
  Send $w_{s}$\;
  Draw $Z_{s}' \in \mathbb{F}[X_{1, \ldots, m}^{\le d}, Y_{1, \ldots, k}^{\le 2\lambda}]$ such that
  $\sum_{\beta \in G^{k}}Z_{s}'(c, \beta) = w_{s}$ and $Z_{s}'(\gamma) = Z_{s}(\gamma)$ for all
  previous queries $\gamma$\;
  Use $S'$ to simulate the sumcheck protocol for the claim
  $\sum_{\beta \in G^{k}}Z_{s}'(c, \beta) = w_{s}$\;
  \KwRet{the view of the simulated $\tilde{V}'$}\;
  \caption{An inefficient simulator for
    \cref{alg:zk-sumcheck}~\cite[p.\ 15:33]{CFGS22}}\label{alg:zk-sumcheck-sim}
\end{algorithm}

\begin{proof}
  We construct a simulator for \cref{alg:zk-sumcheck} in two phases: first we
  construct a simulator that is slow but correct as \cref{alg:zk-sumcheck-sim}.
  Following that, we explain how to convert that simulator into an efficient
  version.

  We will be showing that at each step, every random variable received by the
  (slow) simulator is identical to that of the prover in \cref{alg:zk-sumcheck}.
  If every random variable is identical, then since \cref{alg:zk-sumcheck-sim}
  is constructed similarly to the prover it will output the same result.

  In the simulator, the random choice of $Z_{s}$ in line~\ref{line:sim-pick-z},
  which is identical to the choice of $Z$ in line~\ref{line:p-pick-z} in the
  original algorithm. Similarly, our result $z_{s}$ is identical to the $z$ in
  the original protocol.

  Next, note that $Q_{s}$ is distributed identically to $Q$ from the original
  statement, since the components defining $Q$ are uniformly random. In
  particular, note that $\tilde{\rho}$ is a function constructed with fewer than
  $\lambda^{k}$ queries to $Z$. Hence, due to \cref{cor:indep-ensemble}, this means
  $\tilde{\rho}$ is independent of $R$ so long as $\sum_{\alpha \in H^{m}}R(\alpha) = z$. Hence,
  so long as that formula holds, $\tilde{\rho}F$ is independent of $R$ and thus
  $R + \tilde{\rho}F$ is a uniformly random polynomial subject to the condition
  that $\sum_{\alpha \in H^{m}}Q(\alpha) = \tilde{\rho}a + z$.

  Next, we send $Q_{s}(c) - \tilde{\rho}F(c)$ to the verifier in our simulator. The
  original prover sends $Q(c) - \tilde{\rho}F(c)$. Since $Q_{s}$ and $Q$ are
  identically distributed, so too are these values.

  After that, our simulator draws a new $Z'_{s}$ that is random, but agrees with
  the already-seen $Z_{s}$ everywhere we have already checked. If we define $U$
  to be the set of question-response pairs already seen, then
  \cref{cor:indep-ensemble} gives us the following:
  \begin{multline}\label{eqn:z-query-dist}
    \underset{Z'_{s}}{\mathbb{P}}\mleft[
      Z'_{s}(q) = a \middlemid
      \begin{matrix}
        Z'_{s}(\gamma) = b \quad \forall (\gamma, b) \in U \\
        \sum_{\beta \in G^{k}}Z'_{s}(c, \beta) = w_{s}
      \end{matrix}
    \mright] \\
    = \underset{Z}{\mathbb{P}}\mleft[
      Z(q) = a \middlemid
      \begin{matrix}
        Z(\gamma) = b \quad \forall (\gamma, b) \in U \\
        \sum_{\beta \in G^{k}}Z'_{s}(X, \beta) = Q(X) - \tilde{\rho}F(X)
      \end{matrix}
    \mright]
  \end{multline}
  for any $q \in \mathbb{F}^{m+k}$ and $a \in \mathbb{F}$.

  The left-hand side of this equation is the exactly the distribution of the
  answer to a query $q$ made by $S'$ to $Z'_{s}$. Similarly, the right-hand side
  describes the answer to a query to $Z$ under the same constraints. Since these
  probabilities are identical, it follows that the results of the queries must
  too be identically distributed.

  Lastly, we can transform \cref{alg:zk-sumcheck-sim} into a polynomial-time
  algorithm through using a more efficient encoding system described
  in~\cite{BCFGRS17}; this allows us to maintain some state between simulations
  and thus improve the efficiency to polynomial-time.
\end{proof}

\section{Extending the sumcheck algorithm to $\NEXP$}

Armed with our sumcheck algorithm, we are ready to take on the fight of the rest
of $\NEXP$. Like so many other theorems, we will do this with a $\NEXP$-complete
problem, and as before our problem of choice is $\OSAT$.

\begin{thm}[{\cite[Thm.\ 14.2]{CFGS22}}]\label{thm:pzkipcp-for-nexp}
  There exists a $c \in \mathbb{N}$ such that for any query-bound function $b(n)$,
  $d(n) \in \Omega(n^{c})$, $m(n) \in O(n^{c}\log(b))$, and any sequence of fields
  $\mathbb{F}(n)$ that are field extensions of $\mathbb{F}_{2}$ with
  $\abs{\mathbb{F}(n)} \in \Omega((n^{c}\log(b))^{4})$,
  \begin{equation*}
    \OSAT \in \IPCP\ldipcp{O(n, b)}{\poly(2^{n}, b)}{\poly(n, \log(b))}{\poly(n, \log(b))}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{1/2},
  \end{equation*}
  which is zero-knowledge with query bound $b$.
\end{thm}

\begin{algorithm}[htbp]
  \Repeat{$\sum_{\beta \in G^{k}}Z(\alpha, \beta) = A(\gamma_{2}(\alpha))$ for all $\alpha \in H^{m_{2}}$}{
    $P$: Draw a random
    $Z \in \mathbb{F}[X_{1, \ldots, m}^{\le \abs{H} + 2}, Y_{1, \ldots, m}^{\le 2\abs{H}}]$\;
  }
  $P$: Generate oracle $\pi_{0}$ for \cref{alg:zk-sumcheck} on input
  $(\mathbb{F}, m_{1}+3m_{2}, d, H, 0)$\;
  $P$: Generate oracles $\pi_{1}, \pi_{2}, \pi_{3}$ for \cref{alg:zk-sumcheck} on
  input $(\mathbb{F}, k, 2\abs{H}, \abs{H}, \cdot)$\;
  $P$: Send $(\pi_{0}, \pi_{1}, \pi_{2}, \pi_{3})$\;
  $V$: Choose $x, y \in \mathbb{F}^{r+3s}$ at random\;
  $V$: Send $x$ and $y$ to $P$\;
  Both: Simulate \cref{alg:zk-sumcheck} over the claim $F(x, y) = 0$ with
  $I = \mathbb{F} \setminus H$ and oracle $\pi_{1}$\;
  \For{$i \in \{1, 2, 3\}$}{
    $P$: Send $h_{i} = A(\gamma_{2}(c_{i}'))$ to $V$
  }
  $V$: Substitute the $h_{i}$ into the evaluation of $f$\;
  \If{the claims do not hold}{
    \Reject\;
  }
  \For{$i \in \{1, 2, 3\}$}{
    $P$ and $V$ implement \cref{alg:sumcheck-std} on the claim
    $\sum_{\beta \in H^{k}}Z(c_{i}', \beta) = h_{i}$\;
  }
  \caption{A low-degree IPCP for $\OSAT$~\cite[p.\ 15:36]{CFGS22}}\label{alg:ipcp-o3sat}
\end{algorithm}

\begin{algorithm}[htbp]
  Draw a random polynomial
  $Z_{s} \in \mathbb{F}[X_{1, \ldots, m_{2}}^{\le \abs{H}+2}, Y_{1, \ldots, k}^{\le 2\abs{H}}]$\;
  Let $S_{0}$ be a simulated copy of \cref{alg:zk-sumcheck} on input
  $(\mathbb{F}, m_{1} + 3m_{2}, \deg(f), H, 0)$\;
  \For{$i \in \{1, 2, 3\}$}{
    Let $S_{i}$ be a simulated copy of \cref{alg:zk-sumcheck} on input
    $(\mathbb{F}, k, 2\abs{H}, \abs{H}, \cdot)$\;
  }
  Simulate $\tilde{V}$ to receive $x, y \in \mathbb{F}^{r + 3s}$\;
  Simulate \cref{alg:zk-sumcheck} on the claim $F(x, y) = 0$. To answer the
  single query it makes at $c \in (\mathbb{F} \setminus H)^{r+3s}$, reply with
  $f(x, y, c)$ where $c = (c_{0}, c_{1}, c_{2}, c_{3})$. To compute the values
  $A(\gamma(c_{i}))$, substitute with an $h_{s}^{i} \in \mathbb{F}$ drawn at random\;
  \For{$i \in \{1, 2, 3\}$}{
    Simulate \cref{alg:zk-sumcheck} with the claim
    $\sum_{\beta \in H^{k}}Z(\alpha, \beta) = h_{s}^{i}$, answering queries with $Z_{s}$\;
  }
  \caption{A simulator for \cref{alg:ipcp-o3sat}~\cite[p.\ 15.37]{CFGS22}}\label{alg:o3sat-simulator}
\end{algorithm}

\begin{proof}
  We present an algorithm for $\OSAT$, which we have laid out in
  \cref{alg:ipcp-o3sat}. We will show this is zero-knowledge by implementing a
  simulator in \cref{alg:o3sat-simulator}. For time reasons, the proof of
  correctness for this cannot be reproduced here, but it is in~\cite[Theorem
  14.2]{CFGS22}.
\end{proof}

\begin{cor}\label{nexp-pzkipcp}
  $\NEXP \subseteq \PZKIPCP$.
\end{cor}

\begin{proof}
  Since $\OSAT$ is $\NEXP$-complete as per \cref{thm:o3sat-nexp-complete}, we
  can perform a polynomial reduction from any other language to $\OSAT$ and then
  run \cref{alg:ipcp-o3sat}.
\end{proof}

\section{Zero-knowledge $\MIP*$ for $\NEXP$}\label{sec:zk-mipstar-nexp}

\begin{thm}[{\cite[Lemma 9.1]{CFGS22}}]\label{thm:lift-ipcp-mip}
  Let $L$ be a language, let $m, d, q \in \mathbb{N}$, and let $\mathbb{F}$ be a finite
  field of size $\poly(m, d, q)$ sufficiently large. Then, there exists a
  transformation
  \begin{equation*}
    T: \IPCP\ldipcp{r}{\ell}{c}{q}{\mathbb{F}[X_{1, \ldots, m}^{\le d}]}{\varepsilon}
    \rightarrow \MIP*\mipstar{2}{r+1}{c}{1 - \frac{1}{\poly(m, d)}}.
  \end{equation*}
  such that $(P', V')$ and $T(P', V')$ recognize the same language.

  Further, if the IPCP $(P', V')$ is zero-knowledge with query bound
  $b \ge 2(q+1)md + 3$, then the $\MIP*$ $(P_{1}, P_{2}, V)$ is zero-knowledge.
\end{thm}

The above theorem demonstrates that we can take any IPCP and lift it into a MIP*
while preserving zero-knowledge. Since \cref{alg:ipcp-o3sat} is a zero-knowledge
MIP* algorithm for $\OSAT$ and $\OSAT$ is $\NEXP$-complete, it follows that
$\NEXP \subseteq \PZKMIP$.

\chapter{A zero-knowledge PCP theorem}\label{chap:zk-pcp-theorem}

In \cref{chap:pcp-theorem}, we discussed and proved the PCP theorem. Given we
have seen how widespread zero-knowledge PCPs can be, the question arises of
whether or not the PCP theorem can be recreated entirely with zero-knowledge
PCPs. Not only can it be recreated, but we can also show a PCP-theorem
equivalent for $\NEXP$.

Our proof will proceed in broadly the same manner as our original proof of the
PCP theorem, and it will reuse broad portions of the same machinery. In order to
reuse this, though, we will need a new notion of closeness for our proofs. This
notion of closeness is important because we can show that it preserves
zero-knowledge: that is, a machine that is close to a zero-knowledge machine
must itself be zero-knowledge. This will allow us to more easily prove that we
are constructing zero-knowledge proofs, without going through the hassle of
constructing a simulator every time (and proving that it is in fact a
simulator).

\section{Locally-computable proofs}\label{sec:loc-comp-proof}

Locally-computable proofs are a notion of closeness for Turing machines.
Abstractly, a Turing machine is $\ell$-locally computable from another if it can be
simulated in polynomial time using no more than $\ell$ queries about the behavior
of the source machine.

\begin{defn}[{\cite[Def.\ 3.1]{GOS25}}]\label{def:loc-comp}%
  \index{locally-computable proof} Let $A$ and $A_{0}$ be randomized Turing
  machines, and let $\ell\colon \mathbb{N} \rightarrow \mathbb{N}$. Then $A$ is \emph{$\ell$-locally computable}
  from $A_{0}$ on a subset $C \subseteq \{0, 1\}^{*}$ if there exists an oracle Turing
  machine $f$ that runs in polynomial time and makes no more than $\ell(n)$ queries
  to its oracle such that for every $x \in C$, the distribution of $A(x)$ is
  identical to the distribution of $f$ with oracle $\pi_{0} = A_{0}(x)$.
\end{defn}

While this is not strictly a metric in the formal, metric-space definition (in
particular, there exist distinct machines that are $0$-locally computable from
each other\footnote{Since our simulators are allowed to be polynomial-time, any
  machine in $\P$ is $0$-locally computable from any other machine, as an
  example.}), it still obeys the triangle inequality and any machine is
$0$-locally computable from itself; hence we can think of it as being
metric-\emph{like}.

For local-computation to be useful to us, we will need to rethink how we have
been perceiving PCPs up to this point. So far, we have just thought of a PCP
proof as being a static oracle that gets queried by our verifier. However, in
the real world oracles do not spring in the world fully formed, they must be
generated. Hence, here we will think of the proof as being generated by another
Turing machine.

For us, the most important piece of locally-computable proofs is that they
preserve zero knowledge. This will give us an easier way to prove that a given
algorithm is zero-knowledge, since we can simply show it is locally-computable
from another algorithm we have already shown. We will find this particularly
useful when we are dealing with \emph{transformations} of zero-knowledge PCPs;
if we are starting from a zero-knowledge base, the idea of local computation
will allow us to easily show that our transformed PCP is also zero-knowledge.

\begin{thm}[{\cite[Lemma 3.2]{GOS25}}]\label{thm:local-comp-pzk}
  Let $(P_{0}, V_{0})$ be a PZK-PCP for some language $L$ with query bound
  $q^{*}$, and let $(P, V)$ be a PCP for a language $M$ such that $P$ is
  $\ell$-locally computable from $P_{0}$ on $M$. Then $(P, V)$ is perfect
  zero-knowledge with query bound $q^{*}/\ell$.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{A string $x$ and random coins $r$}
  \KwOracle{A function $\pi_{0}$}
  \KwOut{The interaction transcript of $(P, V^{*})$ on input $x$}
  % NOTE: r is an input
  $T \leftarrow \lbrack\,\rbrack$\;
  Run $V^{*}$ on random coins $r$\;
  \For{each query $\alpha$ that $V^{*}$ makes}{
    $\beta \leftarrow f^{\pi_{0}}(\alpha)$\;
    Push $(\alpha, \beta)$ onto $T$\;
  }
  \KwRet{$(r, T)$}\;
  \caption{A hybrid simulator for a locally-computable PCP~\cite[Construction
    3.3]{GOS25}}\label{alg:hybrid-sim-lc}
\end{algorithm}

\begin{algorithm}[htbp]
  \KwIn{A string $x$}
  \KwOut{The interaction transcript of $(P, V^{*})$ on input $x$}
  % TODO: Define \overline{\Sim}
  Run $\overline{\Sim}_{A_{V^{*}}}(x)$ to obtain $T_{0}$ with random coins $r$\;
  Run $A_{V^{*}}(x, r)$ (\cref{alg:hybrid-sim-lc}) using $T_{0}$ to answer its
  questions\;
  \KwRet{$(r, T)$}\;
  \caption{A PZK simulator for a locally-computable PCP~\cite[Construction
    3.4]{GOS25}}\label{alg:pzk-sim-lc}
\end{algorithm}

\begin{proof}
  We construct a simulator for $(P, V)$ in \cref{alg:pzk-sim-lc}.

  Let $V^{*}$ be a malicious verifier for $(P, V)$, and let $\pi \leftarrow P$ and
  $\pi_{0} \leftarrow P_{0}$ be random variables. Since $P$ is $\ell$-locally computable with
  the function $f$, by definition we have that $\pi$ is identically-distributed to
  $(f^{\pi_{0}}(\alpha))_{\alpha \in \dom(\pi_{0})}$. Since all \cref{alg:hybrid-sim-lc} does is
  compute $f^{\pi_{0}}(\alpha)$ for each query $\alpha$, it will reproduce the same
  transcript as $(P, V^{*})$ would.

  Next, since $(P_{0}, V_{0})$ is a PZK-PCP, and \cref{alg:hybrid-sim-lc} is a
  verifier for $\pi_{0}$, by definition there exists a simulator
  $\overline{\Sim}_{A_{V^{*}}}$ whose output is identically-distributed to
  $\View_{A_{V^{*}}, P_{0}}$, so long as $A_{V^{*}}$ makes no more than $q^{*}$
  queries to $\pi_{0}$. Since $P$ is $\ell$-locally computable from $P_{0}$, it
  follows that \cref{alg:hybrid-sim-lc} makes no more than $\ell$ queries to
  $\pi_{0}$. Hence, \cref{alg:pzk-sim-lc} makes no more than $q^{*}$ queries to
  $\pi_{0}$ so long as $V^{*}$ makes no more than $q^{*}/\ell$ queries to $\pi$. Thus,
  the output of \cref{alg:pzk-sim-lc} is identical to the view of
  \cref{alg:hybrid-sim-lc}.
\end{proof}

\begin{cor}\label{cor:local-comp-pzk}
  Let $(P_{0}, V_{0})$ be a PZK-PCPP for some language $L$ with query bound
  $q^{*}$ and proximity parameter $\delta$, and let $(P, V)$ be a PCPP for a language
  $M$ with proximity parameter $\delta$ such that $P$ is $\ell$-locally computable
  from $V$ on $M$. Then $(P, V)$ is perfect zero-knowledge with query bound
  $q^{*}/\ell$.
\end{cor}

\begin{proof}
  By replacing every instance of PZK-PCP in that proof with PZK-PCPP, every
  piece of the proof still holds, since zero-knowledge is identical for PCPs as
  it is for PCPPs.
\end{proof}

\section{Zero-knowledge proof composition}\label{sec:zk-proof-comp}

Essential to our theorems will be the ability to combine proofs in a way that
preserves zero knowledge. In \cref{thm:composition}, we showed that composing
robust PCPs and PCPPs can result in further PCPs; here we show that that
combination also preserves zero knowledge. To do this, we will leverage the
notion of local computation we defined in the last section. More specifically,
\cref{alg:composed-pcp} is locally-computable from our robust pcp $V_{\out}$,
\emph{regardless} of whether or not the PCPP $V_{\oin}$ is zero-knowledge.

\begin{thm}[{\cite[Theorem 3.7]{GOS25}}]\label{thm:comp-pzk}
  The construction in \cref{thm:composition} is perfect zero-knowledge with
  query bound $q^{*}/q_{\out}$ if $V_{\out}$ is perfect zero-knowledge with
  query bound $q^{*}$.
\end{thm}

\begin{algorithm}[htbp]
  % TODO: Steps 2-4 from the composed PCP
  % Define D_out and \pi_out
  \KwIn{A string $r \in \{0, 1\}^{r_{\out}}$}
  \KwOut{The function $\pi_{r}$}
  $I_{\out} \leftarrow Q_{\out}(x, r)$\;
  Compile $D_{\out}$ (the decision algorithm of $V_{\out}$) on input $r$ into a
  circuit $C_{\out}\colon \{0, 1\}^{n} \times \{0, 1\}^{\ell_{\out}} \rightarrow \{0, 1\}$\;
  Run $P_{\oin}(C_{\out}, \pi_{\out}|_{I_{\out}})$ to get $\pi_{r}$\;
  \KwRet{$\pi_{r}$}\;
  \caption{An algorithm for $\pi_{r}$ from $\pi_{0}$}\label{alg:pi-r-from-pi-0}
\end{algorithm}

\begin{proof}
  All that is needed to prove this theorem is to show that
  \cref{alg:composed-pcp} preserves zero-knowledge, since we have already showed
  that it satisfies the conditions in \cref{thm:composition}. We do this by
  showing $P_{\comp}$ is $q_{\out}$-locally computable from $P_{\out}$.

  We need to show that for any input $x \in L$, the distributions of $P(x)$ and
  $f^{\pi_{0}}(x)$ are identically distributed. Consider the function
  \begin{equation}
    f(O, r) = \begin{cases}
      \pi_{0}(r) & O = b_{0} \\
      \text{\cref{alg:pi-r-from-pi-0}} & O = b_{r}.
    \end{cases}
  \end{equation}
  If $O = b_{0}$, then we make no more than one query to $\pi_{0}$. If
  $O = b_{r}$, then \cref{alg:pi-r-from-pi-0} makes no more than
  $\abs{I_{\out}}$ queries to $\pi_{\out}$ (since that is the size of the domain
  of the restricted function); regardless of $r$ we have that
  $\abs{I_{\out}} \le q_{\out}$ since $I_{\out}$ is a set of queries and
  $q_{\out}$ is the maximum number of queries that $V_{\out}$ makes. Hence $f$
  makes no more than $q_{\out}$ queries.

  Lastly, so long as $V_{\out}$ runs in polynomial time, so must $D_{\out}$ and
  $Q_{\out}$; compiling into a circuit is also known to take polynomial time.
  Lastly, the problem statement tells us $P_{\oin}$ is guaranteed to run in
  polynomial time; it follows that $f$ runs in polynomial time. Lastly, since
  \cref{alg:pi-r-from-pi-0} uses its input $r$ as randomness to all the
  algorithms it calls, it follows that $f$ itself is deterministic (since all
  randomness comes from the choice of $r$). Hence, $P_{\comp}$ is
  $q_{\out}$-locally computable from $P_{\out}$.

  Since $P_{\comp}$ is $q_{\out}$-locally computable from $P_{\out}$, by
  \cref{thm:local-comp-pzk} we have that $P_{\comp}$ is perfect zero-knowledge
  with query bound $q^{*}/q_{\out}$.
\end{proof}

This ability to compose PCPs gives us some useful properties of the class
$\PZKPCP$. In particular, it shows us the ability to reduce a $\PZKPCP$ to a
\emph{single} query in a zero-knowledge manner, simply with a corresponding
worsening of our randomness complexity and zero-knowledge query bounds.

\begin{cor}[{\cite[Corollary 3.11]{GOS25}}]\label{cor:pzk-constant-query}
  \begin{equation}\label{eqn:pzk-constant-query}
    \PZKPCP\pzkpcpr{r}{q}{q^{*}}{\varepsilon}{s}{\Omega(1)} \subseteq
    \PZKPCP\pzkpcpr{r + \log(n)}{1}{q^{*}/q}{\varepsilon}{s}{\Omega(1)}.
  \end{equation}
\end{cor}

\begin{proof}
  Let
  \begin{equation}
    L \in \PZKPCP_{\{0, 1\}}\pzkpcpr{r}{q}{q^{*}}{\varepsilon}{s}{\Omega(1)}.
  \end{equation}
  Then as per \cref{thm:ckt-val-pcpp}, we have
  \begin{equation}
    \CktVal \in \PCPP\pcpp{\log(n) + O(\log^{\varepsilon}(n))}{O(1/\varepsilon)}{k\varepsilon}{1/2},
  \end{equation}
  for some $\varepsilon > 0$ and constant $k$. Define $\varepsilon = \rho/k$. Then, as per
  \cref{thm:comp-pzk}, we have $L$ as desired.
\end{proof}

\section{Zero-knowledge alphabet reduction}\label{sec:zk-alph-red}

Back in \cref{sec:alph-reduction}, we talked about the importance of alphabet
reduction in our proofs. As before, alphabet reduction will be important, but
here we need to show that an alphabet reduction also preserves zero knowledge.
Luckily for us, it does. The idea behind our alphabet reduction is to use
error-correcting codes: since error-correcting codes are designed with the idea
that correct values are far from incorrect ones, so too can they help guard
against incorrect proofs here.

\begin{thm}[{\cite[Lemma 2.13]{BGHSV06}}]%
  \label{thm:zk-alph-red}\label{alphabet reduction!zero-knowledge}
  Let $L$ be a language with a PZK-PCP over the language $\{0, 1\}^{a}$ such
  that
  \[
    L \in \PZKPCP_{\{0,1\}^{a}}\pzkpcpr{r}{q}{q^{*}}{\varepsilon}{s}{\rho}.
  \]
  Then $L$ has a PZK-PCP over the language $\{0, 1\}$ such that
  \[
    L \in \PZKPCP_{\{0,1\}}\pzkpcpr{r}{O(aq)}{q^{*}}{\varepsilon}{s}{\Omega(\rho)}.
  \]
\end{thm}

\begin{proof}
  We proved the non-zero-knowledge version of this as \cref{thm:alph-reduction},
  so all that remains is to prove that \cref{alg:boolean-reduction} preserves
  zero-knowledge.

  First, we show $P$ is $1$-locally computable from $P_{a}$. Consider the
  function $f$ (with oracle $\pi_{a}$) defined by
  \begin{equation}\label{eqn:local-alph-red}
    \begin{aligned}
      f^{\pi_{a}}\colon \{b_{\pi}, b_{\tau}\} \times \dom(\pi_{a}) \times [n] &\rightarrow \{0, 1\} \\
      f^{\pi_{a}}(O, \alpha, i) &\mapsto \begin{cases}
        \pi_{a}(\alpha) & O = b_{\pi} \\
        \ECC(\pi_{a}(\alpha))_{i} & O = b_{\tau}.
      \end{cases}
    \end{aligned}
  \end{equation}
  We show the distribution of $P$ is the same as the distribution of $f$ with
  oracle from $P_{a}$ for all $x$.

  The domain for our function is three values: $O$, a marker for either $\pi_{a}$
  or $\tau$; $\alpha$, the value we query; and $i$, the bit of the error-correcting code
  we wish to query. This is equivalent to a way to access the output of $P$ from
  \cref{alg:boolean-reduction}; since it returns an ordered pair of two
  functions $(\pi_{a}, \tau)$, whenever we query it we need first to choose which of
  the two functions to query, the value we are querying them on, and then since
  we can only look at one bit, if we are querying $\tau$ we need to also determine
  which specific bit we are asking for. Hence, for any $(O, \alpha, i)$, the
  definition of $f$ means that $\pi(O, \alpha, i) = f^{\pi_{0}}(O, \alpha, i)$. Hence the two
  are identically distributed and thus $P$ is $1$-locally computable from
  $P_{a}$.

  Since $P$ is $1$-locally computable from $P_{a}$, it follows from
  \cref{thm:local-comp-pzk} that $P$ is perfect zero-knowledge with the same
  query bound as $P_{a}$.
\end{proof}

\section{Zero-knowledge PCPPs for $\Sum$}\label{sec:zk-pcpp-sum}

In \cref{sec:pcpp-poly-sum}, we described a PCPP for the language $\Sum$. Since
we are now working with zero-knowledge proofs, the question arises of whether
this PCPP can be made zero knowledge. It can, although it requires an algorithm
with a little more complexity than the non-zero knowledge \cref{alg:sum-pcpp}.

\begin{algorithm}[htbp]
  % TODO: KwIn/KwOut
  \Proof{
    Sample $Q \in \mathbb{F}[X_{1, \ldots, m}^{\le d}]$ uniformly\;
    \For{$i \in \{1, \ldots, m\}$}{
      Sample
      $T_{i} \in \mathbb{F}[X_{1, \ldots, i-1}^{\le d}, X_{i}^{\le d-\abs{H}}, X_{i+1, \ldots, m}^{\le d}]$
      uniformly\;
    }
    Define $\pi_{P}(x) = (Q(x), T_{1}(x), \ldots, T_{m}(x))$\;
    Define $Z_{H}(X) = \prod_{a \in H}(X - a)$\;
    Define $Q_{\text{rev}}(X) = Q(X_{\text{rev}})$\;
    Define $R(X) = Q(X) - Q_{\text{rev}}(X) + \sum_{i=1}^{m}Z_{H}(X_{i})T_{i}(X)$\;\nllabel{line:def-r}
    Compute the proof $\pi_{\Sigma}$ for \cref{alg:sum-pcpp} with explicit input
    $(\mathbb{F}, m, d, H, \gamma, \delta)$ and implicit input $F + R$\;
    \KwRet{$(\pi_{\Sigma}, \pi_{P})$}\;
  }
  \Verifier{
    \DontPrintSemicolon
    Emulate \cref{alg:sum-pcpp} on input $F + R$ and proof $\pi_{\Sigma}$. To query
    $F + R$ at some $\alpha \in \mathbb{F}^{m}$, query $F(\alpha)$, $\pi_{P}(\alpha)$, and
    $\pi_{P}(\alpha_{\text{rev}})$, then compute
    \begin{algomathdisplay}\nllabel{line:emulate-sum-pcpp}
      \begin{split}
        (F+R)(\alpha) = F(\alpha) + (\pi_{P}(\alpha))_{1} - (\pi_{P}(\alpha_{\text{rev}}))_{1} \\
        + \sum_{i=1}^{m}Z_{H}(\alpha_{i})(\pi_{P}(\alpha))_{i+1};
      \end{split}
    \end{algomathdisplay}
    \PrintSemicolon
    Perform \cref{alg:robust-low-deg} on $F$ with proximity parameter
    $\delta_{RM} = \min(d, 1/5)$\;\nllabel{line:deg-test-f}
    \If{\cref{alg:robust-low-deg} fails}{
      \Reject\;
    }
    Perform \cref{alg:robust-low-deg} on $\pi_{P}$ with proximity parameter
    $\varepsilon_{P} = \delta_{R}/8$ and degree parameter $d_{P} = md$\;\nllabel{line:emulate-deg-test}
    \eIf{\cref{alg:robust-low-deg} fails}{
      \Reject\;
    }{
      \Accept\;
    }
  }
  \caption{A zero-knowledge robust PCPP for $\Sum$~\cite[Construction
    5.2]{GOS25}}\label{alg:sum-pzk-pcpp}
\end{algorithm}

\begin{thm}[{\cite[Lemma 5.1]{GOS25}}]\label{thm:pcpp-sum-pzk}
  Let $\delta > 0$, $\mathbb{F}$ be a finite field, $H \subseteq \mathbb{F}$,
  $\gamma \in \mathbb{F}$, and $m, d \in \mathbb{N}$ such that $\frac{md}{\abs{\mathbb{F}}} < \delta$
  and $d > \abs{H} + 1$. Then there exists a perfect zero-knowledge PCP of
  proximity for $\Sum[\mathbb{F}, m, d, H, \gamma]$ over the alphabet
  $\mathbb{F}^{m+1}$ with proximity parameter $\delta$ and robustness parameter
  $\rho = \Omega(\delta)$. Further, the verifier makes $O(\abs{\mathbb{F}})$ queries to $F$
  and $\pi$ and the proof length is $O(\abs{\mathbb{F}}^{m})$.
\end{thm}

\begin{proof}
  We construct such an algorithm as \cref{alg:sum-pzk-pcpp}. First, we will show
  that the verifier runs in polynomial time. Then, we will show that this is a
  PCP of proximity by showing correctness in both acceptance and rejection (with
  high probability), and then we will demonstrate zero
  knowledge.

  We know from \cref{thm:pcpp-sum} that \cref{alg:sum-pcpp} runs in polynomial
  time. Since we also do nontrivial work for each query, we need to show that
  that work is itself polynomial---since the sum is over $m$ terms, it is. As per
  \cref{thm:robust-low-deg}, \cref{alg:robust-low-deg} is also computable in
  polynomial time. The rest of the algorithm is just running
  \cref{alg:robust-low-deg} twice; hence in total the verifier in
  \cref{alg:sum-pzk-pcpp} runs in polynomial time.

  Let $F \in \Sum[\mathbb{F}, m, d, H, \gamma]$, and let $(\pi_{\Sigma}, \pi_{P})$ be the honest
  proof. In this case, as we showed in \cref{thm:pcpp-sum}, the simulation of
  \cref{alg:sum-pcpp} we do on line~\ref{line:emulate-sum-pcpp} will always
  succeed. To show that the equation in line~\ref{line:emulate-sum-pcpp} is
  true, note that from the definition of $\pi_{P}$, $(\pi_{P}(\alpha))_{1} = Q(x)$ and
  $(\pi_{P}(\alpha))_{i+1} = T_{i}(\alpha)$ for all $i \ge 1$. Doing these substitutions, the
  right hand side of the equation is exactly $F(\alpha)$ plus the definition of $R$
  from line~\ref{line:def-r}. Since $F$ is defined in the problem statement to
  be a polynomial of degree $d$, the check in line~\ref{line:deg-test-f} will
  always succeed. Further, $Q$ and each $T_{i}$ are defined to be
  multidegree-$d$ $m$-variable polynomials and thus their total degree will be
  less than $md$. Hence, the check in line~\ref{line:emulate-deg-test} will
  pass. As such, when given valid inputs \cref{alg:sum-pzk-pcpp} will always
  succeed.

  Let $F$ be $\delta$-far from $\Sum[\mathbb{F}, m, d, H, \gamma]$. We split into two
  cases: where $F$ is $\delta_{\RM}$-close to $\RM[\mathbb{F}, m, d]$ and where $F$
  is $\delta_{\RM}$-far from $\RM[\mathbb{F}, m, d]$. Similarly to what we did in
  \cref{thm:pcpp-sum}, we will need to introduce a technical lemma to prove some
  of this. Like before, we will leave this unproven, as its proof is highly
  technical and not particularly interesting.

  \begin{lemma}[{\cite[Lemma 5.3]{GOS25}}]\label{lem:sum-ev-close}
    Let $\tilde{F}\colon \mathbb{F}^{m} \rightarrow \mathbb{F}$ be $\delta_{\Sigma}$-far from
    $\Sum[\mathbb{F}, m, d, H, \gamma]$, but $\delta_{\RM}$-close to $\RM[\mathbb{F}, m, d]$
    for $\delta_{\Sigma} > \delta_{\RM} \ge \frac{md}{\abs{\mathbb{F}}}$, $\delta_{\RM} < 1/5$, and
    $d \ge \abs{H} + 1$. Then, for all proofs $(\pi_{\Sigma}^{*}, \pi_{P}^{*})$,
    \begin{multline}
      \underset{c \leftarrow \mathbb{F}^{m-1}}{\mathbb{E}}\mleft[
      \Delta\mleft(
      (\pi_{\Sigma}^{*}(c_{m-2}, \alpha)_{\alpha \in \mathbb{F}}, \pi_{P}^{*}(c, \alpha)_{\alpha \in \mathbb{F}},
      \pi_{P}^{*}(\alpha, c_{\rev})_{\alpha \in \mathbb{F}}, \tilde{F}(c, \alpha)_{\alpha \in \mathbb{F}}),
      \Acc(V)
      \mright)
      \mright] \\
      \in \Omega(\delta_{\RM}).
    \end{multline}
  \end{lemma}

  If $F$ is $\delta_{\RM}$-close to $\RM[\mathbb{F}, m, d]$, then by
  \cref{lem:sum-ev-close}, we have that the expected view of the verifier (more
  specifically, the non-low-degree-test portions thereof) is
  $\Omega(\delta_{\RM}) = \Omega(\delta)$-far from an accepting view. Like we saw in
  \cref{thm:pcpp-sum}, the total proportion of the queries made by this portion
  of the proof is constant. Hence, regardless of the failure rate of the
  low-degree test, the expected distance of any accepting view from the
  verifier's view is $\Omega(\delta)$.

  If $F$ is $\delta_{\RM}$-far from $\RM[\mathbb{F}, m, d]$, then since
  \cref{alg:robust-low-deg} is robust, we know that the distance between any
  accepting view and the verifier's view is in $\Omega(\delta_{\RM}) = \Omega(\delta)$. Like in the
  other case, this forms a constant fraction of the overall queries; hence the
  expected distance is $\Omega(\delta)$.
\end{proof}

\section{A zero-knowledge PCP for $\NP$ and $\NEXP$}\label{sec:pzkpcp-np-nexp}

The next step in our journey is to define any sort of zero-knowledge PCP for
both $\NP$ and $\NEXP$. We will not be defining these to have any of the query
properties we want (they will not even be in the language $\{0, 1\}$ for the
time being), but that is okay for now. For now, it is more important to show
that \emph{any} zero-knowledge PCPs exist, and then in the next section we will
show that these are reducible to zero-knowledge PCPs with the query properties
that we want.

\begin{lemma}[{\cite[Corollary 4.10]{BCFGRS17}}]\label{lem:polysim}\index{PolySim@$\PolySim$}
  There exists a probabilistic algorithm $\PolySim$ such that, for every
  \begin{enumerate}
    \item $\mathbb{F}$ a finite field,
    \item $m, d \in \mathbb{N}$,
    \item $S = \{(\alpha_{1}, \beta_{1}), \ldots, (\alpha_{\ell}, \beta_{\ell})\} \subseteq \mathbb{F}^{m} \times \mathbb{F}$,
    \item and $(\alpha, \beta) \in \mathbb{F}^{m} \times \mathbb{F}$,
  \end{enumerate}
  then
  \begin{equation}
    \mathbb{P}\mleft[\PolySim(\mathbb{F}, m, d, S, \alpha) = \beta\mright]
    = \underset{Q \in \mathbb{F}^{\le d}[X_{1, \ldots, m}]}{\mathbb{P}}\mleft[
      Q(\alpha) = \beta \middlemid
      \begin{array}{c}
        Q(\alpha_{1}) = \beta_{1} \\
        \vdots \\
        Q(\alpha_{\ell}) = \beta_{\ell}
      \end{array}
    \mright].
  \end{equation}
\end{lemma}

Finally, it is time to show an inclusion that involves $\NEXP$! First, note that
this is not remotely our desired inclusion: the language is wrong, and the
$\poly(n)$ query complexity is not very close to the $O(1)$ we actually want.
However, we are still in the home stretch---after this, we have proven every
component we need to reduce this bound down into the one we desire, and we will
do so in the next section.

Additionally, one might notice that we are starting with $\NEXP$, instead of the
much smaller class $\NP$. This is because we are going to be working with the
class $\OSAT$ for \emph{both} inclusions, and since $\OSAT$ is $\NEXP$-complete,
the inclusions are much more straightforward for $\NEXP$. Once we have done
that, however, the reductions necessary to make this work for $\NP$ will not
be too challenging.

\begin{thm}[{\cite[Theorem 6.3]{GOS25}}]\label{thm:nexp-zk-pcp}
  For any query bound $q^{*}(n) \le 2^{\poly(n)}$,
  \[
    \NEXP \subseteq
    \PZKPCP_{\Sigma(n)}\pzkpcpr{\poly(n)+\log(q^{*}(n))}{\poly(n)}{q^{*}(n)}{\varepsilon}{s}{\Omega(1)}.
  \]
  where $\Sigma(n)$ is any alphabet with $\abs{\Sigma(n)} \in \poly(n, q)$.
\end{thm}

\begin{algorithm}[htbp]
  \KwIn{A 3-CNF $B\colon \{0, 1\}^{r+3s+3} \rightarrow \{0, 1\}$}
  \KwOut{Whether $B$ is implicitly satisfiable}
  \Proof{
    Let $A\colon \{0, 1\}^{n} \rightarrow \{0, 1\}$ be a satisfying assignment for $B$\;
    Choose $\hat{C} \in \mathbb{F}[X_{m_{2} + k}^{\le 2(\abs{H} - 1)}]$ randomly such
    that $\sum_{c \in H^{k}}\hat{C}(b, c) = A(\gamma_{2}, b)$ for all
    $b \in H^{m_{2}}$\;\nllabel{line:zk-sample-c}
    \For{$\tau \in \mathbb{F}^{m_{1}+3m_{2}+3}$}{
      Let $\pi_{\tau}$ be a PZK-PCPP for the claim
      \begin{algomathdisplay}
        \sum_{\substack{z \in H^{m_{1}} \\ b_{1}, b_{2}, b_{3} \in H^{m_{2}}}}
        \sum_{\substack{a \in \{0, 1\}^{3} \\ c_{1}, c_{2}, c_{3} \in H^{k}}}
        \delta_{(z,b,a)}(\tau)h_{\hat{C}}(\tau, c_{1}, c_{2}, c_{3}) = 0
      \end{algomathdisplay}
    }
    \KwRet{$(\pi_{C}, (\pi_{\tau})_{\tau \in \mathbb{F}^{m_{1}+3m_{2}+3}})$}\;
  }
  \Verifier{
    $q_{a}, q_{b} \leftarrow 0$\;
    \Repeat{$2q_{a} > q_{b}$ and $2q_{b} > q_{a}$}{
      \Repeat{$2q_{a} > q_{b}$}{
        Run \cref{alg:robust-low-deg} on the proof, with $\varepsilon = 1/100$ and
        $\delta = 1/2$\;
        Add the total number of queries in the last line to $q_{a}$\;
        \If{the above rejects}{
          \Reject\;
        }
      }
      \Repeat{$2q_{b} > q_{a}$}{
        Choose
        $\tau \in \mathbb{F}^{m_{1}} \times (\mathbb{F}^{m_{2}})^{3} \times \mathbb{F}^{3}$ at random\;
        Simulate \cref{alg:sum-pzk-pcpp} with proof $\pi_{\tau}$\;\nllabel{line:zk-sumcheck-pi-tau}
        Add the total number of queries in the last line to $q_{b}$\;
        \For{$i \in \{1, 2, 3\}$}{
          Query $\hat{C}$ at $(\nu_{i}, \eta_{i})$\;
          Add $1$ to $q_{b}$\;
        }
        Compute $h_{\hat{C}}(\tau, \eta_{1}, \eta_{2}, \eta_{3})$\;
        \If{the claim in line~\ref{line:zk-sumcheck-pi-tau} is false}{
          \Reject\;
        }
      }
    }
    \Accept\;
  }
  \caption{A $\PZKPCP$ for $\OSAT$~\cite[Construction 6.4]{GOS25}}\label{alg:pzkpcp-osat}
\end{algorithm}

\begin{algorithm}[htbp]
  \KwIn{A query $\alpha$ to an oracle $O$, either $\pi_{C}$ or $\pi_{\tau}$}
  \KwOut{The result of the query}
  $S \leftarrow \varnothing$\tcp*[r]{$S$ is the set of all previous queries to $\pi_{C}$}
  $T \leftarrow \varnothing$\tcp*[r]{$T$ is the set of all started $\Sim_{\tau}'$ instances}
  \eIf{The query request is to $\pi_{C}$}{
    Sample $\beta \leftarrow \PolySim(\mathbb{F}, m+k, d, S, q)$\;\nllabel{line:query-polysim}
    Add $(\alpha, \beta)$ to $S$\;
    Respond with $\beta$\;
  }{
    \If(\tcp*[f]{When we have not queried this $\tau$ before}){$\tau \notin T$}{
      Add $\tau$ to $T$\;
      Start an instance $\Sim'_{\tau}$ of a simulator for \cref{alg:sum-pzk-pcpp}\;
    }
    Use $\Sim'_{\tau}$ to answer $\alpha$: it may make queries to $\tau$ by asking
    $\pi_{C}$ (answered as in line~\ref{line:query-polysim})\;
  }
  \caption{A simulator for \cref{alg:pzkpcp-osat}~\cite[Construction 6.7]{GOS25}}\label{alg:pcp-osat-sim}
\end{algorithm}

\begin{proof}
  We construct a PZK-PCP for $\OSAT$, a $\NEXP$-complete language, in
  \cref{alg:pzkpcp-osat}. First, note that the combined effect of the three
  ``repeat'' loops is to ensure that the number of queries taken up by each of
  the two inner portions is no more than $1/3$ of the total number of queries.
  This will be useful to us later on when we try to prove soundness.

  We note that \cref{alg:pzkpcp-osat} is exactly identical to
  \cref{alg:pcp-osat}, but where the sumcheck call has been replaced with the
  zero-knowledge sumcheck PCPP we constructed in \cref{alg:sum-pzk-pcpp}. Since
  the zero-knowledge algorithm has the same completeness and soundness
  properties as the nonzero-knowledge algorithm, completeness and soundness hold
  by the same argument we made in \cref{thm:nexp-pcp}.

  Hence, all we need is to show \cref{alg:pcp-osat-sim} is a simulator for
  \cref{alg:pzkpcp-osat} and thus, that it is zero-knowledge. For this, we will use a
  hybrid argument: we will start by constructing a simulator that uses an
  external oracle (and thus is not technically a valid simulator for our
  purposes) and reduce it until we get to a simulator that does not query an
  oracle, proving an equivalence at each step along the way. In the intermediate
  steps, our oracle $Z$ replaces the queries to $\pi_{C}$.

  % NOTE: This is backwards from the original paper since I think it follows
  % more closely that way
  Our sequence of algorithms is as follows:
  \begin{enumerate}
    \item $H_{0}$: $\View_{V^{*}, P}(x)$
    \item $H_{1}$: $\overline{\Sim}^{V^{*}, Z}(x)$, where
          $Z\colon \mathbb{F}^{m_{2}+k}$ is sampled as $\hat{C}$ in
          line~\ref{line:zk-sample-c} in \cref{alg:pzkpcp-osat}
    \item $H_{2}$: $\overline{\Sim}^{V^{*}, Z}(x)$, where
          $Z\colon \mathbb{F}^{m_{2}+k}$ is a uniformly random polynomial of
          multidegree $\abs{H} - 1$
    \item $H_{4}$: $\Sim^{V^{*}}(x)$ (\cref{alg:pcp-osat-sim})
  \end{enumerate}

  To show $H_{0} \equiv H_{1}$, we use the zero-knowledge guarantee of
  \cref{alg:sum-pzk-pcpp}. As per \cref{thm:pcpp-sum-pzk},
  \cref{alg:sum-pzk-pcpp} has an efficient simulator that outputs an identical
  distribution to the view of the verifier. Hence, our $\Sim'_{\tau}$ will output
  a response statistically identical to the verifier's query of $\tau$.

  To show $H_{1} \equiv H_{2}$, we rely on \cref{cor:indep-ensemble} proven earlier.
  From earlier, we know $V^{*}$ makes at most $q^{*}$ queries to the proof, and
  thus $\overline{\Sim}$ makes at most $p(q^{*})$ queries to $Z$. Since
  $p(q^{*}) \le \abs{H}^{k}$, these query answers will be independent from
  $\mleft(\sum_{y \in H^{k}}Z(\alpha, y)\mright)_{\alpha \in \mathbb{F}^{m}}$ as per
  \cref{cor:indep-ensemble}. As such, for all $v$,
  \begin{align*}
    \mathbb{P}[v \leftarrow H_{2}(x)] &= \underset{Z}{\mathbb{P}}\mleft[
                        v \leftarrow \overline{\Sim}^{V^{*}, Z}(x) \middlemid
                        \forall b \in H^{m_{2}},\,\sum_{c \in H^{k}}\hat{C}(b, c) = A(\gamma_{2}(b))
                      \mright] \\
                    &= \underset{Z}{\mathbb{P}}\mleft[
                        v \leftarrow \overline{\Sim}^{V^{*}, Z}(x)
                      \mright] \\
                    &= \mathbb{P}[v \leftarrow H_{1}(x)].
  \end{align*}

  To show $H_{2} \equiv H_{3}$, we use the property of $\PolySim$: the output of the
  algorithm is distributed identically to output of a random polynomial $Z$
  conditional on all the previous results being true about $Z$. Hence, the
  output of $H_{2}$ must be identically distributed to $H_{3}$.

  Since equivalence is transitive, it follows therefore that $H_{0} \equiv H_{3}$ and
  thus the view of $V^{*}$ is always identical to the output of
  \cref{alg:pcp-osat-sim}.
\end{proof}

\begin{cor}[{\cite[Theorem 6.3]{GOS25}}]\label{cor:np-zk-pcp}
  For any query bound $q^{*}(n) \le 2^{\poly(n)}$,
  \[
    \NP \subseteq
    \PZKPCP_{\Sigma(n)}\pzkpcpr{\log(n)+\log(q^{*}(n))}{\poly(\log(n) + \log(q^{*}(n)))}{q^{*}(n)}{\varepsilon}{s}{\Omega(1)}.
  \]
  where $\abs{\Sigma(n)} = \poly(n, q)$.
\end{cor}

\begin{proof}
  For this, we leverage our work from \cref{thm:nexp-zk-pcp}. In that, we used
  the fact that $\OSAT$ is a $\NEXP$-complete problem. However, the Cook-Levin
  variant we proved in \cref{thm:cook-levin-general} is even more general than
  that: in particular it showed that log-length $\OSAT$ is in fact
  $\NP$-complete. Hence, if we adjust the length of our input, the inclusion
  here follows.
\end{proof}

\subsection{Reducing query complexity to constant}\label{sec:constant-pcp-np}

We are now finally able to present our analogues to the classical $\PCP$
theorem. Through our work with $\OSAT$, we do not only have a version of the
standard $\PCP$ theorem (the one involving $\NP$), but another stronger one
involving $\PCP$'s relationship with $\NEXP$.

\begin{thm}[{\cite[Theorem 2]{GOS25}}]\label{thm:zk-pcp}\index{PCP theorem@$\PCP$ theorem!zero-knowledge}
  $\NP \subseteq \PZKPCP[\log(n), 1]$.
\end{thm}

\begin{proof}
  To show this, we will take the PCP for $\NP$ that we showed in
  \cref{cor:np-zk-pcp}, which has relatively weak bounds and an arbitrary
  alphabet, and transform it into one with the exact parameters we seek. We do
  this first through an alphabet reduction (as per \cref{thm:alph-reduction})
  and then through proof-composing it with the algorithm for $\CktVal$ with
  \cref{cor:pzk-constant-query}, we can get a constant query-complexity PCP\@.
  Since several of these class inclusions involve modifying a small number of
  parameters in a relatively complex class, we will be highlighting the changed
  parameters in each inclusion statement.

  Let $q^{*}(n) \le 2^{\poly(n)}$ be arbitrary. This will be the query complexity
  of our final PCP after we do all the class inclusions. As per
  \cref{cor:np-zk-pcp}, we know that
  \begin{equation}\label{eqn:np-subseteq-pzkpcp}
    \NP \subseteq
    \PZKPCP_{\Sigma(n)}\pzkpcpr{\log(n)+\log(\tilde{q}(n))}{\poly(\log(n) + \log(\tilde{q}(n)))}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)}.
  \end{equation}
  for any $\tilde{q} \le 2^{\poly(n)}$ and where
  $\abs{\Sigma(n)} \in \poly(n, \tilde{q})$. \Cref{cor:np-zk-pcp} only guarantees this
  inclusion for alphabets of $\{0, 1\}^{a}$, however a PCP over $\Sigma(n)$ is
  equivalent to a PCP over $\{0, 1\}^{\log_{2}(\abs{\Sigma(n)})}$ by a simple
  relabeling of alphabet items, so this inclusion still holds.

  Next, we perform an alphabet reduction: by \cref{thm:zk-alph-red},
  \begin{equation}\label{eqn:alph-red-np}
    \begin{aligned}
      \PZKPCP_{\color{pumpkin}{\Sigma(n)}}&\pzkpcpr{\log(n)+\log(\tilde{q}(n))}{\poly(\log(n)
                             + \log(\tilde{q}(n)))}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)} \\
      \subseteq \PZKPCP_{\color{pumpkin}{\{0,1\}}}&\pzkpcpr{\log(n)+\log(\tilde{q}(n))}{\poly(\log(n)
                                  + \log(\tilde{q}(n)))}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)}.
    \end{aligned}
  \end{equation}

  Next, we perform proof composition. For any language $L \in \NP$, let
  $Q(n) \in \poly(\log(n) + \log(\tilde{q}(n)))$ be the query complexity of the
  PZK-PCP within the parameters of the right-hand side of \cref{eqn:alph-red-np}
  that recognizes $L$. Since $\tilde{q}$ was arbitrary, we can define it to be
  whatever we like; hence let $\tilde{q}(n)$ be a polynomial in $q^{*}$ and $n$
  large enough that for all $n \in \mathbb{N}$, $\tilde{q}(n)/Q(n) \ge q^{*}(n)$. By
  \cref{cor:pzk-constant-query}, we have that
  \begin{equation}\label{eqn:proof-comb-np}
    \begin{aligned}
      \PZKPCP_{\{0,1\}}&\pzkpcpr{\log(n)+\color{pumpkin}{\log(\tilde{q}(n))}}{\color{pumpkin}{\poly(\log(n)
                         + \log(\tilde{q}(n)))}}{\color{pumpkin}{\tilde{q}(n)}}{\varepsilon}{\color{plum}{s}}{\color{plum}{\Omega(1)}} \\
      \subseteq \PZKPCP_{\{0,1\}}&\pzkpcp{\log(n)+\color{pumpkin}{\log(q^{*}(n))}}{\color{pumpkin}{1}}{\color{pumpkin}{q^{*}(n)}}{\varepsilon}.
    \end{aligned}
  \end{equation}

  At the beginning of the proof, we said $q^{*}$ was arbitrary; hence we can set
  $q^{*} \in O(1)$. Thus, we get
  \begin{equation}\label{eqn:qstar-arbitrary-np}
      \PZKPCP_{\{0,1\}}\pzkpcp{\log(n)+\log(q^{*}(n))}{1}{q^{*}(n)}{\varepsilon} \subseteq \PZKPCP[\log(n), 1].
  \end{equation}
  By combining the inclusions in
  \crefrange{eqn:np-subseteq-pzkpcp}{eqn:qstar-arbitrary-np}, we get that
  $\NP \subseteq \PZKPCP[\log(n), 1]$, as desired.
\end{proof}

\begin{thm}[{\cite[Theorem 7.1]{GOS25}}]\label{thm:zk-pcp-nexp}
  $\NEXP \subseteq \PZKPCP[\poly(n), 1]$.
\end{thm}

\begin{proof}
  Broadly speaking, this proof will proceed in the same style as the proof for
  \cref{thm:zk-pcp}. Let $q^{*} \le 2^{\poly(n)}$ be arbitrary. This will be the
  query complexity of our final PCP after we do all the class inclusions. As per
  \cref{thm:nexp-zk-pcp}, we know that
  \begin{equation}\label{eqn:nexp-subseteq-pzkpcp}
    \NEXP \subseteq \PZKPCP_{\Sigma(n)}\pzkpcpr{\poly(n)+\log(\tilde{q}(n))}{\poly(n)}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)}.
  \end{equation}
  for any $\tilde{q} \le 2^{\poly(n)}$ and where
  $\abs{\Sigma(n)} \in \poly(n, \tilde{q})$. As before, \cref{cor:np-zk-pcp} only
  guarantees this inclusion for alphabets of $\{0, 1\}^{a}$, however a PCP over
  $\Sigma(n)$ is equivalent to a PCP over $\{0, 1\}^{\log_{2}(\abs{\Sigma(n)})}$ by a
  simple relabeling of alphabet items, so this inclusion still holds.

  Next, we perform an alphabet reduction: by \cref{thm:zk-alph-red},
  \begin{equation}\label{eqn:alph-red-nexp}
    \begin{aligned}
      \PZKPCP_{\color{pumpkin}{\Sigma(n)}}&\pzkpcpr{\poly(n)+\log(\tilde{q}(n))}{\color{pumpkin}{\poly(n)}}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)} \\
      \subseteq \PZKPCP_{\color{pumpkin}{\{0,1\}}}&\pzkpcpr{\poly(n)+\log(\tilde{q}(n))}{\color{pumpkin}{\poly(n,\log(\tilde{q}(n)))}}{\tilde{q}(n)}{\varepsilon}{s}{\Omega(1)}.
    \end{aligned}
  \end{equation}

  Next, we perform proof composition. For any language $L \in \NP$, let
  $Q(n) \in \poly(n)$ be the query complexity of the PZK-PCP within the parameters
  of the right-hand side of \cref{eqn:alph-red-np} that recognizes $L$. Since
  $\tilde{q}$ was arbitrary, we can define it to be whatever we like; hence let
  $\tilde{q}(n) = q^{*}(n) \cdot Q(n)$ for all $n \in \mathbb{N}$. By
  \cref{cor:pzk-constant-query}, we have that
  \begin{equation}\label{eqn:proof-comb-nexp}
    \begin{aligned}
      \PZKPCP_{\{0,1\}}&\pzkpcpr{\color{pumpkin}{\poly(n)}}{\color{pumpkin}{\poly(n,
                         \log(\tilde{q}(n)))}}{\color{pumpkin}{\tilde{q}(n)}}{\varepsilon
                         }{\color{plum}{s}}{\color{plum}{\Omega(1)}} \\
      \subseteq \PZKPCP_{\{0,1\}}&\pzkpcp{\color{pumpkin}{\poly(n) + \log(q^{*}(n))}}{\color{pumpkin}{1}
                           }{\color{pumpkin}{q^{*}(n)}}{\varepsilon}.
    \end{aligned}
  \end{equation}

  At the beginning of the proof, we said $q^{*}$ was arbitrary; hence we can set
  $q^{*} \in O(1)$. Thus, we get
  \begin{equation}\label{eqn:qstar-arbitrary-nexp}
      \PZKPCP_{\{0,1\}}\pzkpcp{\poly(n)+\log(q^{*}(n))}{1}{q^{*}(n)}{\varepsilon} \subseteq \PZKPCP[\poly(n), 1].
  \end{equation}
  By combining the inclusions in
  \crefrange{eqn:nexp-subseteq-pzkpcp}{eqn:qstar-arbitrary-nexp}, we get that
  $\NEXP \subseteq \PZKPCP[\poly(n), 1]$, as desired.
\end{proof}

\begin{appendices}

\chapter{More on extension polynomials}\label[appendix]{app:ext-poly}

In this appendix, we will work through some of the algebra we mentioned but did
not go into detail about in \cref{sec:polynomial}.

\section{A proof of \cref{eqn:delta-is-delta}}\label{sec:delta-is-delta}

Our goal is to demonstrate the following:
\begin{equation}\label{eqn:delta-is-iverson}
  [x = y] = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
    \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
  \mright)\mright)
\end{equation}
for all $x, y \in H^{n}$. We will do this in two cases: one where $x = y$ and one
where $x \ne y$.

First, assume $x \ne y$ (so we want to show $\delta_{y}(x) = 0$). In this case, there
exists at least one $i$ where $x_{i} \ne y_{i}$. For this $i$, for each $\omega$ there
exists some $\gamma \in H \setminus \{\omega\}$ such that either $x_{i} = \gamma$ or
$y_{i} = \gamma$.\footnote{This piece fails in the case where $x_{i} = y_{i}$, since
  if $\omega = x_{i} = y_{i}$ neither of the terms will ever be zero.} As
such, it follows that either $(x_{i} - \gamma) = 0$ or $(\gamma_{i} - \gamma) = 0$. Hence, for
this $i$ the sum will be entirely over zero terms (since there will be at least
one zero term in the product for each $\omega$). As such, this means that the $i$th
term of our outermost product is $0$, and hence the entire product is $0$, as
desired.

When $x = y$ (and so we want to show $\delta_{y}(x) = 1$), the above equation
simplifies to
\begin{equation}
  [x = y] = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
    \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)^{2}}{(\omega - \gamma)^{2}}
  \mright)\mright)
\end{equation}
Whenever $\omega \ne x_{i}$, the innermost product becomes $0$ since there will be a
term where $\gamma = x_{i}$. Hence, we can simplify this further to
\begin{equation}
  [x = y] = \prod_{i=1}^{m}\mleft(\prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)^{2}}{(x_{i} - \gamma)^{2}}\mright).
\end{equation}
Since $\gamma \ne x_{i}$, we can simplify the fraction to $1$; since we have two nested
products it follows that the equation as a whole simplifies to $1$.

\section{Algebra behind \cref{eqn:delta-poly-small}}\label{sec:delta-poly-small}

Our goal is to show that the equation in \cref{eqn:delta-poly} simplifies to
that of \cref{eqn:delta-poly-small} when $H = \{0, 1\}^{n}$.

As a refresher, our starting equation has the form
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft(\sum_{\omega \in H}\mleft(
  \prod_{\gamma \in H \setminus \{\omega\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
  \mright)\mright).
\end{equation}
We start by manually substituting the outer sum:
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft(\mleft(
    \prod_{\gamma \in \{0, 1\} \setminus \{0\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
    \mright) + \mleft(
    \prod_{\gamma \in \{0, 1\} \setminus \{1\}}\frac{(x_{i} - \gamma)(y_{i} - \gamma)}{(\omega - \gamma)^{2}}
    \mright)
  \mright).
\end{equation}
Next, notice that the inner products are actually each over one term, so we can
manually substitute there:
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft(
    \frac{(x_{i} - 1)(y_{i} - 1)}{(0 - 1)^{2}} +
    \frac{(x_{i} - 0)(y_{i} - 0)}{(1 - 0)^{2}}
  \mright).
\end{equation}
Next, we simplify, taking note that the denominator of both fractions is $1$:
\begin{equation}
  \delta_{y}(x) = \prod_{i = 1}^{m}\mleft((x_{i} - 1)(y_{i} - 1) + x_{i}y_{i}\mright).
\end{equation}
From here, we take advantage of the fact that $y \in \{0, 1\}^{n}$; here we split
our product into two smaller products: one where $y_{i} = 0$ and one where
$y_{i} = 1$.
\begin{equation}
  \delta_{y}(x) = \mleft(\prod_{i:y_{i}=0}(x_{i} - 1)(0 - 1) + 0x_{i}\mright)
  \mleft(\prod_{i:y_{i}=1}(x_{i} - 1)(1 - 1) + x_{i}1\mright).
\end{equation}
Finally, we simplify, bringing us to \cref{eqn:delta-poly-small}.
\begin{equation}
  \delta_{y}(x) = \mleft(\prod_{i:y_{i}=0}(1 - x_{i})\mright)\mleft(\prod_{i:y_{i}=1}x_{i}\mright).
\end{equation}

\end{appendices}

\printbibliography[heading=bibintoc]{}

\printindex{}

\end{document}
