@article{AW09,
  author = {Aaronson, Scott and Wigderson, Avi},
  title = {Algebrization: A New Barrier in Complexity Theory},
  year = {2009},
  issue_date = {February 2009},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {1},
  number = {1},
  issn = {1942-3454},
  doi = {10.1145/1490270.1490272},
  abstract = {Any proof of P ≠ NP will have to overcome two barriers: relativization and natural proofs. Yet over the last decade, we have seen circuit lower bounds (e.g., that PP does not have linear-size circuits) that overcome both barriers simultaneously. So the question arises of whether there is a third barrier to progress on the central questions in complexity theory.In this article, we present such a barrier, which we call algebraic relativization or algebrization. The idea is that, when we relativize some complexity class inclusion, we should give the simulating machine access not only to an oracle A, but also to a low-degree extension of A over a finite field or ring.We systematically go through basic results and open problems in complexity theory to delineate the power of the new algebrization barrier. First, we show that all known nonrelativizing results based on arithmetization---both inclusions such as IP = PSPACE and MIP = NEXP, and separations such as MAEXP ⊄ P/poly---do indeed algebrize. Second, we show that almost all of the major open problems---including P versus NP, P versus RP, and NEXP versus P/poly---will require non-algebrizing techniques. In some cases, algebrization seems to explain exactly why progress stopped where it did: for example, why we have superlinear circuit lower bounds for PromiseMA but not for NP.Our second set of results follows from lower bounds in a new model of algebraic query complexity, which we introduce in this article and which is interesting in its own right. Some of our lower bounds use direct combinatorial and algebraic arguments, while others stem from a surprising connection between our model and communication complexity. Using this connection, we are also able to give an MA-protocol for the Inner Product function with O (√nlogn) communication (essentially matching a lower bound of Klauck), as well as a communication complexity conjecture whose truth would imply NL ≠ NP.},
  journal = {ACM Trans. Comput. Theory},
  month = {2},
  articleno = {2},
  numpages = {54},
  keywords = {query complexity, oracles, interactive proofs, communication complexity, arithmetization, Low-degree polynomials},
  annotate = {An introduction to algebrization and complexity theory.},
}

@book{AB09,
  author = {Arora, Sanjeev and Barak, Boaz},
  title = {Computational Complexity: A Modern Approach},
  year = {2009},
  isbn = {978-0-521-42426-4},
  publisher = {Cambridge University Press},
  address = {USA},
  edition = {1st},
  abstract = {This beginning graduate textbook describes both recent achievements and classical results of computational complexity theory. Requiring essentially no background apart from mathematical maturity, the book can be used as a reference for self-study for anyone interested in complexity, including physicists, mathematicians, and other scientists, as well as a textbook for a variety of courses and seminars. More than 300 exercises are included with a selected hint set.},
  doi = {10.5555/1540612},
}

% https://www.scottaaronson.com/papers/pnp.pdf
@Inbook{Aar16,
  author={Aaronson, Scott},
  editor={Nash, Jr., John Forbes and Rassias, Michael Th.},
  title={{$P \mathop{=}\limits^{?} NP$}},
  bookTitle={Open Problems in Mathematics},
  year={2016},
  publisher={Springer International Publishing},
  address={Cham},
  pages={1--122},
  abstract={In 1950, John Nash sent a remarkable letter to the National Security Agency, in which---seeking to build theoretical foundations for cryptography---he all but formulated what today we call the {\$}{\$}{\backslash}mathsf{\{}P{\}}{\backslash}mathop{\{} ={\}}{\backslash}limits^{\{}?{\}}{\backslash}mathsf{\{}NP{\}}{\$}{\$}problem, and consider one of the great open problems of science. Here I survey the status of this problem in 2016, for a broad audience of mathematicians, scientists, and engineers. I offer a personal perspective on what it's about, why it's important, why it's reasonable to conjecture that P ≠ NP is both true and provable, why proving it is so hard, the landscape of related problems, and crucially, what progress has been made in the last half-century toward solving those problems. The discussion of progress includes diagonalization and circuit lower bounds; the relativization, algebrization, and natural proofs barriers; and the recent works of Ryan Williams and Ketan Mulmuley, which (in different ways) hint at a duality between impossibility proofs and algorithms.},
  isbn={978-3-319-32162-2},
  doi={10.1007/978-3-319-32162-2_1},
  url={https://link.springer.com/chapter/10.1007/978-3-319-32162-2_1},
}

@article{BGS75,
  author = {Baker, Theodore and Gill, John and Solovay, Robert},
  title = {Relativizations of the {$\mathcal{P} \overset{?}{=} \mathcal{NP}$} Question},
  journal = {SIAM Journal on Computing},
  volume = {4},
  number = {4},
  pages = {431-442},
  year = {1975},
  doi = {10.1137/0204037},
  URL = {https://doi.org/10.1137/0204037},
  eprint = {https://doi.org/10.1137/0204037},
  abstract = {We investigate relativized versions of the open question of whether every language accepted nondeterministically in polynomial time can be recognized deterministically in polynomial time. For any set X, let \$\mathcal{P}^X (\text{resp. }\mathcal{NP}^X )\$ be the class of languages accepted in polynomial time by deterministic (resp. nondeterministic) query machines with oracle X. We construct a recursive set A such that \$\mathcal{P}^A = \mathcal{NP}^A \$. On the other hand, we construct a recursive set B such that \$\mathcal{P}^B \ne \mathcal{NP}^B \$. Oracles X are constructed to realize all consistent set inclusion relations between the relativized classes \$\mathcal{P}^X \$, \$\mathcal{NP}^X \$, and co \$\mathcal{NP}^X \$, the family of complements of languages in \$\mathcal{NP}^X \$. Several related open problems are described. }
}

@article{LFKN92,
  author = {Lund, Carsten and Fortnow, Lance and Karloff, Howard and Nisan, Noam},
  title = {Algebraic methods for interactive proof systems},
  year = {1992},
  issue_date = {Oct. 1992},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {39},
  number = {4},
  issn = {0004-5411},
  url = {https://doi.org/10.1145/146585.146605},
  doi = {10.1145/146585.146605},
  abstract = {A new algebraic technique for the construction of interactive proof systems is presented. Our technique is used to prove that every language in the polynomial-time hierarchy has an interactive proof system. This technique played a pivotal role in the recent proofs that IP = PSPACE [28] and that MIP = NEXP [4].},
  journal = {J. ACM},
  month = oct,
  pages = {859–868},
  numpages = {10},
  keywords = {interactive proof systems}
}

@article{CFGS22,
  author = {Chiesa, Alessandro and Forbes, Michael A. and Gur, Tom and Spooner, Nicholas},
  title = {Spatial Isolation Implies Zero Knowledge Even in a Quantum World},
  year = {2022},
  issue_date = {April 2022},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {69},
  number = {2},
  issn = {0004-5411},
  url = {https://doi.org/10.1145/3511100},
  doi = {10.1145/3511100},
  abstract = {Zero knowledge plays a central role in cryptography and complexity. The seminal work of Ben-Or et al. (STOC 1988) shows that zero knowledge can be achieved unconditionally for any language in NEXP, as long as one is willing to make a suitable physical assumption: if the provers are spatially isolated, then they can be assumed to be playing independent strategies.Quantum mechanics, however, tells us that this assumption is unrealistic, because spatially-isolated provers could share a quantum entangled state and realize a non-local correlated strategy. The MIP* model captures this setting.In this work, we study the following question: Does spatial isolation still suffice to unconditionally achieve zero knowledge even in the presence of quantum entanglement?We answer this question in the affirmative: we prove that every language in NEXP has a 2-prover zero knowledge interactive proof that is sound against entangled provers; that is, NEXP ⊆ ZK-MIP*.Our proof consists of constructing a zero knowledge interactive probabilistically checkable proof with a strong algebraic structure, and then lifting it to the MIP* model. This lifting relies on a new framework that builds on recent advances in low-degree testing against entangled strategies, and clearly separates classical and quantum tools.Our main technical contribution is the development of new algebraic techniques for obtaining unconditional zero knowledge; this includes a zero knowledge variant of the celebrated sumcheck protocol, a key building block in many probabilistic proof systems. A core component of our sumcheck protocol is a new algebraic commitment scheme, whose analysis relies on algebraic complexity theory.},
  journal = {J. ACM},
  month = {1},
  articleno = {15},
  numpages = {44},
  keywords = {algebraic complexity, sumcheck protocol, interactive PCPs, quantum entangled strategies, multi-prover interactive proofs, Zero knowledge}
}

@book{CLRS,
  title = {Introduction to Algorithms},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
  edition = {4},
  isbn = {978-0-262-04630-5},
  lccn = {2021037260},
  year = {2022},
  publisher = {MIT Press},
  url = {https://mitpress.mit.edu/9780262046305/introduction-to-algorithms},
}

@book{Sip97,
  author = {Sipser, Michael},
  title = {Introduction to the Theory of Computation},
  year = {1996},
  isbn = {978-0-534-94728-6},
  publisher = {International Thomson Publishing},
  doi = {10.1145/230514.571645},
  edition = {1},
  pagetotal = {396},
}

@article{Sav70,
  title = {Relationships between nondeterministic and deterministic tape complexities},
  journal = {Journal of Computer and System Sciences},
  volume = {4},
  number = {2},
  pages = {177-192},
  year = {1970},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(70)80006-X},
  url = {https://www.sciencedirect.com/science/article/pii/S002200007080006X},
  author = {Walter J. Savitch},
  abstract = {The amount of storage needed to simulate a nondeterministic tape bounded Turingmachine on a deterministic Turing machine is investigated. Results include the following: Theorem. A nondeterministic L(n)-tape bounded Turing machine can be simulated by a deterministic [L(n)]2-tape bounded Turing machine, provided L(n)≥log2n. Computations of nondeterministic machines are shown to correspond to threadings of certain mazes. This correspondence is used to produce a specific set, namely the set of all codings of threadable mazes, such that, if there is any set which distinguishes nondeterministic tape complexity classes from deterministic tape complexity classes, then this is one such set.}
}

@inproceedings{BFL90,
  author={Babai, L. and Fortnow, L. and Lund, C.},
  booktitle={Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science},
  title={Nondeterministic exponential time has two-prover interactive protocols},
  year={1990},
  volume={},
  number={},
  pages={16-25 vol.1},
  keywords={Protocols;Polynomials;Upper bound;Joining processes;Circuits;Extrapolation;Cryptography;Graphics},
  doi={10.1109/FSCS.1990.89520}
}

@article{JKRS09,
  author = {Juma, Ali and Kabanets, Valentine and Rackoff, Charles and Shpilka, Amir},
  title = {The Black-Box Query Complexity of Polynomial Summation},
  year = {2009},
  issue_date = {April 2009},
  publisher = {Birkhauser Verlag},
  address = {CHE},
  volume = {18},
  number = {1},
  issn = {1016-3328},
  url = {https://doi.org/10.1007/s00037-009-0263-7},
  doi = {10.1007/s00037-009-0263-7},
  journal = {Comput. Complex.},
  month = apr,
  pages = {59–79},
  numpages = {21},
  keywords = {68Q25, 68Q17, 68Q15}
}

@book{Ru76,
  author = {Rudin, Walter},
  title = {Principles of Mathematical Analysis},
  year = {1976},
  publisher = {McGraw-Hill},
  isbn = {978-0-07-085613-4},
  pagetotal = {342},
  edition = {3},
}

@inproceedings{Cook71,
  author = {Cook, Stephen A.},
  title = {The complexity of theorem-proving procedures},
  year = {1971},
  isbn = {9781450374644},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/800157.805047},
  doi = {10.1145/800157.805047},
  abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
  booktitle = {Proceedings of the Third Annual ACM Symposium on Theory of Computing},
  pages = {151–158},
  numpages = {8},
  location = {Shaker Heights, Ohio, USA},
  series = {STOC '71}
}

@inproceedings{SM73,
  author = {Stockmeyer, L. J. and Meyer, A. R.},
  title = {Word problems requiring exponential time},
  year = {1973},
  isbn = {9781450374309},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/800125.804029},
  doi = {10.1145/800125.804029},
  abstract = {The equivalence problem for Kleene's regular expressions has several effective solutions, all of which are computationally inefficient. In [1], we showed that this inefficiency is an inherent property of the problem by showing that the problem of membership in any arbitrary context-sensitive language was easily reducible to the equivalence problem for regular expressions. We also showed that with a squaring abbreviation ( writing (E)2 for E\texttimes{}E) the equivalence problem for expressions required computing space exponential in the size of the expressions.In this paper we consider a number of similar decidable word problems from automata theory and logic whose inherent computational complexity can be precisely characterized in terms of time or space requirements on deterministic or nondeterministic Turing machines. The definitions of the word problems and a table summarizing their complexity appears in the next section. More detailed comments and an outline of some of the proofs follows in the remaining sections. Complete proofs will appear in the forthcoming papers [9, 10, 13]. In the final section we describe some open problems.},
  booktitle = {Proceedings of the Fifth Annual ACM Symposium on Theory of Computing},
  pages = {1–9},
  numpages = {9},
  location = {Austin, Texas, USA},
  series = {STOC '73}
}

@article{CKS81,
  author = {Chandra, Ashok K. and Kozen, Dexter C. and Stockmeyer, Larry J.},
  title = {Alternation},
  year = {1981},
  issue_date = {Jan. 1981},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {28},
  number = {1},
  issn = {0004-5411},
  url = {https://dl.acm.org/doi/10.1145/322234.322243},
  doi = {10.1145/322234.322243},
  journal = {J. ACM},
  month = jan,
  pages = {114–133},
  numpages = {20},
}

@inproceedings{MS24,
  author = {Mastel, Kieran and Slofstra, William},
  title = {Two Prover Perfect Zero Knowledge for MIP*},
  year = {2024},
  isbn = {9798400703836},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3618260.3649702},
  doi = {10.1145/3618260.3649702},
  abstract = {The recent MIP*=RE theorem of Ji, Natarajan, Vidick, Wright, and Yuen shows that the complexity class MIP* of multiprover proof systems with entangled provers contains all recursively enumerable languages. Prior work of Grilo, Slofstra, and Yuen [FOCS '19] further shows (via a technique called simulatable codes) that every language in MIP* has a perfect zero knowledge (PZK) MIP* protocol. The MIP*=RE theorem uses two-prover one-round proof systems, and hence such systems are complete for MIP*. However, the construction in Grilo, Slofstra, and Yuen uses six provers, and there is no obvious way to get perfect zero knowledge with two provers via simulatable codes. This leads to a natural question: are there two-prover PZK-MIP* protocols for all of MIP*? In this paper, we show that every language in MIP* has a two-prover one-round PZK-MIP* protocol, answering the question in the affirmative. For the proof, we use a new method based on a key consequence of the MIP*=RE theorem, which is that every MIP* protocol can be turned into a family of boolean constraint system (BCS) nonlocal games. This makes it possible to work with MIP* protocols as boolean constraint systems, and in particular allows us to use a variant of a construction due to Dwork, Feige, Kilian, Naor, and Safra [Crypto '92] which gives a classical MIP protocol for 3SAT with perfect zero knowledge. To show quantum soundness of this classical construction, we develop a toolkit for analyzing quantum soundness of reductions between BCS games, which we expect to be useful more broadly. This toolkit also applies to commuting operator strategies, and our argument shows that every language with a commuting operator BCS protocol has a two prover PZK commuting operator protocol.},
  booktitle = {Proceedings of the 56th Annual ACM Symposium on Theory of Computing},
  pages = {991–1002},
  numpages = {12},
  keywords = {MIP*, interactive proofs, nonlocal games, perfect zero knowledge},
  location = {Vancouver, BC, Canada},
  series = {STOC 2024},
}

@inproceedings{GOS24,
  author = {Gur, Tom and O'Connor, Jack and Spooner, Nicholas},
  title = {Perfect Zero-Knowledge PCPs for {\#P}},
  year = {2024},
  isbn = {9798400703836},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3618260.3649698},
  doi = {10.1145/3618260.3649698},
  abstract = {We construct perfect zero-knowledge probabilistically checkable proofs (PZK-PCPs) for every language in {\#P}. This is the first construction of a PZK-PCP for any language outside BPP. Furthermore, unlike previous constructions of (statistical) zero-knowledge PCPs, our construction simultaneously achieves non-adaptivity and zero knowledge against arbitrary (adaptive) polynomial-time malicious verifiers. Our construction consists of a novel masked sumcheck PCP, which uses the combinatorial nullstellensatz to obtain antisymmetric structure within the hypercube and randomness outside of it. To prove zero knowledge, we introduce the notion of locally simulatable encodings: randomised encodings in which every local view of the encoding can be efficiently sampled given a local view of the message. We show that the code arising from the sumcheck protocol (the Reed-Muller code augmented with subcube sums) admits a locally simulatable encoding. This reduces the algebraic problem of simulating our masked sumcheck to a combinatorial property of antisymmetric functions.},
  booktitle = {Proceedings of the 56th Annual ACM Symposium on Theory of Computing},
  pages = {1724-1730},
  numpages = {7},
  keywords = {Coding Theory, Computational Complexity, Cryptography},
  location = {Vancouver, BC, Canada},
  series = {STOC 2024},
}

@book{APL,
  author = {Iverson, Kenneth E.},
  title = {A Programming Language},
  year = {1962},
  isbn = {978-0471430148},
  publisher = {John Wiley and Sons, Inc.},
  pagetotal = {286},
  url = {https://dl.acm.org/doi/10.5555/1098666},
  edition = {1},
}

@article{Knu92,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2325085},
 author = {Donald E. Knuth},
 journal = {The American Mathematical Monthly},
 number = {5},
 pages = {403--422},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Two Notes on Notation},
 urldate = {2024-11-19},
 volume = {99},
 year = {1992}
}

@Article{Tur36,
  title = {On Computable Numbers, with an Application to the Entscheidungsproblem},
  author = {Turing, A. M.},
  journal = {Proceedings of The London Mathematical Society},
  year = {1936},
  publisher = {Oxford University Press},
  volume = {42},
  pages = {230-265},
  number = {1},
  doi = {10.1112/PLMS/S2-42.1.230},
}

@book{Go01,
  author = {Goldreich, Oded},
  maintitle = {Foundations of Cryptography},
  year = {2001},
  isbn = {978-0-511-54689-1},
  publisher = {Cambridge University Press},
  volume = {1},
  edition = {1},
  pagetotal = {372},
  doi = {10.1017/CBO9780511546891},
}

@article{JNVWY21,
  author = {Ji, Zhengfeng and Natarajan, Anand and Vidick, Thomas and Wright, John and Yuen, Henry},
  title = {MIP* = RE},
  year = {2021},
  issue_date = {November 2021},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {64},
  number = {11},
  issn = {0001-0782},
  url = {https://dl.acm.org/doi/10.1145/3485628},
  doi = {10.1145/3485628},
  abstract = {Note from the Research Highlights Co-Chairs: A Research Highlights paper appearing in Communications is usually peer-reviewed prior to publication. The following paper is unusual in that it is still under review. However, the result has generated enormous excitement in the research community, and came strongly nominated by SIGACT, a nomination seconded by external reviewers.The complexity class NP characterizes the collection of computational problems that have efficiently verifiable solutions. With the goal of classifying computational problems that seem to lie beyond NP, starting in the 1980s complexity theorists have considered extensions of the notion of efficient verification that allow for the use of randomness (the class MA), interaction (the class IP), and the possibility to interact with multiple proofs, or provers (the class MIP). The study of these extensions led to the celebrated PCP theorem and its applications to hardness of approximation and the design of cryptographic protocols.In this work, we study a fourth modification to the notion of efficient verification that originates in the study of quantum entanglement. We prove the surprising result that every problem that is recursively enumerable, including the Halting problem, can be efficiently verified by a classical probabilistic polynomial-time verifier interacting with two all-powerful but noncommunicating provers sharing entanglement. The result resolves long-standing open problems in the foundations of quantum mechanics (Tsirelson's problem) and operator algebras (Connes' embedding problem).},
  journal = {Commun. ACM},
  month = oct,
  pages = {131–138},
  numpages = {8}
}

@article{Sha92,
  author = {Shamir, Adi},
  title = {IP = PSPACE},
  year = {1992},
  issue_date = {Oct. 1992},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {39},
  number = {4},
  issn = {0004-5411},
  url = {https://doi.org/10.1145/146585.146609},
  doi = {10.1145/146585.146609},
  abstract = {In this paper, it is proven that when both randomization and interaction are allowed, the proofs that can be verified in polynomial time are exactly those proofs that can be generated with polynomial space.},
  journal = {J. ACM},
  month = oct,
  pages = {869–877},
  numpages = {9},
  keywords = {IP, PSPACE, interactive proofs}
}

@InProceedings{BCFGRS17,
  author={Ben-Sasson, Eli and Chiesa, Alessandro and Forbes, Michael A. and Gabizon, Ariel and Riabzev, Michael and Spooner, Nicholas},
  editor={Kalai, Yael and Reyzin, Leonid},
  title={Zero Knowledge Protocols from Succinct Constraint Detection},
  booktitle={Theory of Cryptography},
  year={2017},
  publisher={Springer International Publishing},
  address={Cham},
  pages={172--206},
  abstract={We study the problem of constructing proof systems that achieve both soundness and zero knowledge unconditionally (without relying on intractability assumptions). Known techniques for this goal are primarily combinatorial, despite the fact that constructions of interactive proofs (IPs) and probabilistically checkable proofs (PCPs) heavily rely on algebraic techniques to achieve their properties.},
  isbn={978-3-319-70503-3},
}

@article{AS98,
  author = {Arora, Sanjeev and Safra, Shmuel},
  title = {Probabilistic checking of proofs: a new characterization of NP},
  year = {1998},
  issue_date = {Jan. 1998},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {45},
  number = {1},
  issn = {0004-5411},
  url = {https://doi.org/10.1145/273865.273901},
  doi = {10.1145/273865.273901},
  abstract = {We give a new characterization of NP: the class NP contains exactly those languages L for which membership proofs (a proof that an input x is in L) can be verified probabilistically in polynomial time using logarithmic number of random bits and by reading sublogarithmic number of bits from the proof.We discuss implications of this characterization; specifically, we show that approximating Clique and Independent Set, even in a very weak sense, is NP-hard.},
  journal = {J. ACM},
  month = jan,
  pages = {70–122},
  numpages = {53},
  keywords = {NP-completeness, approximation algorithms, complexity hierarchies, computations on polynomials and finite fields, error-correcting codes, hardness of approximations, interactive computation, probabilistic computation, proof checking, reducibility and completeness, trade-offs/relations among complexity measures}
}

@inproceedings{KR08,
  author = {Kalai, Yael Tauman and Raz, Ran},
  title = {Interactive PCP},
  year = {2008},
  isbn = {9783540705826},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg},
  url = {https://doi.org/10.1007/978-3-540-70583-3_44},
  doi = {10.1007/978-3-540-70583-3_44},
  abstract = {A central line of research in the area of PCPs is devoted to constructing short PCPs. In this paper, we show that if we allow an additional interactive verification phase, with very low communication complexity, then for some NP languages, one can construct PCPs that are significantly shorter than the known PCPs (without the additional interactive phase) for these languages. We give many cryptographical applications and motivations for our results and for the study of the new model in general.},
  booktitle = {Proceedings of the 35th International Colloquium on Automata, Languages and Programming, Part II},
  pages = {536–547},
  numpages = {12},
  location = {Reykjavik, Iceland},
  series = {ICALP '08}
}

@article{Pra75,
  author = {Pratt, Vaughan R.},
  title = {Every Prime Has a Succinct Certificate},
  journal = {SIAM Journal on Computing},
  volume = {4},
  number = {3},
  pages = {214-220},
  year = {1975},
  doi = {10.1137/0204018},
  URL = {https://doi.org/10.1137/0204018},
  eprint = {https://doi.org/10.1137/0204018},
  abstract = { To prove that a number n is composite, it suffices to exhibit the working for the multiplication of a pair of factors. This working, represented as af string, is of length bounded by a polynomial in \$\log \_2 n\$. We show that the same property holds for the primes. It is noteworthy that almost no other set is known to have the property that short proofs for membership or nonmembership exist for all candidates without being known to have the property that such proofs are easy to come by. It remains an open problem whether a prime n can be recognized in only \$\log \_2^\alpha n\$ operations of a Turing machine for any fixed \$\alpha \$.The proof system used for certifying primes is as follows.Axiom. \$(x,y,1)\$.Inference Rules. \[ R\_1 :\quad(p,x,a),q \vdash (p,x,qa)\quad\text{provided }x^{(p - 1)/q} \not\equiv 1(\bmod p)\text{ and }q | (p - 1). \]\[ R\_2 :\quad(p,x,p - 1) \vdash p\quad\text{provided } x^{p - 1} \equiv 1(\bmod p). \]Theorem 1. pis a theorem\$\equiv p\$is a prime.Theorem 2. pis a theorem\$\supset p\$has a proof of\$\lceil {4\log \_2 p} \rceil \$lines. }
}

@article{JNVWY20,
  author       = {Zhengfeng Ji and Anand Natarajan and Thomas Vidick and John Wright and Henry Yuen},
  title        = {Quantum soundness of the classical low individual degree test},
  journal      = {CoRR},
  volume       = {abs/2009.12982},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.12982},
  eprinttype   = {arXiv},
  eprint       = {2009.12982},
  timestamp    = {Wed, 30 Sep 2020 16:16:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-12982.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{BF91,
  author={Babai, L{\'a}szl{\'o} and Fortnow, Lance},
  title={Arithmetization: A new method in structural complexity theory},
  journal={computational complexity},
  year={1991},
  month={Mar},
  day={01},
  volume={1},
  number={1},
  pages={41-66},
  abstract={We introduce a technique of arithmetization of the process of computation in order to obtain novel characterizations of certain complexity classes viamultivariate polynomials. A variety of concepts and tools of elementary algebra, such as the degree of polynomials and interpolation, becomes thereby available for the study of complexity classes.},
  issn={1420-8954},
  doi={10.1007/BF01200057},
  url={https://link.springer.com/article/10.1007/BF01200057},
}

@inproceedings{Has97,
  author = {H\r{a}stad, Johan},
  title = {Some optimal inapproximability results},
  year = {1997},
  isbn = {0897918886},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/258533.258536},
  doi = {10.1145/258533.258536},
  booktitle = {Proceedings of the Twenty-Ninth Annual ACM Symposium on Theory of Computing},
  pages = {1–10},
  numpages = {10},
  location = {El Paso, Texas, USA},
  series = {STOC '97}
}

@misc{GOS25,
      title={A Zero-Knowledge PCP Theorem},
      author={Tom Gur and Jack O'Connor and Nicholas Spooner},
      year={2024},
      eprint={2411.07972},
      archivePrefix={arXiv},
      primaryClass={cs.CC},
      url={https://arxiv.org/abs/2411.07972},
}

@article{Cook73,
  title = {A hierarchy for nondeterministic time complexity},
  journal = {Journal of Computer and System Sciences},
  volume = {7},
  number = {4},
  pages = {343-353},
  year = {1973},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(73)80028-5},
  url = {https://www.sciencedirect.com/science/article/pii/S0022000073800285},
  author = {Cook, Stephen A.},
  abstract = {We prove the following theorem in this paper: For any real numbers r1, r2, 1≤r1<r2, there is a set A of strings which has nondeterministic time complexity nr2, but not nondeterministic time complexity nr1. The computing devices are nondeterminsitic multitape Turing machines.},
}

@inproceedings{BGKW88,
  author = {Ben-Or, Michael and Goldwasser, Shafi and Kilian, Joe and Wigderson, Avi},
  title = {Multi-prover interactive proofs: how to remove intractability assumptions},
  year = {1988},
  isbn = {0897912640},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/62212.62223},
  doi = {10.1145/62212.62223},
  abstract = {Quite complex cryptographic machinery has been developed based on the assumption that one-way functions exist, yet we know of only a few possible such candidates. It is important at this time to find alternative foundations to the design of secure cryptography. We introduce a new model of generalized interactive proofs as a step in this direction. We prove that all NP languages have perfect zero-knowledge proof-systems in this model, without making any intractability assumptions.The generalized interactive-proof model consists of two computationally unbounded and untrusted provers, rather than one, who jointly agree on a strategy to convince the verifier of the truth of an assertion and then engage in a polynomial number of message exchanges with the verifier in their attempt to do so. To believe the validity of the assertion, the verifier must make sure that the two provers can not communicate with each other during the course of the proof process. Thus, the complexity assumptions made in previous work, have been traded for a physical separation between the two provers.We call this new model the multi-prover interactive-proof model, and examine its properties and applicability to cryptography.},
  booktitle = {Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing},
  pages = {113–131},
  numpages = {19},
  location = {Chicago, Illinois, USA},
  series = {STOC '88},
}

@article{VW16,
  url = {http://dx.doi.org/10.1561/0400000068},
  year = {2016},
  volume = {11},
  journal = {Foundations and Trends in Theoretical Computer Science},
  title = {Quantum Proofs},
  doi = {10.1561/0400000068},
  issn = {1551-305X},
  number = {1-2},
  pages = {1-215},
  author = {Thomas Vidick and John Watrous},
}

@article{Din07,
  author = {Dinur, Irit},
  title = {The PCP theorem by gap amplification},
  year = {2007},
  issue_date = {June 2007},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {54},
  number = {3},
  issn = {0004-5411},
  url = {https://doi.org/10.1145/1236457.1236459},
  doi = {10.1145/1236457.1236459},
  abstract = {The PCP theorem [Arora and Safra 1998; Arora et. al. 1998] says that every language in NP has a witness format that can be checked probabilistically by reading only a constant number of bits from the proof. The celebrated equivalence of this theorem and inapproximability of certain optimization problems, due to Feige et al. [1996], has placed the PCP theorem at the heart of the area of inapproximability.In this work, we present a new proof of the PCP theorem that draws on this equivalence. We give a combinatorial proof for the NP-hardness of approximating a certain constraint satisfaction problem, which can then be reinterpreted to yield the PCP theorem.Our approach is to consider the unsat value of a constraint system, which is the smallest fraction of unsatisfied constraints, ranging over all possible assignments for the underlying variables. We describe a new combinatorial amplification transformation that doubles the unsat-value of a constraint-system, with only a linear blowup in the size of the system. The amplification step causes an increase in alphabet-size that is corrected by a (standard) PCP composition step. Iterative application of these two steps yields a proof for the PCP theorem.The amplification lemma relies on a new notion of “graph powering” that can be applied to systems of binary constraints. This powering amplifies the unsat-value of a constraint system provided that the underlying graph structure is an expander.We also extend our amplification lemma towards construction of assignment testers (alternatively, PCPs of Proximity) which are slightly stronger objects than PCPs. We then construct PCPs and locally-testable codes whose length is linear up to a polylog factor, and whose correctness can be probabilistically verified by making a constant number of queries. Namely, we prove SAT ∈ PCP 1/2,1[log2(n⋅poly log n), O(1)].},
  journal = {J. ACM},
  month = {6},
  pages = {12–es},
  numpages = {44},
  keywords = {PCP, Gap amplification}
}

@Inbook{GT20,
  author={Goldreich, Oded and Teichner, Liav},
  editor={Goldreich, Oded},
  title={Super-Perfect Zero-Knowledge Proofs},
  bookTitle={Computational Complexity and Property Testing: On the Interplay Between Randomness and Computation},
  year={2020},
  publisher={Springer International Publishing},
  address={Cham},
  pages={119--140},
  abstract={We initiate a study of super-perfect zero-knowledge proof systems. Loosely speaking, these are proof systems for which the interaction can be perfectly simulated in strict probabilistic polynomial-time. In contrast, the standard definition of perfect zero-knowledge only requires that the interaction can be perfectly simulated by a strict probabilistic polynomial-time that is allowed to fail with probability at most one half.},
  isbn={978-3-030-43662-9},
  doi={10.1007/978-3-030-43662-9_8},
  url={https://doi.org/10.1007/978-3-030-43662-9_8},
}

@inproceedings{BGHSV06,
  author = {Ben-Sasson, Eli and Goldreich, Oded and Harsha, Prahladh and Sudan, Madhu and Vadhan, Salil},
  title = {Robust PCPs of proximity, shorter PCPs and applications to coding},
  year = {2004},
  isbn = {1581138520},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1007352.1007361},
  doi = {10.1145/1007352.1007361},
  abstract = {We continue the study of the trade-off between the length of PCP sand their query complexity, establishing the following main results(which refer to proofs of satisfiability of circuits of size n): 1 We present PCPs of length exp(\~{O}(log log n)2)•n that can be verified by making o(log logn) Boolean queries.For every ε>0, we present PCPs of length exp(logε n)• n that can be verified by making a constant number of Boolean queries. In both cases, false assertions are rejected withconstant probability (which may be set to be arbitrarily close to 1). The multiplicative overhead on the length of the proof, introduced by transforming a proof into a probabilistically checkable one, is just quasi-polylogarithmic in the first case (ofquery complexity o(log logn)), and 2(log n)ε, for any ε>0, in the second case (of constant query complexity). In contrast, previous results required at least 2 √logn overhead in the length, even to get query complexity 2 √log n. Our techniques include the introduction of a new variant of PCPs that we call "Robust PCPs". These new PCPs facilitate proof composition, which is a central ingredient in construction of PCP systems. (A related notion and its composition properties were discovered independently by Dinur and Reingold. ) Our main technical contribution is a construction of a "length-efficient" Robust PCP. While the new construction uses many of the standard techniques in PCPs, it does differ from previous constructions in fundamental ways, and in particular does not use the "parallelization" step of Arora et al. . The alternative approach may be of independent interest. We also obtain analogous quantitative results for locally testable codes. In addition, we introduce a relaxed notion of locally decodable codes,and present such codes mapping k information bits to code words of length κ1+ε, for any ε>0.},
  booktitle = {Proceedings of the Thirty-Sixth Annual ACM Symposium on Theory of Computing},
  pages = {1–10},
  numpages = {10},
  keywords = {PCP, locally decodable codes, locally testable codes, probabilistically checkable proofs, property testing},
  location = {Chicago, IL, USA},
  series = {STOC '04}
}

@article{ALMSS98,
  author = {Arora, Sanjeev and Lund, Carsten and Motwani, Rajeev and Sudan, Madhu and Szegedy, Mario},
  title = {Proof verification and the hardness of approximation problems},
  year = {1998},
  issue_date = {May 1998},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {45},
  number = {3},
  issn = {0004-5411},
  url = {https://doi.org/10.1145/278298.278306},
  doi = {10.1145/278298.278306},
  abstract = {We show that every language in NP has a probablistic verifier that checks membership proofs for it using logarithmic number of random bits and by examining a constant number of bits in the proof. If a string is in the language, then there exists a proof such that the verifier accepts with probability 1 (i.e., for every choice of its random string). For strings not in the language, the verifier rejects every provided “proof” with probability at least 1/2. Our result builds upon and improves a recent result of Arora and Safra [1998] whose verifiers examine a nonconstant number of bits in the proof (though this number is a very slowly growing function of the input length).As a consequence, we prove that no MAX SNP-hard problem has a polynomial  time approximation scheme, unless NP = P. The class MAX SNP was defined by Papadimitriou and Yannakakis [1991] and hard problems for this class include vertex cover, maximum satisfiability, maximum cut, metric TSP, Steiner trees and shortest superstring. We also improve upon the clique hardness results of Feige et al. [1996] and Arora and Safra [1998] and show that there exists a positive ε such that approximating the maximum clique size in an N-vertex graph to within a factor of Nε is NP-hard.},
  journal = {J. ACM},
  month = may,
  pages = {501–555},
  numpages = {55},
  keywords = {randomness, proof verification, optimization, NP-completeness}
}

@article{Ham50,
  author={Hamming, R. W.},
  journal={The Bell System Technical Journal},
  title={Error detecting and error correcting codes},
  year={1950},
  volume={29},
  number={2},
  pages={147-160},
  keywords={},
  doi={10.1002/j.1538-7305.1950.tb00463.x}
}

@article{Par21,
  title = {Smooth and Strong PCPs},
  volume = {30},
  ISSN = {1420-8954},
  url = {https://link.springer.com/article/10.1007/s00037-020-00199-3},
  DOI = {10.1007/s00037-020-00199-3},
  number = {1},
  journal = {Computational Complexity},
  publisher = {Springer Science and Business Media LLC},
  author = {Paradise, Orr},
  year = {2021},
  month = jan
}

@inproceedings{KPT97,
  author = {Kilian, Joe and Petrank, Erez and Tardos, G\'{a}bor},
  title = {Probabilistically checkable proofs with zero knowledge},
  year = {1997},
  isbn = {0897918886},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/258533.258643},
  doi = {10.1145/258533.258643},
  booktitle = {Proceedings of the Twenty-Ninth Annual ACM Symposium on Theory of Computing},
  pages = {496–505},
  numpages = {10},
  location = {El Paso, Texas, USA},
  series = {STOC '97}
}

@inproceedings{Imp95,
  author={Impagliazzo, R.},
  booktitle={Proceedings of Structure in Complexity Theory. Tenth Annual IEEE Conference},
  title={A personal view of average-case complexity},
  year={1995},
  volume={},
  number={},
  pages={134-147},
  keywords={Computer science;Drives;Distributed computing;Cryptography;Complexity theory;Bibliographies},
  doi={10.1109/SCT.1995.514853},
}

@article{Mul54,
  author={Muller, D. E.},
  journal={Transactions of the I.R.E. Professional Group on Electronic Computers},
  title={Application of Boolean algebra to switching circuit design and to error detection},
  year={1954},
  volume={EC-3},
  number={3},
  pages={6-12},
  keywords={Switching circuits;Systematics;Symbols;Voltage;Switches;Reviews;Relays},
  doi={10.1109/IREPGELC.1954.6499441},
}

@article{Sch80,
  author = {Schwartz, J. T.},
  title = {Fast Probabilistic Algorithms for Verification of Polynomial Identities},
  year = {1980},
  issue_date = {Oct. 1980},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {27},
  number = {4},
  issn = {0004-5411},
  url = {https://doi.org/10.1145/322217.322225},
  doi = {10.1145/322217.322225},
  journal = {J. ACM},
  month = {10},
  pages = {701–717},
  numpages = {17},
}

@InProceedings{Zip79,
  author={Zippel, Richard},
  editor={Ng, Edward W.},
  title={Probabilistic algorithms for sparse polynomials},
  booktitle={Symbolic and Algebraic Computation},
  year={1979},
  publisher={Springer Berlin Heidelberg},
  address={Berlin, Heidelberg},
  pages={216--226},
  abstract={In this paper we have tried to demonstrate how sparse techniques can be used to increase the effectiveness of the modular algorithms of Brown and Collins. These techniques can be used for an extremely wide class of problems and can applied to a number of different algorithms including Hensel's lemma. We believe this work has finally laid to rest the bad zero problem.},
  isbn={978-3-540-35128-3},
  doi={10.1007/3-540-09519-5_73},
}
